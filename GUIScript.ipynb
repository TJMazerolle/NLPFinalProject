{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_links(search_term, limit = 100):\n",
    "\n",
    "    driver = webdriver.Firefox()\n",
    "    \n",
    "    url_path = f'https://www.investing.com/search/?q={search_term}&tab=news'\n",
    "    driver.get(url_path)\n",
    "\n",
    "    links = []\n",
    "\n",
    "    pos = 0\n",
    "    while len(driver.find_elements(by=By.CLASS_NAME, value=\"articleItem\")) < limit:\n",
    "        pos += 500\n",
    "        driver.execute_script(f'window.scrollTo(0, {pos})')\n",
    "\n",
    "    article_items = driver.find_elements(by=By.CLASS_NAME, value=\"articleItem\")  \n",
    "    for article in article_items:\n",
    "        if article.get_attribute(\"class\") != \"js-article-item articleItem     \": \n",
    "            link = article.find_element(by=By.CLASS_NAME, value=\"title\")         \n",
    "            links.append(link.get_attribute(\"href\"))   \n",
    "            \n",
    "    driver.quit()\n",
    "    return links\n",
    "\n",
    "def get_article_text(link):\n",
    "\n",
    "    response = requests.get(link)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        article = soup.find('div', class_='WYSIWYG articlePage')\n",
    "        date = soup.find(\"div\", class_ = \"contentSectionDetails\")\n",
    "\n",
    "        # Extracting article text\n",
    "        script = article.find(\"script\")\n",
    "        if script != None:\n",
    "            script.extract()\n",
    "        img_carousel = article.find('div', class_='imgCarousel')\n",
    "        if img_carousel != None:\n",
    "            img_carousel.extract()\n",
    "        related_instruments_wrapper = article.find('div', class_='relatedInstrumentsWrapper')\n",
    "        if related_instruments_wrapper != None:\n",
    "            related_instruments_wrapper.extract()\n",
    "        paragraph = article.find(\"p\")\n",
    "        if paragraph != None:\n",
    "            em = paragraph.find(\"em\")\n",
    "            if em != None:\n",
    "                paragraph.extract()\n",
    "        text_inside_div = article.get_text()\n",
    "        \n",
    "        # Extracting article date\n",
    "        cont_sect_det = soup.find_all(\"div\", class_ = \"contentSectionDetails\")\n",
    "        date = cont_sect_det[1].find(\"span\").text\n",
    "        \n",
    "        return [text_inside_div.strip(), date[10:-11]]\n",
    "    \n",
    "    else:\n",
    "        return [None, None]\n",
    "    \n",
    "def get_article_texts(links, tracker = False):\n",
    "    results = []\n",
    "    for i, link in enumerate(links):\n",
    "        results.append(get_article_text(link))\n",
    "        if tracker:\n",
    "            print(i + 1, \"/\", len(links))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert GUI here.  The GUI will take in an input (which is expected to be a stock symbol) and store it in the variable below.\n",
    "\n",
    "stock_symbol = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is where the back-end analysis begins.  First, we get our handful of links given the search term above using the get_article_links() function.\n",
    "links = get_article_links(stock_symbol)\n",
    "\n",
    "# Then we take the list of links and get the corresponding text from each article using the get_article_texts() function.\n",
    "texts = get_article_texts(links)\n",
    "\n",
    "# Next, we take the list of article texts and feed them into the model to get sentiments.  At this point, the model is not yet built, so I will write some placeholder code here for now.\n",
    "def sentiment_model(list_of_texts):\n",
    "    return None\n",
    "sentiments = sentiment_model(texts)\n",
    "\n",
    "# Lastly, we can take the mode of the predicted sentiments to get the overall sentiment of the given articles.  Note that there are different ways we can do this; I'm just saying mode for now for simplicity.\n",
    "from statistics import mode\n",
    "overall_sentiment = mode(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our GUI should output the value of overall_sentiment."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
