{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_links(search_term, limit = 100, driver = webdriver.Firefox()):\n",
    "    \n",
    "    url_path = f'https://www.investing.com/search/?q={search_term}&tab=news'\n",
    "    driver.get(url_path)\n",
    "\n",
    "    links = []\n",
    "\n",
    "    pos = 0\n",
    "    while len(driver.find_elements(by=By.CLASS_NAME, value=\"articleItem\")) < limit:\n",
    "        pos += 500\n",
    "        driver.execute_script(f'window.scrollTo(0, {pos})')\n",
    "\n",
    "    article_items = driver.find_elements(by=By.CLASS_NAME, value=\"articleItem\")  \n",
    "    for article in article_items:\n",
    "        if article.get_attribute(\"class\") != \"js-article-item articleItem     \": \n",
    "            link = article.find_element(by=By.CLASS_NAME, value=\"title\")         \n",
    "            links.append(link.get_attribute(\"href\"))             \n",
    "            \n",
    "    driver.quit()\n",
    "    return links\n",
    "\n",
    "article_links = get_article_links(\"msft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"BEIJING (Reuters) - Shanghai authorities told Microsoft (NASDAQ:MSFT)'s visiting president on Tuesday they want his company to promote artificial intelligence technology to boost businesses there, the Chinese financial hub's government said. \\nChen Jining, Shanghai's Communist party Secretary, made the remarks while meeting Microsoft President and Vice Chair Brad Smith, the government said. Shanghai was also open to Microsoft collaborating on studying technology-related governance frameworks and standards, Chen said.\",\n",
       " 'By Foo Yun Chee, Supantha Mukherjee and Martin Coulter BRUSSELS/STOCKHOLM/LONDON (Reuters) - European Union ambitions to take a lead in landmark rules for artificial intelligence hang in the balance as member states and lawmakers meet on Wednesday to try to hammer out a deal on biometric surveillance and how to regulate systems like ChatGPT. If agreed, the EU\\'s first-of-a-kind AI Act, which was proposed by the European Commission two years ago, could serve as the benchmark for countries seeking an alternative to the United States\\' light-touch approach and China\\'s interim rules. Talks between EU members and lawmakers will start at 1400 GMT and are expected to run into the early hours of Thursday, with the most likely outcome a provisional deal on principles but not crucial details, five people directly involved said. A final deal would then need to be agreed before legislation could be put in place, which could pave the way towards it becoming law before European parliamentary elections in June. But without a deal, the AI Act is likely to be shelved due to a lack of time, resulting in the 27-member bloc losing its first-mover advantage in regulating the technology. Alexandra van Huffelen, Dutch minister for digitalisation, told Reuters it was critical the EU finds a compromise, particularly on generative AI, by the end of the year. \"The world is watching us: citizens, stakeholders, NGOs and the private sector want us to agree on a meaningful piece of legislation regarding AI, including GPAI,\" she said referring to general purpose AI systems, which have a wide range of uses. DEMANDS The proposed AI rules face conflicting EU demands. The two biggest are over the use of AI in biometric surveillance and foundation models, the generative AI such as Microsoft (NASDAQ:MSFT) backed OpenAI which trains on large sets of data to perform various tasks. EU lawmakers want to ban the use of AI in biometric surveillance, while governments want an exception for national security, defence and military purposes. A late proposal by France, Germany and Italy to let makers of generative AI models self-regulate added more uncertainty. \\nEU ambassadors and lawmakers held separate preparatory meetings last week, but differences remain which could make it difficult to clinch a deal, said the people involved in the talks, who declined to be named because they are confidential. An official from one major EU country said whatever the meeting\\'s outcome, there will still be a lot more work to do.',\n",
       " \"HONG KONG (Reuters) - China's Commerce Minister Wang Wentao met with Microsoft (NASDAQ:MSFT) President Brad Smith on Wednesday, and exchanged views on Microsoft's development in China, Sino-US economic and trade relations and other issues, the Chinese commerce ministry said. China hopes Microsoft can play a constructive role in China-US cooperation in artificial intelligence and other areas, the ministry quoted Wang as saying, adding that China welcomes multinational companies to continue to do business in the country.\",\n",
       " \"Microsoft announced its first-ever custom chips for cloud and AI services.\\nThe chips take a new approach to cloud infrastructure and will reduce dependency on NVIDIA.\\nWhile a threat, the collaborative nature of AI advancement will keep NVIDIA in the top spot among chipmakers.\\nIt is unlikely that NVIDIA Corporation (NASDAQ:NVDA) will lose its top spot in the AI world soon. The company has too big a lead and too much market share to fall by the wayside without some fundamental shift in the landscape. However, losing market share to competitors like Advanced Micro Devices Inc (NASDAQ:AMD) and Microsoft Corporation (NASDAQ:MSFT) is an equally significant risk.\\nThe AI market, including chips, infrastructure, and services, will continue to grow, but loss of market share would provide an unstable footing for NVIDIA that will weigh on the market over time.\\nMicrosoft is best positioned to disrupt NVIDIA’s hold on the AI market. It provides a significant portion of cloud services to the Internet and is gaining market share from AWS NASDAQ: AMZN because of its utility to AI. It now focuses on proprietary microchips, and more competition for NVIDIA will undoubtedly arise.\\nThe caveat for investors is that much of the cloud and AI is built on collaborative efforts that include hardware, software, and services from multiple vendors. Microsoft is building some chips but isn’t severing ties with NVIDIA or Advanced Micro Devices; it’s building on existing partnerships to advance AI infrastructure and services for all.Microsoft aims at AI dynasty, takes a new approach to the CloudMicrosoft announced its Azure cloud architecture's first two custom chips in early November. The chips are slated to come out in 2024 and will reduce dependency on NVIDIA but not erase it. The chips are a new approach to the cloud that aims to reduce power use, improve efficiency and lower costs by optimizing Microsoft’s cloud and AI stack from bottom to top.\\nThe chips, Azure Maia 100 and Azure Cobalt 100 are cloud- and AI-specific and intended to be a launch pad for future AI development by Microsoft and its clients. The Cobalt 100 is a CPU for general cloud services with a 128-core Arm Neoverse base customized for Microsoft.\\nThe MAIA 100 is an AI accelerator for running large workloads, such as training LLM, including the models used by OpenAI, a collaborator on the project. OpenAI is working with Microsoft on the systems' design and testing, which is underway now.\\nHowever, this doesn’t mean NVIDIA or Advanced Micro Devices will be removed from the Microsoft cloud picture. Microsoft uses and will continue to use NVIDIA and AMD microchips, GPUs and accelerators in its cloud infrastructure.\\nLikewise, NVIDIA is running its generative AI foundry service on Azure. The service provides a platform for businesses to customize AI models and is already in demand. The biggest takeaway from MSFT’s decision is that the new chips and infrastructure will aid AI inference and the AI industry.Analysts drive Microsoft to new highs The analysts weren’t moved by the chip announcement but rather by the OpenAI debacle that left the company and Microsoft in a better position than before. The botched ousting of Sam Altman resulted in a new, stronger board of directors and the reinstatement of Mr. Altman to the helm.\\nDan Ives of Wedbush called it a Cinderella ending for the company, reiterating an Outperform rating and a $425 price target. That target is well above the consensus estimate of $388, which assumes fair value at current market pricing and raises the consensus target. The consensus price listed by Marketbeat.com is up nearly 30% compared to last year and trending higher following the latest earnings release. In that report, Microsoft outperformed expectations and raised guidance on strengths in Azure and cloud services.The Technical Outlook: Microsoft hits new high, new highs on deckThe price action in Microsoft confirmed the primary uptrend with a bounce that began following the Q3 earnings release. The market advanced more than 15% to break above the all-time high and set new highs for three consecutive weeks. The indicators confirm the move, consistent with a trend-following action, but now show an overbought market. The overbought condition could lead to a price correction, but a consolidation is more likely. The market may pull back modestly in that scenario before finding support and moving higher. Microsoft Stock Chart \\nOriginal Post\",\n",
       " \"Amid a surge propelled by AI, Microsoft (NASDAQ:MSFT) is steadily closing in on Apple (NASDAQ:AAPL)'s coveted position as the world's most valuable company.\\n \\n\\nMicrosoft received a fresh growth boost in 2023 after the unprecedented demand for AI solutions disrupted the tech sector. Under Satya Nadella’s leadership, Microsoft positioned itself as one of the frontrunners of the ongoing AI revolution, and thanks to this accelerated growth pace, it is becoming an increasing threat to Apple’s ‘most valuable company’ title.\\nMicrosoft’s Rise Backed by AI Goldrush\\nApple became the world’s most valuable company in 2011 when its market cap hit $340 billion.\\nSince then, the iPhone maker has rarely lost this status as it continued its steady growth in the years to come. The company reached the $1 trillion milestone in 2018 and surged close to a $3 trillion valuation this year.\\nHowever, there is a growing threat to Apple’s top spot, and it’s none other than its long-time rival Microsoft.\\nFueled by the ongoing AI revolution, Microsoft’s valuation surged to over $2.8 trillion in 2023, just 5.7% short of Apple’s $2.98 trillion. The tech giant has been on a relentless rally in 2023, printing new all-time highs multiple times in recent months amid strong demand for its AI and cloud services.\\nAt the moment, MSFT’s stock price is sitting at $377.2, compared to Apple’s $189.7. Microsoft’s shares must surge above the $400.5 mark to dethrone Apple as the most valuable business.\\nSatya Nadella’s Impact on the Tech Giant\\nMicrosoft’s remarkable ascent in 2023 can be attributed to a harmonious blend of factors. However, when viewed through a wider lens, the company’s new growth stage began in 2014 with the appointment of Satya Nadella as its CEO.\\nSince taking the helm, the visionary Indian-American business executive has brought significant strategic shifts to the tech giant. Notably, Nadella adopted a cloud-first, mobile-first strategy, reinforcing Microsoft’s dominance in the evolving tech space.\\nThis turned out to be a transformational move for Microsoft, resulting in unprecedented growth in its cloud computing platform, Azure. But in reality, Nadella’s pursuit of innovation has been felt across Microsoft’s entire product and services lineup, including Windows OS and Microsoft Surface.\\nFast forward to 2023, the year of the AI revolution, Nadella left nothing to chance. Under his leadership, the company struck a breakthrough partnership with ChatGPT maker OpenAI, pouring $13 billion into Sam Altman’s startup. In return, Microsoft accessed ChatGPT’s underlying large language model (LLM), GPT, which it then used to refresh its products and services – from GitHub over Bing and Office to Azure.\\n***\\nThis article was originally published on The Tokenist. Check out The Tokenist’s free newsletter, Five Minute Finance, for weekly analysis of the biggest trends in finance and technology.\\nNeither the author, Tim Fries, nor this website, The Tokenist, provide financial advice. Please consult our website policy prior to making financial decisions.\",\n",
       " 'MSFTIn a previous piece, I posted an analysis of Microsoft (MSFT), which, at the time, I thought was bullish with a good risk-return relationship.Back then, the stock was at $31. Yesterday it closed at $31.45. Target still remains $32,50 as previous important highs are going to be retested. My view on this stock is still bullish as long as it stays above $30.90. I think MSFT will accelerate toward $32.50 where it will take a pause. Long term, $28 is my stop and $30.90 is my short-term stop. Depending on your risk profile you could choose between those two. Longer-term target, of course, is much higher than $32.50, towards $42.Disclosure: None of the information or opinions expressed in this blog constitutes a solicitation for the purchase or sale of any security or other instrument. Nothing in this article constitutes investment advice and any recommendations that may be contained herein have not been based upon a consideration of the investment objectives, financial situation or particular needs of any specific recipient. Any purchase or sale activity in any securities or other instrument should be based upon your own analysis and conclusions.',\n",
       " \"BEIJING (Reuters) - Shanghai authorities told Microsoft (NASDAQ:MSFT)'s visiting president on Tuesday they want his company to promote artificial intelligence technology to boost businesses there, the Chinese financial hub's government said. \\nChen Jining, Shanghai's Communist party Secretary, made the remarks while meeting Microsoft President and Vice Chair Brad Smith, the government said. Shanghai was also open to Microsoft collaborating on studying technology-related governance frameworks and standards, Chen said.\",\n",
       " 'By Foo Yun Chee, Supantha Mukherjee and Martin Coulter BRUSSELS/STOCKHOLM/LONDON (Reuters) - European Union ambitions to take a lead in landmark rules for artificial intelligence hang in the balance as member states and lawmakers meet on Wednesday to try to hammer out a deal on biometric surveillance and how to regulate systems like ChatGPT. If agreed, the EU\\'s first-of-a-kind AI Act, which was proposed by the European Commission two years ago, could serve as the benchmark for countries seeking an alternative to the United States\\' light-touch approach and China\\'s interim rules. Talks between EU members and lawmakers will start at 1400 GMT and are expected to run into the early hours of Thursday, with the most likely outcome a provisional deal on principles but not crucial details, five people directly involved said. A final deal would then need to be agreed before legislation could be put in place, which could pave the way towards it becoming law before European parliamentary elections in June. But without a deal, the AI Act is likely to be shelved due to a lack of time, resulting in the 27-member bloc losing its first-mover advantage in regulating the technology. Alexandra van Huffelen, Dutch minister for digitalisation, told Reuters it was critical the EU finds a compromise, particularly on generative AI, by the end of the year. \"The world is watching us: citizens, stakeholders, NGOs and the private sector want us to agree on a meaningful piece of legislation regarding AI, including GPAI,\" she said referring to general purpose AI systems, which have a wide range of uses. DEMANDS The proposed AI rules face conflicting EU demands. The two biggest are over the use of AI in biometric surveillance and foundation models, the generative AI such as Microsoft (NASDAQ:MSFT) backed OpenAI which trains on large sets of data to perform various tasks. EU lawmakers want to ban the use of AI in biometric surveillance, while governments want an exception for national security, defence and military purposes. A late proposal by France, Germany and Italy to let makers of generative AI models self-regulate added more uncertainty. \\nEU ambassadors and lawmakers held separate preparatory meetings last week, but differences remain which could make it difficult to clinch a deal, said the people involved in the talks, who declined to be named because they are confidential. An official from one major EU country said whatever the meeting\\'s outcome, there will still be a lot more work to do.',\n",
       " \"HONG KONG (Reuters) - China's Commerce Minister Wang Wentao met with Microsoft (NASDAQ:MSFT) President Brad Smith on Wednesday, and exchanged views on Microsoft's development in China, Sino-US economic and trade relations and other issues, the Chinese commerce ministry said. China hopes Microsoft can play a constructive role in China-US cooperation in artificial intelligence and other areas, the ministry quoted Wang as saying, adding that China welcomes multinational companies to continue to do business in the country.\",\n",
       " None,\n",
       " 'By Diane Bartz WASHINGTON (Reuters) - U.S. antitrust enforcers will argue on Wednesday that a federal judge got it wrong when she ruled that Microsoft (NASDAQ:MSFT)\\'s $69 billion deal to buy \"Call of Duty\" maker Activision Blizzard (NASDAQ:ATVI) was legal under competition law, in their latest attempt to stop the deal. Microsoft closed the deal, originally proposed in January 2022 as the biggest acquisition in the history of the gaming industry, on Oct. 13 of this year after obtaining the approval of British regulators. The Federal Trade Commission, however, is expected to tell a three-judge appeals court panel in California that the lower-court judge held the agency to too high a standard, effectively requiring it to prove that the deal was anticompetitive when the standard is simply that the deal raises serious competitive concerns. The FTC is fighting an uphill battle, given that it lost the lower-court fight and that the EU and Britain have signed off on the deal. The legal battle is part of a broader push by the Biden administration to fight mergers and price hikes that affect consumers in areas ranging from medicines to airline tickets. The FTC is also expected to argue the judge was wrong to rely on deals that Microsoft struck with rivals to distribute games as proof the merger would not hurt competition. The FTC filed a lawsuit aimed at stopping the deal in December 2022, arguing that Microsoft would use Activision\\'s popular games to suppress competition to its Xbox consoles and dominate fast-growing subscription and cloud gaming businesses. But a federal judge in California ruled in July that it failed to make its case. \\nMicrosoft is expected to argue that the FTC has failed to show that the judge erred in her ruling. It will also contend that the agency failed to show that Microsoft had an incentive to withhold \"Call of Duty\" from rival gaming platforms. The judges on the panel are scheduled to be Daniel Collins and Danielle Forrest, both nominated by former President Donald Trump, and Jennifer Sung, nominated by President Joe Biden.',\n",
       " 'By Martin Coulter LONDON (Reuters) -Amazon has told Britain\\'s antitrust authority its rival Microsoft (NASDAQ:MSFT) uses business practices that restrict customer choice in the cloud computing market, the second major company to criticise the U.S. tech giant\\'s operations. Britain\\'s Competition and Markets Authority (CMA) launched an investigation into the country’s cloud computing industry in October, following a referral from media regulator Ofcom that highlighted Amazon (NASDAQ:AMZN) and Microsoft’s dominance of the market. In a letter published on the CMA’s website on Tuesday, Amazon said changes to Microsoft’s terms of services had made it difficult for customers to switch to alternative cloud providers, or run competitors’ services alongside.  “To use many of Microsoft’s software products with these other cloud services providers, a customer must purchase a separate license even if they already own the software,” Amazon said. “This often makes it financially unviable for a customer to choose a provider other than Microsoft.”  Last week, Reuters reported Google (NASDAQ:GOOGL) had submitted a similar letter to the watchdog, claiming Microsoft’s business practices had left its rivals at an unfair disadvantage. Google made six recommendations to the CMA, including forcing Microsoft to improve interoperability for customers using its Azure service and alongside other cloud programmes, and banning it from withholding security updates from those that switch. \\nIn its own submission to the CMA, Microsoft said Britain\\'s cloud computing market remained competitive.  \"There are many sources of competition in the cloud market in the UK. Google, Oracle (NYSE:ORCL),  IBM  (NYSE:IBM) and many other cloud players are also investing billions of pounds in cloud infrastructure globally to satisfy demand and are competing strongly for each customer workload where they operate,\" it wrote.',\n",
       " \"In the bustling semiconductor industry, QUALCOMM Inc. (NASDAQ:QCOM) stands out as a key player, particularly known for its wireless technology prowess and a strong foothold in the smartphone market. With the industry at a critical juncture, marked by rapid technological advancements and shifting market dynamics, Qualcomm (NASDAQ:QCOM)'s strategic moves and financial health are under intense scrutiny. This analysis delves into the company's performance, product segments, competitive landscape, market trends, regulatory environments, and management strategies, aiming to provide potential investors with a comprehensive understanding of Qualcomm's position and prospects.Company Profile and Market PositionQualcomm's reputation in the digital wireless communications equipment sector is formidable, thanks to its intellectual property in CDMA and orthogonal frequency division multiplexing access technologies. The company's integrated circuits, based on CDMA technology, are essential for a range of applications, including workforce tracking, asset management, and wireless content enablement. Qualcomm's market position is further bolstered by its Snapdragon 5G Modem-RF Systems, which are integral to the current and upcoming smartphone launches.Strategic Partnerships and AgreementsA significant feather in Qualcomm's cap is the extension of its supply agreement with tech giant Apple Inc. (NASDAQ:AAPL), which now runs through FY27. This deal is not just about numbers; it's a testament to Qualcomm's technological leadership and the strategic importance of its products. Analysts view this extended partnership as a bridge to a more diversified revenue profile for Qualcomm, particularly in the automotive sector, with the potential to create sustainable earnings and robust annual cash flow.Financial Health and Stock PerformanceThe financial estimates paint a positive picture, with adjusted diluted EPS showing a steady climb from the actual FY22 figure of $12.53 to an estimated $10.93 in FY25. Qualcomm's stock has demonstrated resilience, with a 4% uptick year-to-date, although it has seen an 11% decline over the last twelve months. Trading at a multiple below its five-year average, the company appears undervalued relative to its earnings power, according to analysts.Product Segments and DiversificationBeyond handsets, Qualcomm has been making inroads into the automotive and PC markets. The company's long-term optionality is a standout, with secure design wins in auto and potential in PCs. Qualcomm's exclusivity agreement with Microsoft (NASDAQ:MSFT) for Arm-based Windows PCs, set to expire in 2024, is a key area of focus. The Snapdragon Summit, starting October 24, is expected to shed light on Qualcomm's strategies in this segment post-exclusivity period.Competitive Landscape and Market TrendsThe competitive landscape for Qualcomm is complex, with the handset market nearing a cyclical trough after a prolonged period of inventory correction. Qualcomm's edge lies in its long-term agreements and diversification efforts, which provide a cushion against the intense competition as 5G matures. However, the company must navigate the risks of a permanent Android share loss to Apple and the pressure to use domestic suppliers in China.Regulatory Environment and External FactorsQualcomm's reliance on third-party foundries introduces risks, such as potential production limitations or increasing costs. Additionally, the company must contend with the challenges of enforcing essential patent licenses. Economic slowdowns, affecting consumer and corporate spending on smartphone upgrades, also pose a threat to Qualcomm's revenue streams.Bear CaseAre Qualcomm's handset revenues at risk?The handset segment, despite being a major revenue generator for Qualcomm, faces long-term risks from increased competition and the maturation of 5G technology. As 5G technology becomes more widespread, the entry of new players could dilute Qualcomm's market share, affecting its profitability and unit shipments. Moreover, a shift in market preference towards Apple's devices could shrink Qualcomm's total addressable market, impacting its long-term revenue potential in the handset business.Can Qualcomm sustain growth amid economic headwinds?Qualcomm's reliance on the broader smartphone market, which is currently experiencing pressures, raises concerns about its ability to sustain growth. An economic downturn could lead to reduced consumer and corporate spending on smartphone upgrades, directly impacting Qualcomm's bottom line. Additionally, operational expenditure improvements may be limited, as maintaining the Apple business requires significant resources, potentially constraining profitability.Bull CaseHow will Qualcomm's agreement with Apple impact its future?The extended supply agreement with Apple is a significant win for Qualcomm, ensuring a stable revenue stream and reinforcing its position in the iPhone ecosystem until at least 2026. This deal is expected to contribute substantially to Qualcomm's earnings, with an estimated $1.50 added to EPS from the Apple QCT business alone. The agreement also underscores Qualcomm's technological prowess and its ability to secure long-term commitments from industry giants.Does Qualcomm have growth potential beyond smartphones?Qualcomm's diversification strategy, particularly its forays into the automotive and PC sectors, positions the company for growth beyond its traditional smartphone market. The company's expertise in modem development, especially amidst evolving 5G standards, provides it with a competitive edge. With secure design wins in the auto industry and potential in PCs, Qualcomm has multiple avenues for revenue expansion and EPS reacceleration.SWOT AnalysisStrengths:\\n- Strong supply agreement with Apple extending through FY27.\\n- Technological leadership in CDMA and 5G Modem-RF Systems.\\n- Diversification into automotive and PC sectors.\\nWeaknesses:\\n- Exposure to risks from the maturation of 5G technology.\\n- Dependence on third-party foundries for production.\\n- Economic sensitivity of the smartphone market.\\nOpportunities:\\n- Potential for significant free cash flow generation.\\n- Optionality from long-term agreements and diversification efforts.\\n- Expansion into new markets post-smartphone exclusivity agreements.\\nThreats:\\n- Increased competition in the handset market.\\n- Apple's internal modem development efforts.\\n- Regulatory and geopolitical tensions affecting market dynamics.Analysts Targets- Wolfe Research: Outperform; $145 (September 18, 2023).\\n- Barclays Capital Inc.: Overweight; $140 (September 12, 2023).\\n- Rosenblatt Securities Inc.: $145 (September 11, 2023).\\n- Evercore ISI: In Line; $140 (September 11, 2023).\\n- Deutsche Bank Securities Inc.: Hold; $120 (September 11, 2023).\\nThe timeframe for this analysis spans from September to October 2023.InvestingPro InsightsAs Qualcomm (NASDAQ:QCOM) continues to navigate the semiconductor landscape, real-time data and insights become crucial for investors seeking to understand the company's financial health and stock performance. According to InvestingPro data, Qualcomm's market capitalization stands at a robust $145.52 billion, reflecting its significant presence in the industry. The company's P/E ratio, a key indicator of its valuation, is currently at 20.19, with an adjusted P/E ratio for the last twelve months as of Q4 2023 at 17.94. These figures suggest a potentially attractive valuation relative to the company's earnings power.\\nInvestingPro Tips highlight Qualcomm's strong financial discipline and investor-friendly practices. Notably, Qualcomm has raised its dividend for 21 consecutive years, signaling confidence in its financial stability and commitment to returning value to shareholders. Additionally, 16 analysts have revised their earnings upwards for the upcoming period, indicating a positive outlook on the company's profitability.\\nFor those looking to delve deeper into Qualcomm's financial metrics and strategic positioning, InvestingPro offers additional tips that could enrich your analysis. Currently, there are 15 more InvestingPro Tips available for Qualcomm, which can be accessed by subscribers. As a reminder, the InvestingPro subscription is now on a special Cyber Monday sale with a discount of up to 60%. Plus, use the coupon code research23 to get an additional 10% off a 2-year InvestingPro+ subscription. These insights can be a valuable asset in making informed investment decisions about Qualcomm and its place in the dynamic semiconductor market.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " 'By Gloria Dickie, Elizabeth Piper and Alexander Cornwell DUBAI (Reuters) -The United Arab Emirates and several charities at the U.N. climate summit on Sunday offered $777 million in financing for eradicating neglected tropical diseases that are expected to worsen as temperatures climb. Climate-related factors \"have become one of the greatest threats to human health in the 21st century\", COP28 President Sultan Ahmed Al-Jaber said in a statement.  The pledges, made as the COP28 summit on Sunday focused on climate-related health risks, included $100 million from the UAE and another $100 million from the Bill and Melinda Gates Foundation. Others to announce funds for climate-related health issues included Belgium, Germany and the U.S. Agency for International Development.  The World Bank launched a program to explore possible support measures for public health in developing countries, where climate-related health risks are especially high. The burden of tropical diseases will worsen as the world warms, along with other climate-driven health threats including malnutrition, malaria, diarrhoea and heat stress. Many tropical diseases are already easy to treat. River blindness and sleeping sickness, for example, are both endemic to Africa and spread through parasitic worms and flies that are likely to proliferate in a warming world. More than 120 countries have signed a COP28 declaration acknowledging their responsibility to keep people safe amid global warming.  The declaration made no mention of fossil fuels, the main source of climate-warming emissions, which the Global Climate and Health Alliance called a \"glaring omission\".  Activists including physicians in white coats held a small demonstration on Sunday within the COP28 compound to raise awareness of the issue.  \"We are in a lot of trouble,\" said Joseph Vipond, an emergency physician from Alberta, Canada. He recalled the case of a child dying from an asthma attack made worse by smoke inhalation from Western Canada\\'s record wildfires this year. \"This is having real world impacts.\"  Climate change is also increasing the frequency of dangerous storms and more erratic rainfall.  In September Storm Daniel killed more than 11,000 people in Libya, and last year\\'s massive flooding in Pakistan fueled a 400% increase in malaria cases across the country, according to the World Health Organization.  Earlier on Sunday, Microsoft (NASDAQ:MSFT) co-founder turned philanthropist Bill Gates said scientists were working on new treatments for and prevention of mosquito-spread malaria as the rise in temperatures creates more hospitable habitat for the insects to breed.  \"We have new tools at the lab level that decimate mosquito populations,\" said Gates, whose foundation supports public health research and projects for the developing world.  \"These new innovations give us a chance, at a reasonable cost, to make progress.\"  Former U.S. Secretary of State Hillary Clinton also spoke on Sunday, urging reform to the world\\'s insurance system as another key requirement to keep people safe.  \"Right now insurance companies are pulling out of so many places, they\\'re not insuring homes, they\\'re not insuring businesses,\" Clinton said, addressing a panel on women and climate resiliency.  \"It\\'s people everywhere who are going to be left out with no backup, no insurance for their business or their home.\" \\n___ For daily comprehensive coverage on COP28 in your inbox, sign up for the Reuters Sustainable  Switch  (NYSE:SWCH) newsletter here.',\n",
       " \"(Reuters) - Office rental firm IWG said on Tuesday it will resume regular dividend payouts and set a medium-term core profit target of $1 billion, betting on permanent hybrid work models and improved pricing. Office landlords are slowly recovering from pandemic lows as employers opt for permanent hybrid working models, in which employees need to be in the office for a stipulated number of days. Switzerland-headquartered IWG, which owns the Spaces and Regus brands, has said it is looking to acquire more of collapsed rival WeWork's sites on expectations of strong demand.  The company, which paused dividends in 2020, said it expects to pay a final dividend of 1 pence per share.  \\nIWG, which has about 3,500 work locations in more than 120 countries, counts Microsoft (NASDAQ:MSFT), Disney, Samsung (KS:005930) and HSBC, among others, as its clients.  Shares in the London-listed firm were up 2.5% to 153.1 pence at 1350 GMT.\",\n",
       " 'What Happened:\\nShares of data-mining and analytics company Palantir (NYSE:PLTR)\\nfell 8.2% in the morning session as stocks pulled back, and yields rose (leading to a bond selloff) after the S&P 500 reached a new high for the year in the final week of November 2023. Throughout 2023, major indices demonstrated robust performances, with the Nasdaq surging over 45% and the S&P 500 gaining 20% with a month remaining in the year. Investors may be asking if the market is getting ahead of itself.\\nThe Federal Reserve has been raising interest rates to combat inflation, and the latest data showed that their efforts may be paying off. As a result, there seems to be increased optimism in the market that because inflation is stabilizing, interest rates could stabilize or even move lower. The bullish sentiment could be shifting in the other direction, even if just for a short period. As a reminder, lower rates are good for stock valuations, especially for tech companies where the market needs to discount back cash flows further out in the future. When the math is done to discount these cash flows back to today, a lower assumed discount rate leads to higher present values.\\nThe stock market overreacts to news, and big price drops can present good opportunities to buy high-quality stocks. Is now the time to buy Palantir? Find out by reading the original article on StockStory.\\nWhat is the market telling us:\\nPalantir\\'s shares are very volatile and over the last year have had 47 moves greater than 5%. In context of that, today\\'s move is indicating the market considers this news meaningful but not something that would fundamentally change its perception of the business. \\nThe previous big move we wrote about was 14 days ago, when the company gained 5.1% on the news that OpenAI\\'s CEO Sam Altman is leaving the company. According to the Board, Altman\\'s departure \"follows a deliberative review process by the board, which concluded that he was not consistently candid in his communications with the board, hindering its ability to exercise its responsibilities. The board no longer has confidence in his ability to continue leading OpenAI.\" As a result, President and co-founder Greg Brockman announced that he\\'s quitting OpenAI. \\nSubsequently, Microsoft (NASDAQ:MSFT) CEO Satya Nadella revealed that Altman and his former OpenAI co-founder, Greg Brockman, are joining Microsoft to lead a new advanced AI research team. This move signals Microsoft\\'s commitment to leading the rapidly growing AI market. The combination of Altman and Brockman\\'s expertise with Microsoft\\'s vast resources could create a formidable force in the AI space. This development shows that AI platforms could be potential acquisition targets of larger tech and internet companies that want to quickly add AI capabilities and talent.\\nPalantir is up 189% since the beginning of the year, but at $18.47 per share it is still trading 13.4% below its 52-week high of $21.34 from November 2023. Investors who bought $1,000 worth of Palantir\\'s shares at the IPO in September 2020 would now be looking at an investment worth $1,944.',\n",
       " \"In the swiftly evolving world of technology, HP Inc (NYSE:HPQ). stands as a notable entity in the IT Hardware and Communications Equipment sector. The company's strategic focus on the PC and printing markets positions it among the prominent players contending for market share in a competitive landscape. With divergent views from analysts, HP Inc.'s trajectory is under scrutiny, as market participants weigh the potential impacts of market trends and company initiatives on its performance.Company OverviewHP Inc., headquartered in Palo Alto, California, is a leading provider in the personal computing and printing industry. The company's product offerings span across a diverse range, including PCs, gaming systems, peripherals, and printers. HP Inc. is also known for its subscription-based models in the printing segment, aiming to navigate a path toward sustainable revenue streams.Market Performance and Competitive LandscapeThe company's stock has seen fluctuations in line with its market cap, which as of November 20, 2023, stood at approximately $27.66 billion. HP Inc. competes with giants such as Dell (NYSE:DELL), Lenovo, and Apple (NASDAQ:AAPL) in the PC market, while in the printer segment, its rivals include Canon and Epson. The competitive intensity is palpable, with HP Inc. making strategic moves to secure its position and drive growth.Strategic Direction and InnovationsAnalysts have highlighted HP Inc.'s efforts to integrate AI into PCs as a potential driver for revenue growth, with the expectation that AI integration could lead to increased average selling prices. The company is also focusing on structural improvements in the print segment, shifting towards subscriptions and profit-upfront models, which are anticipated to bolster operating profit growth.Financial Outlook and Analyst ProjectionsThere is a consensus among analysts that HP Inc. is expected to see a return to growth in the PC market by 2024, with replacement cycles and the end of Microsoft (NASDAQ:MSFT) Windows 10 support in October 2025 acting as catalysts. Despite this, there are concerns about the print segment, where the company faces challenges from generic competitors and slow growth in new markets like 3D printing.\\nThe company's financial execution has been strong, with revenue and earnings growth post-separation from Hewlett Packard Enterprise (NYSE:HPE). HP Inc. has increased its market share over time and has been recognized for its innovative product development.Analyst Ratings and Price TargetsThe ratings and price targets for HP Inc. have varied, with some firms maintaining a cautious stance due to anticipated weakness in the PC and Print end markets through the first half of 2024. On the other hand, upgrades to Buy ratings from other firms reflect a more optimistic view based on improvements in the PC ecosystem, cost reductions, and attractive valuations. Price targets from analysts range from $23 to $33, reflecting the mixed outlook on the company's future performance.Bear CaseWill HP Inc.'s market challenges lead to underperformance?Analysts have expressed concerns about the persistent weakness in the PC and Print end markets, which are expected to continue affecting HP Inc.'s performance at least through the first half of 2024. The company's underweight rating by some analysts is predicated on these market challenges, suggesting that there may be more downside potential relative to other stocks in the sector.Can competitive pressures in the Print sector hinder HP Inc.'s growth?The Print sector presents its own set of challenges for HP Inc., with competitive intensity and issues in supplies causing headwinds. Analysts worry that the company's growth expectations for Print hardware may be overly optimistic, and the operating margins in this segment are already at the high end of the new guidance range, which could make consistent profit growth difficult to achieve.Bull CaseHow could AI integration and PC market improvements boost HP Inc.'s revenues?Analysts are optimistic about the PC market's recovery, with AI integration expected to catalyze revenue growth. HP Inc.'s focus on AI could lead to increased average selling prices and drive a conservative long-term guide for the Personal Systems segment. Additionally, market share gains and positive supply chain commentary suggest an improving demand environment for PCs.What are the prospects for HP Inc.'s cost reductions and shareholder returns?Significant cost reductions are expected to support margin and earnings recovery for HP Inc. Analysts also anticipate higher free cash flow generation, which could result in increased share buybacks and enhanced shareholder returns. The company's undemanding valuations and prospects for higher free cash flow generation are seen as bullish indicators.SWOT AnalysisStrengths:\\n- Strong brand presence and diversified product portfolio.\\n- Innovative product development and market share growth.\\n- Strong financial execution with revenue and earnings growth.\\nWeaknesses:\\n- Expected continued weakness in key markets into the first half of 2024.\\n- Challenges in the printing business due to generic competitors.\\n- Competitive PC market with potential for continued weak demand.\\nOpportunities:\\n- AI integration in PCs could lead to increased average selling prices.\\n- Potential PC market growth with upcoming refresh cycles.\\n- Structural improvements in the print segment could drive operating profit growth.\\nThreats:\\n- Risks from major shareholders potentially selling their stake.\\n- Possible margin pressure from worsening macroeconomic conditions.\\n- Intense competition in both PC and Print sectors.Analysts Targets- Barclays Capital Inc.: Underweight with a price target of $23.00 (November 21, 2023).\\n- Citi Research: Buy rating with a target price of $33.00 (November 13, 2023).\\n- Morgan Stanley & Co. LLC: Equal-weight rating with a price target of $31.00 (October 11, 2023).\\nThe analysis spans from October to November 2023.InvestingPro InsightsAs investors consider the future of HP Inc. amidst a challenging market, real-time data from InvestingPro provides a deeper understanding of the company's current valuation and financial health. With a market capitalization of $28.52 billion and a trailing P/E ratio of 7.33, HP appears to offer value in terms of earnings. The adjusted P/E ratio suggests that the company is trading at a discount relative to its earnings over the last twelve months as of Q4 2023. Despite a decline in revenue growth of -14.71% over the same period, HP Inc. maintains a strong dividend yield of 3.82%, reflecting the company's commitment to returning value to shareholders.\\nTwo notable InvestingPro Tips highlight the company's financial prudence and investor appeal: HP Inc. has raised its dividend for 7 consecutive years and has maintained dividend payments for 53 consecutive years, underscoring its reliability in rewarding investors. Additionally, the company's valuation implies a strong free cash flow yield, which is a positive sign for investors looking for companies with the potential to generate cash and sustain dividends.\\nFor those seeking comprehensive analysis, InvestingPro offers additional insights with over 10 InvestingPro Tips for HP Inc., available at: https://www.investing.com/pro/HPQ. Subscribers can take advantage of the special Cyber Monday sale, with discounts of up to 60% and an additional 10% off on a 2-year InvestingPro+ subscription using the coupon code research23. This is an opportune moment to access valuable investment strategies and data to guide decision-making in the dynamic tech sector.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " \"In the ever-evolving landscape of technology, Microsoft Corporation (NASDAQ:MSFT) stands as a titan, continually adapting to the tides of change. With its roots deeply embedded in the software industry, the company has expanded its domain to include consumer electronics, personal computers, cloud computing, and most notably, artificial intelligence (AI). Analysts from Wall Street have been closely monitoring Microsoft's strategic maneuvers, particularly its foray into generative AI and cloud services, which are poised to redefine the company's growth trajectory.Market Performance and Product SegmentsMicrosoft has demonstrated robust financial health, with a strong balance sheet and positive revenue growth across its diverse segments. Its Intelligent Cloud division, especially Azure, has shown remarkable strength, outshining competitors in individual vendor growth. Analysts have noted that Microsoft's AI capabilities within Azure serve as a key differentiator in the competitive cloud market. The company's productivity segment, which includes the Microsoft Office suite, has also been a consistent performer, with new AI-driven features like M365 Copilot expected to contribute significantly to future revenue.Competitive Landscape and Market TrendsThe tech giant's strategic partnership with OpenAI has positioned Microsoft at the forefront of the AI revolution. This collaboration has led to the integration of cutting-edge technologies like ChatGPT into Microsoft's operations, potentially transforming the company's AI ambitions. Moreover, Microsoft's involvement with OpenAI has undergone strategic organizational changes, potentially favoring Microsoft in commercializing AI advancements.\\nThe company's move into AI is not without its challenges, though. Analysts have expressed concerns over the management of hypergrowth technologies and the fragile nature of OpenAI's nonprofit board structure, which could affect governance and stability. Nonetheless, Microsoft's long-term aspirations to control its AI destiny could have significant implications for its growth and industry positioning.Regulatory Environment and Customer BaseMicrosoft's customer base spans across large enterprises and individual consumers worldwide. The company's commitment to innovation and technology has secured a loyal following, with products like Azure and Office 365 being staples in the enterprise world. However, Microsoft's global reach also subjects it to various regulatory environments, which could impact its operations, especially as it delves deeper into AI and cloud computing.Management and StrategyUnder the leadership of CEO Satya Nadella, Microsoft has embraced a strategy that prioritizes cloud computing and AI. The company's executive decisions, such as the involvement in OpenAI's board changes, reflect a deliberate approach to secure a favorable position in the AI market. Microsoft's management has been commended for its efficient cost management, reflected in the company's robust operating income.Potential Impacts of External FactorsMicrosoft's journey is not without potential headwinds. Analysts have pointed out that macroeconomic pressures and a high-rate environment could pose risks to the company's performance. Moreover, the competitive landscape in cloud computing, with players like Amazon (NASDAQ:AMZN) and Google (NASDAQ:GOOGL), remains a battleground where Microsoft must continue to innovate to maintain its edge.Upcoming Product LaunchesThe company has several key product launches that have piqued the interest of investors. The M365 Copilot is set to become a pivotal product, with its general availability potentially accelerating Microsoft's revenue growth in AI. Additionally, the integration of ChatGPT and other AI technologies into Microsoft's product suite is expected to create new revenue streams and solidify its position in the AI space.Stock PerformanceIn terms of stock performance, Microsoft has maintained a resilient posture. While the exact current stock price is not provided, the company's market capitalization reflects its status as a heavyweight in the industry. The stock has received positive ratings from analysts, with price targets suggesting confidence in Microsoft's future prospects.Bear CaseIs Microsoft's AI strategy a risk to its cloud dominance?The company's aggressive push into AI, while seen as a growth driver, could also present risks. Analysts have noted that Microsoft's AI strategy needs careful management to avoid jeopardizing its cloud ambitions. The potential for AI to disrupt existing business models and introduce new competitive dynamics may require Microsoft to adapt quickly to maintain its cloud dominance.Can Microsoft navigate the complex AI regulatory landscape?As Microsoft deepens its involvement in AI, the company faces a complex regulatory landscape that could impact its operations. Analysts have raised concerns about the potential for increased scrutiny and regulation of AI technologies, which could pose challenges for Microsoft. Ensuring compliance and navigating these regulations will be critical for the company's continued success in AI.Bull CaseWill Microsoft's AI investments accelerate its growth?Analysts are bullish about Microsoft's strategic investments in AI, particularly through its partnership with OpenAI. The integration of AI technologies like ChatGPT is expected to drive significant growth for Microsoft, with the potential to reshape industries and create new market opportunities. The company's early move into AI is seen as a first-mover advantage that could capitalize on a rapidly expanding market.Can Microsoft's cloud services lead the AI revolution?Microsoft's Azure cloud platform is uniquely positioned to lead the AI revolution. With strong growth in the public cloud sector and Azure's AI capabilities, Microsoft is expected to outperform its peers. Analysts believe that the company's positive industry view and overweight stock rating suggest confidence in Microsoft's market position and future performance in the AI-driven cloud landscape.SWOT AnalysisStrengths:\\n- Diversified product range with strong positions in software, cloud services, and AI.\\n- Strategic partnership with OpenAI, positioning Microsoft at the forefront of AI innovation.\\n- Robust financial performance with positive revenue growth across segments.\\nWeaknesses:\\n- Potential risks associated with managing hypergrowth technologies like AI.\\n- Regulatory challenges that could impact AI operations and cloud services.\\nOpportunities:\\n- Upcoming product launches, such as M365 Copilot, expected to drive significant revenue growth.\\n- First-mover advantage in AI technology, with potential for rapid scaling and market leadership.\\nThreats:\\n- Competitive pressures in the cloud computing space from companies like Amazon and Google.\\n- Macroeconomic conditions that could affect enterprise IT spending and overall performance.Analysts Targets- D.A. Davidson & Co.: Buy rating with a price target of $415.00 (November 27, 2023).\\n- Piper Sandler: Overweight rating with a price target of $425.00 (November 21, 2023).\\n- Barclays: Overweight rating with a price target of $421.00 (November 16, 2023).\\n- BMO Capital Markets: Outperform rating with a price target of $400.00 (September 27, 2023).\\n- Evercore ISI: Outperform rating with a price target of $432.00 (November 20, 2023).\\n- HSBC Securities (USA) Inc.: Buy rating with a price target of $413.00 (October 26, 2023).\\nThe timeframe used for this article spans from September to November 2023.InvestingPro InsightsAs Microsoft continues to navigate the competitive and rapidly evolving tech landscape, real-time metrics and expert analysis become invaluable for investors looking to gauge the company's performance. According to InvestingPro data, Microsoft boasts a substantial market capitalization of $2.77 trillion, reflecting its dominant position in the market. The company's P/E ratio stands at 35.94, indicating investor confidence in its earnings potential despite a valuation that some might consider steep. This is further reinforced by Microsoft's revenue growth over the last twelve months as of Q1 2024, which was a solid 7.5%, showcasing the company's ability to increase its sales in a challenging economic climate.\\nInvestingPro Tips shed light on Microsoft's financial prudence and strategic strengths. The company has consistently rewarded its shareholders, raising its dividend for 18 consecutive years, and maintaining dividend payments for 21 years. This track record of returning value to stockholders is a testament to Microsoft's stable financial management and commitment to its investors. Moreover, Microsoft's ability to yield a high return on invested capital and on assets suggests operational efficiency and a strong competitive edge in the software industry.\\nFor investors looking to delve deeper into Microsoft's financial health and strategic positioning, there are additional InvestingPro Tips available, providing nuanced insights into the company's market performance and investment potential. Subscribers can unlock these insights on InvestingPro, which is currently offering a special Cyber Monday sale with discounts of up to 60%. Plus, by using the coupon code research23, investors can receive an extra 10% off a 2-year InvestingPro+ subscription, further enriching their investment toolkit with valuable data and expert analysis.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " 'By Akash Sriram (Reuters) -  SentinelOne  (NYSE:S) surged nearly 14% on Wednesday after a strong quarterly revenue forecast signaled its entry into the cybersecurity big leagues, challenging larger rivals like Microsoft (NASDAQ:MSFT) and CrowdStrike (NASDAQ:CRWD). The stock hit a more than one-year high and could add nearly $1 billion in value if gains hold, topping up a rise of about 37% notched for the year so far. SentinelOne\\'s consolidated platform for enterprise customers as well as the competitive advantages of its Data Lake product are among the top factors behind its growth and rising customer base, analysts have said. Its efforts to tap growing demand for security services focused on end-user devices such as laptops and smartphones are also paying off. \"We view SentinelOne as an emerging challenger in the endpoint security space, a prominent part of the cybersecurity stack that has been dominated by larger competitors such as Microsoft and CrowdStrike,\" Morningstar analysts said in a note.  SentinelOne has also rolled out products such as the generative AI-powered Purple AI and Singularity platform to help plug vulnerabilities that come with the rising digital presence of businesses.  \"We continue to win a significant majority of competitive evaluations against both next-gen and legacy endpoint providers,\" said CEO Tomer Weingarten (NYSE:WRI). D.A. Davidson analysts said Data Lake, a data collection and investigation product, offers notable cost savings and superior speed than rival Splunk (NASDAQ:SPLK), which is set to be bought by Cisco (NASDAQ:CSCO) for $28 billion.  At least eight brokerages raised their price targets on SentinelOne, with an average rating of \"buy\" and median target of $20, in line with closing prices before its results on Tuesday.  \\nSentinelOne forecast fourth-quarter revenue of $169 million and projected $616 million in annual sales, both surpassing estimates, according to LSEG data. The stock has a 12-month forward price-to-sales ratio - which measures valuation - of 7.7, compared with larger rival CrowdStrike\\'s 15.1 and 10.7 for Microsoft, whose operations also include endpoint security.',\n",
       " None,\n",
       " None,\n",
       " 'By Stephen Nellis (Reuters) - International Business Machines (NYSE:IBM) on Monday showed a new quantum computing chip and machine that it hopes will serve as the building blocks of much larger systems a decade from now. Researchers around the world are trying to perfect quantum computing, which relies on quantum mechanics to reach computing speeds far faster than classical silicon-based computers. The challenge has been to create quantum computers that are reliable enough in the real world to consistently beat conventional computers. Microsoft (NASDAQ:MSFT), Alphabet (NASDAQ:GOOGL)\\'s Google and China\\'s Baidu (NASDAQ:BIDU) , along with startups and nation states, are all racing to develop quantum machines. As quantum researchers have made the machines big enough to outpace classical computers, they have struggled with data errors. On Monday,  IBM  showed what it says is a new way of connecting chips together inside machines and then connecting machines together which, when combined with a new error-correction code, could produce compelling quantum machines by 2033. The first machine to use them is called Quantum (NASDAQ:QMCO) System Two, which uses three \"Heron\" chips. Dario Gil, IBM\\'s senior vice president and director of research, said that progress will appear fairly steady until 2029, when the full effect of the error-correction technologies come into play. After that, the machines should see a sharp uptick in capabilities, similar to how AI systems that developed slowly for the past 15 years became vastly more sophisticated over the past year. \\n\"You\\'re going to have to tie them together,\" Gil said of IBM\\'s newest chips. \"You\\'re going to have to do many of these things together to be practical about it. Because if not, it\\'s just a paper exercise.\" IBM is not the only quantum player targeting machines within the next few years. Startup PsiQuantum, which is working with GlobalFoundries (NASDAQ:GFS) to make its chips, told Reuters earlier this year it plans to have a commercial machine within six years.',\n",
       " \"As Oracle Corporation (NYSE:ORCL) navigates through a transformative phase, its aggressive push into cloud services has caught Wall Street's attention. Analysts collectively acknowledge the company's significant strides in the cloud sector, particularly in Oracle Cloud Infrastructure (OCI), and its potential to reshape Oracle's growth trajectory. This deep-dive analysis will explore Oracle's performance across different markets, product segments, and competitive landscape, offering insights into the company's strategy and future outlook.Company Profile and Financial HealthOracle, a technology behemoth founded in 1977 and headquartered in Redwood (NYSE:RWT) City, California, has been a cornerstone in the software industry. With a market capitalization hovering around $307 billion, the company's financial health appears robust. Oracle's diverse portfolio includes databases, cloud-engineered systems, and enterprise software products, with its services offered through both cloud-based and on-premise deployments.\\nThe company's stock has demonstrated resilience, with a lower-than-average price movement, and it sustains a dividend yield of 1.4%. Analysts note that Oracle's P/E ratios are expected to contract from 21.9x in 2023 to 17.5x in 2025, indicating a favorable earnings outlook. The integration of Cerner (NASDAQ:CERN), a recent acquisition, is anticipated to further bolster earnings growth, with Oracle's dividend outlook projected to rise.Cloud Business and Market PositionOracle's cloud business is a focal point of its growth strategy. Analysts predict strong cloud sales growth potential over the next several years, driven by high demand for cloud infrastructure services, particularly for AI model training. Oracle's OCI is competitive against industry giants like Azure, Google (NASDAQ:GOOGL) Cloud Platform (GCP), and Amazon (NASDAQ:AMZN) Web Services (AWS), offering potential savings of 20% to 40% over three years. The company regularly signs deals over $1 billion, with a recent infrastructure company proposing a $1.5 billion contract for AI training due to Oracle's competitive pricing.\\nDespite an uncertain macroeconomic environment, Oracle's financial guidance anticipates non-GAAP EPS growth from $5.47 in FY24 to $6.82 in FY26, on the back of consistent revenue growth. The company's valuation metrics, including a CY24E EV/revenue multiple of 6.7x, are in line with its large-cap technology peers.Competitive Landscape and Market TrendsOracle competes with industry leaders such as SAP, Amazon, Microsoft (NASDAQ:MSFT), Snowflake (NYSE:SNOW), and  MongoDB  (NASDAQ:MDB). The company's transition from licensing products to offering them on a subscription basis in the cloud is expected to improve profitability. Oracle's international revenues account for 45% of total revenues, and it maintains solid ratings from Standard & Poor's and Moody's (NYSE:MCO).\\nThe competitive landscape is evolving with the rise of generative AI, where Oracle has established partnerships with key players like Tesla (NASDAQ:TSLA), Nvidia (NASDAQ:NVDA), Meta (NASDAQ:META), and Google. The company's focus on AI is reshaping its growth trajectory, with partnerships with digital natives and AI innovators positioning Oracle as a significant player in this space.Regulatory Environment and Customer BaseOracle's customer base spans various sectors, with the company's products being integral to enterprise software solutions. The regulatory environment has been conducive to Oracle's growth, with no major impediments noted by analysts. The company's ability to navigate complex regulatory landscapes, particularly in international markets, contributes to its stable customer base.Management and StrategyOracle's management has set ambitious targets, aiming for $65 billion in revenue by FY26, with operating margins of 45% and over 10% annualized EPS growth. The company's strategy includes interoperability and minimizing integration costs, a shift from its previous approach that is seen as partner-friendly. Oracle's aggressive assumptions, particularly from net new customers, suggest a total addressable market (TAM) of over $250 billion for IaaS opportunities.Potential Impacts of External FactorsExternal factors such as economic uncertainty and supply constraints in AI chips and data center construction could impact Oracle's growth. The company's valuation concerns based on EV/free cash flow metrics compared to peers and the potential failure of cloud products to offset on-premise decline are risks highlighted by analysts.Upcoming Product Launches and Stock PerformanceOracle's upcoming product launches, particularly in cloud services, are expected to drive revenue growth. The company's stock performance has outpaced the S&P 500 and the Russell 3000, indicating strong investor confidence in Oracle's growth prospects.Bear CaseIs Oracle's cloud growth sustainable?Analysts express concerns regarding the sustainability of Oracle's cloud growth. Risks include the potential failure of cloud products to offset the decline in on-premise sales, loss of market share to competitors, and a reduction in overall IT spending. Economic uncertainty may impact business operations, and revenue recognition could lag behind order signings in new ventures like AI and Health sectors. Valuation concerns based on EV/free cash flow metrics compared to peers also pose a challenge.Can Oracle maintain its competitive edge?While Oracle has shown strong AI momentum with over $4 billion in signed contracts related to its Gen2 Cloud, there are uncertainties about maintaining high growth rates due to supply constraints. The aggressive TAM, particularly from net new customers, may be overly optimistic. Questions about the ability to accelerate revenue growth and the darker days of the economic downturn yet to come could also affect Oracle's competitive edge.Bull CaseWill Oracle's strategic partnerships drive growth?Oracle's expansion of its partnership with Microsoft and other tech giants emphasizes the importance of its technology being widely available. The company's strategy to become more partner-friendly and its interoperability approach may attract new customers seeking multi-cloud services. Large-scale contracts and customer engagements indicate robust business growth, with bullish projections on cloud revenue and the large TAM from Oracle's support base conversion and new infrastructure cloud customers.Is Oracle's AI focus a game-changer?Oracle's positioning to be a significant player in AI could reshape its growth trajectory over the next 3-5 years. The company's broadening investor appetite for its role in AI, strong partnerships with leading digital and AI companies, and positive adjustments in revenue estimates due to underlying AI momentum suggest a bullish outlook. Oracle's AI-driven growth strategy and robust partnerships, despite a weaker Q2 outlook, indicate long-term revenue targets that reflect optimism about the company's trajectory.SWOT AnalysisStrengths:\\n- Leader in enterprise software with a wide array of products.\\n- Strong cloud sales growth potential.\\n- Aggressive push into AI and cloud services.\\n- Solid partnerships with tech entities like Nvidia and Microsoft.\\nWeaknesses:\\n- Risks associated with the transition to cloud services.\\n- Potential decline in on-premise sales.\\n- Valuation concerns based on EV/free cash flow metrics.\\nOpportunities:\\n- Significant TAM for IaaS opportunities.\\n- Growth drivers from database or Oracle apps migration to OCI.\\n- Potential for increased profit margins following full Cerner integration.\\nThreats:\\n- Economic uncertainty impacting business operations.\\n- Competition from other cloud database vendors.\\n- Supply constraints in AI chips and data center construction.Analysts Targets- BMO Capital Markets Corp.: Market Perform rating with a price target of $130.00 (September 25, 2023).\\n- Piper Sandler: Overweight rating with a price target of $130.00 (September 22, 2023).\\n- Barclays Capital Inc.: Overweight rating with a price target of $147.00 (September 22, 2023).\\n- Evercore ISI: In Line rating with a price target of $131.00 (September 22, 2023).\\n- Deutsche Bank Securities Inc.: Buy rating with a price target of $135.00 (September 18, 2023).\\n- J.P. Morgan Securities LLC.: Neutral rating with a price target of $100.00 (September 13, 2023).\\nThis analysis is based on reports from September to December 2023.InvestingPro InsightsOracle Corporation's (NYSE:ORCL) recent performance and strategic moves have made it a standout in the software industry, not just for its products but also for its financial health and investment potential. According to InvestingPro data, Oracle has a robust market capitalization of $313.74 billion and is trading at a price-to-earnings (P/E) ratio of 33.13. Interestingly, the company's adjusted P/E ratio for the last twelve months as of Q1 2024 is 30.79, which reflects a more favorable earnings outlook when combined with a PEG ratio of 0.55, indicating potential for earnings growth relative to its share price.\\nOracle's commitment to shareholder returns is evident with its dividend track record, as one of the InvestingPro Tips highlights that the company has raised its dividend for 10 consecutive years. Moreover, the company has maintained dividend payments for 15 consecutive years, which showcases its financial stability and reliability as an income-generating investment.\\nThe company's financial health is further supported by its revenue growth of 15.41% over the last twelve months as of Q1 2024. This growth trajectory is in line with Oracle's strategic push into cloud services and AI, which are expected to be key drivers for the company's future expansion.\\nFor investors seeking more in-depth analysis and additional insights, there are 12 more InvestingPro Tips available on the platform, which can be accessed at https://www.investing.com/pro/ORCL. To take advantage of these insights, consider subscribing to InvestingPro, now available at a special Cyber Monday sale with discounts of up to 60%. Use the coupon code research23 to get an additional 10% off a 2-year InvestingPro+ subscription, and enrich your investment strategy with exclusive tips and data.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " 'Investing.com – U.S. stocks were lower after the close on Monday, as losses in the Technology, Basic Materials and Oil & Gas sectors led shares lower.\\nAt the close in NYSE, the Dow Jones Industrial Average fell 0.11%, while the S&P 500 index declined 0.54%, and the NASDAQ Composite index declined 0.84%.\\nThe best performers of the session on the Dow Jones Industrial Average were 3M Company (NYSE:MMM), which rose 3.61% or 3.60 points to trade at 103.45 at the close. Meanwhile, Merck & Company Inc (NYSE:MRK) added 1.56% or 1.61 points to end at 105.07 and  Nike  Inc (NYSE:NKE) was up 1.46% or 1.66 points to 115.14 in late trade.\\nThe worst performers of the session were Salesforce Inc (NYSE:CRM), which fell 3.59% or 9.33 points to trade at 250.67 at the close. Intel Corporation (NASDAQ:INTC) declined 3.18% or 1.39 points to end at 42.35 and Microsoft Corporation (NASDAQ:MSFT) was down 1.43% or 5.37 points to 369.14.\\nThe top performers on the S&P 500 were Bath & Body Works Inc. (NYSE:BBWI) which rose 8.91% to 36.91, IDEXX Laboratories Inc (NASDAQ:IDXX) which was up 7.05% to settle at 516.61 and  Norwegian Cruise Line Holdings Ltd  (NYSE:NCLH) which gained 6.64% to close at 17.51.\\nThe worst performers were  Alaska Air  Group Inc (NYSE:ALK) which was down 14.25% to 34.07 in late trade,  Albemarle  Corp (NYSE:ALB) which lost 4.96% to settle at 119.90 and Freeport-McMoran Copper & Gold Inc (NYSE:FCX) which was down 4.14% to 37.62 at the close.\\nThe top performers on the NASDAQ Composite were Hut 8 Mining Corp (NASDAQ:HUT) which rose 363.00% to 10.51, SilverSun Technologies Inc (NASDAQ:SSNT) which was up 240.33% to settle at 12.49 and  Hawaiian Holdings  Inc (NASDAQ:HA) which gained 192.59% to close at 14.22.\\nThe worst performers were  U Power  Ltd (NASDAQ:UCAR) which was down 55.59% to 0.68 in late trade, Meta Materials Inc (NASDAQ:MMAT) which lost 36.45% to settle at 0.07 and Conduit Pharmaceuticals Inc (NASDAQ:CDT) which was down 34.48% to 3.61 at the close.\\nRising stocks outnumbered declining ones on the New York Stock Exchange by 1470 to 1424 and 66 ended unchanged; on the Nasdaq Stock Exchange, 2083 rose and 1408 declined, while 125 ended unchanged.\\nShares in U Power Ltd (NASDAQ:UCAR) fell to all time lows; down 55.59% or 0.86 to 0.68. Shares in SilverSun Technologies Inc (NASDAQ:SSNT) rose to 52-week highs; up 240.33% or 8.82 to 12.49. Shares in Meta Materials Inc (NASDAQ:MMAT) fell to 5-year lows; falling 36.45% or 0.04 to 0.07. Shares in Hawaiian Holdings Inc (NASDAQ:HA) rose to 52-week highs; rising 192.59% or 9.36 to 14.22. \\nThe CBOE Volatility Index, which measures the implied volatility of S&P 500 options, was up 3.56% to 13.08.\\nGold Futures for February delivery was down 1.96% or 40.90 to $2,048.80 a troy ounce. Elsewhere in commodities trading, Crude oil for delivery in January fell 1.07% or 0.79 to hit $73.28 a barrel, while the February Brent oil contract fell 0.84% or 0.66 to trade at $78.22 a barrel.\\nEUR/USD was unchanged 0.42% to 1.08, while USD/JPY rose 0.26% to 147.20.\\nThe US Dollar Index Futures was up 0.38% at 103.59.',\n",
       " 'By Gloria Dickie, Elizabeth Piper and Alexander Cornwell DUBAI (Reuters) -The United Arab Emirates and several charities at the U.N. climate summit on Sunday offered $777 million in financing for eradicating neglected tropical diseases that are expected to worsen as temperatures climb. Climate-related factors \"have become one of the greatest threats to human health in the 21st century\", COP28 President Sultan Ahmed Al-Jaber said in a statement.  The pledges, made as the COP28 summit on Sunday focused on climate-related health risks, included $100 million from the UAE and another $100 million from the Bill and Melinda Gates Foundation. Others to announce funds for climate-related health issues included Belgium, Germany and the U.S. Agency for International Development.  The World Bank launched a program to explore possible support measures for public health in developing countries, where climate-related health risks are especially high. The burden of tropical diseases will worsen as the world warms, along with other climate-driven health threats including malnutrition, malaria, diarrhoea and heat stress. Many tropical diseases are already easy to treat. River blindness and sleeping sickness, for example, are both endemic to Africa and spread through parasitic worms and flies that are likely to proliferate in a warming world. More than 120 countries have signed a COP28 declaration acknowledging their responsibility to keep people safe amid global warming.  The declaration made no mention of fossil fuels, the main source of climate-warming emissions, which the Global Climate and Health Alliance called a \"glaring omission\".  Activists including physicians in white coats held a small demonstration on Sunday within the COP28 compound to raise awareness of the issue.  \"We are in a lot of trouble,\" said Joseph Vipond, an emergency physician from Alberta, Canada. He recalled the case of a child dying from an asthma attack made worse by smoke inhalation from Western Canada\\'s record wildfires this year. \"This is having real world impacts.\"  Climate change is also increasing the frequency of dangerous storms and more erratic rainfall.  In September Storm Daniel killed more than 11,000 people in Libya, and last year\\'s massive flooding in Pakistan fueled a 400% increase in malaria cases across the country, according to the World Health Organization.  Earlier on Sunday, Microsoft (NASDAQ:MSFT) co-founder turned philanthropist Bill Gates said scientists were working on new treatments for and prevention of mosquito-spread malaria as the rise in temperatures creates more hospitable habitat for the insects to breed.  \"We have new tools at the lab level that decimate mosquito populations,\" said Gates, whose foundation supports public health research and projects for the developing world.  \"These new innovations give us a chance, at a reasonable cost, to make progress.\"  Former U.S. Secretary of State Hillary Clinton also spoke on Sunday, urging reform to the world\\'s insurance system as another key requirement to keep people safe.  \"Right now insurance companies are pulling out of so many places, they\\'re not insuring homes, they\\'re not insuring businesses,\" Clinton said, addressing a panel on women and climate resiliency.  \"It\\'s people everywhere who are going to be left out with no backup, no insurance for their business or their home.\" \\n___ For daily comprehensive coverage on COP28 in your inbox, sign up for the Reuters Sustainable  Switch  (NYSE:SWCH) newsletter here.',\n",
       " 'DUBAI (Reuters) - After two days of back-to-back speeches by world leaders, the COP28 climate summit turns its attention on Sunday to the reality of climate change fuelling more sickness and disease.  It will be the first time that the annual U.N. talks feature public health on the agenda.  With malnutrition, malaria, diarrhoea and heat stress all on the rise - and threatening to stretch already-struggling health services - countries and businesses both are anxious for more ways to protect people as temperatures continue to climb for decades.  The health-themed day will draw a cast of headliners, including Microsoft (NASDAQ:MSFT) co-founder and philanthropist Bill Gates, who is expected to join the United Arab Emirates in launching a climate health initiative.  Former U.S. Secretary of State Hillary Clinton will also visit the sprawling COP28 compound, where more than 70,000 people from across the world have gathered for the two-week conference.  Clinton was due to take part in an event on women and climate change.  \\nCOP28 will also see an appearance from former U.S. Vice President Al Gore, who shared the 2007 Nobel Peace Prize with the U.N.\\'s Intergovernmental Panel on Climate Change for their work to increase public knowledge about global warming.  On Saturday, current U.S. Vice President Kamala Harris sought to promote Washington\\'s global climate leadership, saying her country had once again become \"a global leader in the fight against the climate crisis\".',\n",
       " \"Quiver Quantitative - Google (NASDAQ:GOOGL) (GOOG) has recently urged the UK's Competition and Markets Authority (CMA) to investigate Microsoft (NASDAQ:MSFT)'s cloud computing practices, alleging anti-competitive behavior that stifles the market. This request emerges amid broader scrutiny of the cloud computing industry dominated by Microsoft (MSFT) and Amazon (NASDAQ:AMZN) in various global regions, including the UK, European Union, and the United States. Google's push for regulatory intervention was detailed in a letter to the CMA, which Reuters obtained.\\nThe letter highlights Google's concerns about Microsoft's licensing practices, which they claim unfairly limit competition and customer choice in the UK's cloud computing sector. According to Google, Microsoft's practices effectively compel customers to rely on Azure, Microsoft's cloud service, even if they prefer the offerings of competitors like Amazon Web Services (AWS) or Google Cloud. This situation, Google argues, directly harms customers and represents a significant obstacle to market competition. In response to these allegations, the CMA has remained silent, choosing not to comment publicly.\\nWhile Microsoft has attempted to address these concerns by updating its licensing rules to promote competition, rivals, including Google, have found these changes insufficient. A Microsoft spokesperson defended the company's efforts, noting that over 100 independent cloud providers have embraced their revised policies and that competition in the cloud hyperscaler market remains robust. Despite these claims, Google Cloud Vice President Amit Zavery criticizes Microsoft's approach, advocating for a multi-cloud strategy that allows customers greater freedom to move between different cloud services according to their needs.\\nThe core of the dispute centers around Microsoft's policy updates related to using their software licenses, like Windows, in the cloud. These updates have seemingly made it more expensive for customers to use Google (GOOGL) or AWS instead of Azure. Google argues that while AWS, the market leader, doesn't impose similar constraints, Microsoft's licensing restrictions prevent fair competition. Google's recommendations to the CMA include enforcing Microsoft to enhance interoperability for customers using Azure alongside other services and prohibiting Microsoft from withholding security updates from clients switching to other providers.\\nThis article was originally published on Quiver Quantitative\",\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'Business intelligence software company, Domo (NASDAQ:DOMO), Inc., reported Q3 FY 2024 earnings, exceeding expectations with key top-line metrics. The company\\'s shift towards a consumption-based pricing model and the introduction of a freemium model have been instrumental in driving user growth and adoption of premium features. Domo also announced plans to refinance and extend the maturity of its outstanding debt.\\nKey takeaways from the call:Domo achieved its highest operating income of $5 million and an operating margin of 6%.Over 20% of Domo\\'s Annual Recurring Revenue (ARR) is now on the consumption model, with plans to shift the majority of revenue to this model by the end of next year.The company has launched a freemium model, which has resulted in customers signing up for paid deals within a short sales cycle.Domo has made architectural changes to allow customers to keep data and associated querying and processing with partners like Snowflake (NYSE:SNOW), AWS, and Microsoft (NASDAQ:MSFT).The company reported Q3 billings of $74.8 million, a 1% YoY increase, and total revenue of $79.7 million, also a 1% YoY increase. Subscription revenue accounted for 89% of total revenue and grew 3% YoY.Domo expects a non-GAAP net loss per share of $0.05 to $0.09 for Q4, and $0.24 to $0.28 for the full year.The company is in the process of refinancing its debt to reduce interest rates and extend maturity.Domo\\'s consumption model, which allows customers to pay based on usage rather than a fixed seat-based model, has seen success, winning new customers and upselling existing ones. The company reported examples of a Canadian retailer that switched to Domo to consolidate data, a healthcare software company that tripled its contract size after adopting Domo\\'s consumption model, and an educational software company that chose Domo for its ETL needs and embedded analytics.\\nThe company\\'s freemium model, Domo Everywhere, provides data to partners\\' customers and turns data provision into a profit center. The company is confident that this model will continue to drive adoption and revenue growth.\\nDomo also reported cost optimization measures, including a reduction of its workforce by about 7%. The company aims to achieve positive operating cash flow for FY \\'24 and free cash flow positive for FY \\'25. Domo (NASDAQ: DOMO) has engaged an investment bank to help with refinancing and extending the maturity of its outstanding debt, with significant interest from potential lenders.\\nThe company resolved several at-risk enterprise accounts and expects a recovery in the next three to four quarters. Domo believes it is making the right moves for long-term growth and feels better positioned with its customers. The shift to consumption has been positive, allowing the company to upsell and save accounts. The architectural change allows Domo to keep computing charges and credits when partnering with other companies.InvestingPro InsightsDomo, Inc.\\'s latest earnings report has demonstrated a strategic pivot that\\'s garnering attention, with their shift to a consumption-based pricing model and the launch of a freemium service. In light of these developments, let\\'s delve into some key metrics and tips from InvestingPro that could provide a deeper understanding of the company\\'s financial health and market position.\\nInvestingPro Data shows Domo\\'s market capitalization stands at $342.43M, reflecting the market\\'s current valuation of the company. Despite an impressive gross profit margin of 76.47% over the last twelve months as of Q2 2024, the company\\'s P/E ratio is -4.39, indicating that investors are expecting negative earnings. This aligns with the company\\'s current trajectory, as it is not anticipated to be profitable this year. Additionally, the revenue growth has been reported at 11.47% for the last twelve months as of Q2 2024, which, while positive, shows a deceleration when compared to previous periods.\\nAn InvestingPro Tip that stands out is the concern over Domo\\'s ability to meet short-term obligations, as its liquid assets fall short of its immediate liabilities. This could be a crucial factor for investors to consider, especially in the context of the company\\'s plans to refinance its debt. Moreover, with two analysts revising their earnings downwards for the upcoming period, there is a notable caution in the market sentiment surrounding Domo\\'s future performance.\\nFor readers interested in a more comprehensive analysis, there are additional InvestingPro Tips available, which can shed light on aspects such as stock price volatility, the increase in total debt over consecutive years, and a strong return over the last month. Domo\\'s strategic moves and the market\\'s response highlight the importance of staying informed with real-time data and expert insights.\\nInvestingPro subscribers can access these insights and more, and with the special Cyber Monday sale, now is an opportune time to consider a subscription with discounts of up to 60%. Plus, use the coupon code sfy23 to get an extra 10% off a 2-year InvestingPro+ subscription.Full transcript -  Domo Inc  (DOMO) Q3 2024:Operator: Good day, everyone, and welcome to the Domo Third Quarter Fiscal Year 2024 Earnings Call. Today\\'s call is being recorded. All lines have been placed on mute to prevent any background noise. After the speakers\\' remarks, there will be a question-and-answer session. [Operator Instructions] I would now like to turn the call over to Peter Lowry, Vice President, Investor Relations. Please go ahead, sir.\\nPeter Lowry: Good afternoon, and welcome. On the call today, we have Josh James, our Founder and CEO, and David Jolley, our Chief Financial Officer. I\\'ll lead off with our safe harbor statement and then on to the call. Our press release was issued after the market close and is posted on the Investor Relations section of our website, where this call is also being webcast. Statements made on this call include forward-looking statements related to our business under federal securities laws. These statements are subject to a variety of risks, uncertainties, and assumptions. These include, but are not limited to, statements about future and prospects or financial projections and cash position; statements regarding the potential of our consumption-based pricing; statements about our sales team and technology, our expectations for new business opportunities, transactions and initiatives; statements regarding our channel of communications and upcoming events; statements regarding the potential of artificial intelligence and its impact on our business; and statements regarding the impact of macroeconomic and other conditions on our business. For discussion of these risks and uncertainties, please refer to documents we file with the SEC, in particular, today\\'s press release, our most recently filed annual report on Form 10-K and our most recently filed quarterly report on Form 10-Q. These documents contain and identify important risk factors and other information that may cause our actual results to differ materially from those contained in our forward-looking statements. In addition, during today\\'s call, we will discuss non-GAAP financial measures, which we believe are useful as supplemental measures of Domo\\'s performance. Other than revenue, unless otherwise stated, we will be discussing our results of operations on a non-GAAP basis. These non-GAAP measures should be considered in addition to and not as a substitute for or in isolation from our GAAP results. Please refer to the tables in our earnings press release for a reconciliation of our non-GAAP financial measures to their most directly comparable GAAP measure, which we have posted to the Investor Relations section of our website at domoinvestors.com. With that, I\\'ll turn it over to Josh. Josh?\\nJosh James: Thank you, Pete, and thank you everyone for joining the call today. In Q3, we were able to exceed guidance for our key top-line metrics, including revenue, subscription revenue, and billings. A highlight in the quarter is that we had our highest operating income in history of $5 million, and our highest operating margin in history of 6%. Over the past few years, and especially the last few quarters, we have been incubating critical pivots that are finally coming together. They are clear and powerful priorities that are removing friction and strengthening our ability to deliver unmatched value to the market. Specifically, several years ago, we decided to test an idea: \"What would customers do if they had unlimited access to features for an unlimited number of users and all visualization for free?\" It was a simple value prop to customers. Pay for the value you are realizing. Well, after positive feedback we decided to run an even broader pilot last year, and the pilot proved to be a smash hit. We now feel like we\\'ve reached critical mass with over 20% of our ARR on the consumption model. As we continue to look at the results from this very large sample size, we feel very confident in making the decision and saying we\\'re going all in on consumption. By the end of next year, we expect to have the vast majority of our revenue on the consumption model. Again, we now have over 400 customers on consumption contracts, representing over 15% of our customer base and over 20% of our ARR. When customers move to consumption, we are seeing user counts growing at almost 3 times the speed of seat-based customers. And we are seeing 3 times the adoption rate on premium features like data science. We\\'ve also rolled out reporting so our customers can see in real time what their consumption patterns are. So far, even with the highest usage our customers are seeing, the feedback has been incredibly positive because customers recognize the value of that usage. As an example, it took us eight years at a fast food chain to get an ARR of about $200,000. Now that they\\'ve converted to consumption, this company has committed to an ARR of over $550,000 over the next three years by expanding the use of Domo across the organization. One fun story to relate that has happened to me on multiple occasions over the last few months is watching how our customers react to the new model. I\\'ve seen the epiphanies go off in their eyes as they recognize and then look at people internally in the room and tell them conclusively that because they now have unlimited users, they can start looking at sunsetting all of their other BI technologies and legacy reporting tools and use Domo instead. Well, I couldn\\'t have said it better myself. In support of our consumption strategy and to pave a completely open path to adoption, we\\'ve also launched a new freemium model. Freemium was impossible before our consumption model and gives everyone a risk-free opportunity to get in and try Domo with no obligations and no restrictions. Domo customers have access to all the Domo features with unlimited users and they can tackle any use case they want. And when they want to go bigger, they can click from within Domo to buy consumption credits and have an unlimited highway to multiply their impact on their business. This approach seamlessly aligns with our core philosophy of delivering value before requiring payment, reinforcing our commitment to providing accessible and valuable solutions to our users. We rolled out our free offering last month and will be rapidly iterating on it over the next quarter to focus on user experience and easy onboarding. We think that long term, this alters our ability to attract new customers and give them a friction-free path to move through the pipeline from free to paid usage to sharing with more users to broad use case adoption. Of course, this naturally leads to expanding credits and being ready for a long-term relationship with our sales and support organization. This new flow evolves us from having to work with cold leads to being able to talk with happy customers who already see the value of the platform and are ready to lean in more. To demonstrate the power of having a freemium model, let me share a story from two weeks ago. Our sales team had been calling a CTO prospect for over a year with no luck. One of our salespeople called the CTO on a Friday afternoon and left a message about freemium and the free credits it comes with. The CTO proceeded to sign up for a free instance, and over the weekend, multiple users connected to disparate data sources and built data flows powering over 40 reports. By Monday, the CTO was in love with the product and signed up for a three-year deal with a total contract value well into the six figures. And that was a three-day sales cycle. Our freemium product also completely changes the conversation with potential partners who have wanted to leverage our Domo Everywhere product to deliver data experiences through Domo for their own customers. In the past, if we had a customer with, say, 20,000 external end users, it would have required a major upfront investment, which often led our customer to reduce the use case to maybe just the top 5% of external customers. With freemium, however, we can give all 20,000 of those users a free instance of Domo immediately at no cost. This creates a very meaningful introduction to Domo for those end users with an obvious upgrade experience because they can experience the value and immediately expand and put more of their own data in our platform. In a consumption world, focusing on adoption through product-led growth and support programs is the critical path to success for both customers and for Domo. Increased adoption leads to happier and more successful customers, and the corollary is, of course, increased revenue. As we roll out features and training that support adoption, we\\'ve seen our customers rapidly expand their usage of our platform compared to when they were under seat-based pricing. For example, one of our largest customers had been a customer for six years. In those first six years, they had grown to 3,500 active users and 17 departments. Then, they converted to our consumption model. The growth was rapid. In just one additional year, they added 2,300 more active users and more than doubled the number of departments and use cases. This has dramatically increased the return for the customer and, of course, has strengthened our relationship in the account. In support of our shift to the consumption model, focusing on our customers\\' adoption of our platform brings complete alignment between us and our customers around multiplying value. It\\'s all about opening up unlimited use cases to address a completely expanding list of customer needs. And it helps us learn more about what drives customer success. For example, which product features and functionality in our products really drive expanded usage? What of our support behaviors drive additional adoption of our products? It shifts the dynamic from trying to sell the customer more products to helping them find more ways to receive value. Now, the progress we\\'ve made with our consumption model and with our launch of freemium has dramatically altered our ability to be successful within the ecosystem and our partners. Only recently, we\\'ve changed our architecture to allow Domo to drive consumption for partners like Snowflake, AWS, and Microsoft. Before now, we\\'ve had conflict in the channel, where we sometimes drove consumption or compute away from our vendors. With the architecture changes, we now allow customers to choose to keep the data and the associated querying and processing of data with our partners. It was a substantial investment on our part, but we are very excited that this has all been reconciled. Because of these changes, we\\'ll be making some announcements soon, describing partnerships where customers are able to retire spend by purchasing Domo through various app stores and marketplaces from major tech players. As it relates to AI, this is another area where consumption allows our customers to get in and start seeing the value of AI in their business without an upfront commitment or investment. As mentioned earlier, we\\'ve seen dramatically higher uptake in our data science offerings among our consumption customers compared to our seat-based customers, and we expect to see similar levels of adoption as we continue to expand our AI service layer and other AI offerings. The consumption model will expose many more customers to AI because they don\\'t have to sign a contract before they start using it. This in turn, of course, drives consumption. We have several AI-related product launches lined up for the coming months that will help our customers build reports and interact with their data in a ChatGPT-like fashion. Now, to illustrate the impact of this new model that has already penetrated over 20% of our ARR, please let me share some real-life examples from some of our customers. So first, a significant new logo win with a Canadian retailer that was using competing BI solutions was having issues with silo data and with connecting to data in disparate systems. The company chose Domo for our consumption model, which made it easy for them to sunset legacy seat-based tools when they weren\\'t sure they were getting the value that they needed. We are starting to see more and more of these cases, and it\\'s certainly good to be the consolidator. A healthcare software company was using our Domo Everywhere solution to provide data to their medical customers. The company was adding new Domo Everywhere customers at a faster rate than expected and it was challenging under a seat-based model where they had to commit to their investment before receiving the value. Since transferring over to consumption now, our customer has tripled their contract size with us, and yes, that math works. An educational software company was debating which vendor to use for their ETL needs. They entered into an upsell contract with Domo, not only because of our ETL features, but because our consumption contract structure allowed them to predict their cost with a high level of confidence. Additionally, the company had been considering using Domo Everywhere to provide embedded analytics to thousands of their end users. Moving to a consumption model, opened the door for them to test out our Domo Everywhere experience in a very cost-effective manner. And then, because of the value they\\'ve seen in the Domo platform, this customer has committed to dramatically alter the scale of their investment and agreed to a two-year six-figure ARR contract in Q3 with a significant upsell built into the second year. Is consumption driving adoption? It certainly looks like it. Another example is a financial services company that purchased Domo to consolidate data from multiple loan origination systems. The consumption model was key to their decision to go with Domo because it unlocked our data science and sandbox features, which were critical to their use case and would have been outside their budget under our seat-based model. Does access to all of Domo help customers unlock the value of the entire platform and become more committed to the entirety of our platform? I think so. A digital customer engagement platform company has been a Domo customer for a decade. The initial use case was for sales and marketing analytics. However, about a year ago, the company was considering a cancellation because they felt like they were paying too much per seat for about 350 users. What saved the account was a move to consumption with unlimited users. Using our product, they created an app, and because they have unlimited users, they were able to deploy the app company-wide, and now have over a thousand users on Domo\\'s platform. Not only did we save an account that was going to cancel, several months after they converted to consumption, they actually committed to an increased level of consumption and upped their spend with us. Now, would we have been able to save this customer without consumption? No. And now we have an upsell. Looking outside of Q3, here\\'s a few more examples of how consumption is changing the game for us. A healthcare analytics company, which is using several of our larger competitors, is looking to replace some business objects and other legacy providers. Consumption is allowing the company to replace the other vendors and expand Domo without the friction of a new contract discussion. Can Domo benefit from vendor consolidation? Yes, we can. Another small highlight is a digital asset company that was about to cancel because they thought we were too expensive for the number of users they had in the account on seat-based pricing. They moved to a consumption contract with unlimited users. Additionally, they agreed to triple their contract size, and then, just last week, agreed to triple it again. So, they are now close to 10x their original size, as opposed to just recently being on the brink of canceling. Do I wish that all my customers were on consumption contracts? Yes, I do. Lastly, an insurance company that was paying us $200,000 a year moved to consumption because our seat-based model didn\\'t allow them to expand as rapidly as they wanted to. With the initial move to consumption, they increased their contract with us by over $100,000. 15 months later, after they had been able to fully realize the value Domo can provide due to having unlimited users and functionality, they added another $200,000 annually to their contract to replace their spend with Cognos. So, in totality, I think these are some great examples of multiple customers that highlight the progress we are making as a company. Now, while we are marching toward improving the prospects of the company, we are also optimizing our costs so we can operate as efficiently as possible. To that end, we\\'ve made changes that impacts approximately 7% of our workforce, as well as reductions in contractors, marketing programs, and other expenses. We are cognizant of the effect this has on people and would like to take a moment to express our gratitude to everyone for their contributions. Now, as we look to the future, I\\'m sure you can feel my energy around these pivots we\\'re making and the signals we\\'re seeing from customers and partners that resoundingly convince us that they\\'re the right moves. Even when we were growing 30% quarter-over-quarter -- year-over-year, I wasn\\'t this optimistic about our future and our stability as I am now. We\\'re quickly migrating to the consumption model. In Q4, we\\'ll have the vast majority of our new logo customers on consumption and we will continue to encourage our existing customers to switch to consumption, resulting in the vast majority of our ARR transition to the consumption model within the next year. Freemium, our adoption programs, and our AI investments will continue to bolster our efforts in moving to consumption where customers are able to experience value and see the vision of what Domo can mean to their company before having to pay and commit to everything. All of these changes will also lead to a dramatic shift in our approach and success with partners and the broader ecosystem over the next 12 months, which should meaningfully impact our ability to generate leads efficiently. Domo is becoming a much more interesting company with prospects that excite our broader team. And with that, I\\'ll now turn it over to David.\\nDavid Jolley: Thanks, Josh. I love those examples. Like you, I\\'m excited about our key areas of focus and believe we\\'re really well positioned to execute on the opportunities in front of us. Now, while we aspire to higher growth rates than we\\'re currently experiencing, I\\'m pleased that we were able to exceed the billings guidance that we provided at the beginning of the quarter. We delivered Q3 billings of $74.8 million, a year-over-year increase of 1%. Total revenue was $79.7 million, also a year-over-year increase of 1%. Subscription revenue represented 89% of our total revenue and grew... [Technical Difficulty]\\nOperator: One moment, everyone, while we reconnect the speaker line. Please stand by, and do not disconnect. Once again, everyone, please stand by. Once again, everyone, we are reconnecting the speaker line. Please stand by.\\nDavid Jolley: All right, are we back live again?\\nOperator: You are live. Please go ahead.\\nDavid Jolley: All right, very good. Sorry for the short delay. But thanks, Josh. I appreciate that and appreciate those great examples. Like you, I\\'m excited about our key areas of focus and believe we\\'re well positioned to execute on the opportunities in front of us. Now, while we aspire to higher growth rates than we\\'re currently experiencing, I\\'m pleased that we were able to exceed the billings guidance that we provided at the beginning of the quarter. We delivered Q3 billings of $74.8 million, a year-over-year increase of 1%. Total revenue was $79.7 million, also a year-over-year increase of 1%. Subscription revenue represented 89% of our total revenue and grew 3% year-over-year. And our ARR grew roughly in-line with subscription revenue growth. In reviewing the metrics that will impact the remainder of the year, our current RPO was $230.8 million, consistent with last year. And our total RPO grew 4% to $367.2 million as of October 31. On a dollar-weighted measure, we continue to have approximately two-thirds of our customers in our multi-year contracts. Our gross retention was above 85%, and net retention was about 95%. Last quarter, we identified potential renewal challenges with several large customers. And while we and some of our customers continue to face challenging IT spending environment, in Q3, these renewals discussions played out somewhat better than expected, which did help our results. In regards to the large renewal risks that we had identified last quarter, we have saved a few of them and have not identified any beyond those that we had identified in last quarter for the fourth quarter. Moving on to margins and profitability. Our subscription gross margin was 84.8%, up 0.2 percentage points from Q3 of last year. And non-GAAP operating margin was a record high 6.3%, up 5.4 percentage points from a year ago. Our net loss was very close to breakeven at $24,000, which is our best result to date, and a big improvement from a net loss of $4.4 million a year ago. Net loss per share was $0, based on 36.3 million weighted average shares outstanding, basic and diluted. In Q3, cash used in operations was approximately $4.3 million. We capitalized approximately $2 million of software costs, resulting in a decline of our cash balance of $6.5 million from last quarter to $57.4 million. Cash flow from operations in Q3 was negatively impacted by the timing of collections on some receivables. However, we\\'re still on track to generate positive operating cash flow for FY \\'24, and therefore, expect our Q4 cash flow from operations to be in the range of $3 million to $4 million. Looking forward to next year, we\\'re committed to not only being operating cash flow positive, but we are targeting free cash flow positive for FY \\'25. In order to bring our cost structure in alignment with this target, we recently reduced our headcount-related expense by approximately 7% and also optimize a handful of other costs. For Q4 top-line metrics, we\\'re guiding to a billing range of $102 million to $103 million, and expect GAAP revenue to be in the range of $79 million to $80 million. For the full year of fiscal \\'24, we expect billings to be in the range of $317.7 million to $318.7 million, and we expect GAAP revenue to be in the range of $317.8 million to $318.8 million, representing year-over-year growth of approximately 3%. We expect non-GAAP net loss per share, basic and diluted, of $0.05 to $0.09 for Q4. This assumes a 36.8 million weighted average shares outstanding, basic and diluted. For the full year, we expect non-GAAP net loss per share, basic and diluted, of $0.24 to $0.28. This assumes 36.1 million weighted average shares outstanding, basic and diluted. Additionally of note is the fact that we\\'ve engaged an investment bank to assist us with refinancing and extending the maturity of our outstanding debt. And at this point in the process, we have a significant level of interest from potential lenders. In conclusion, we posted better-than-expected top-line results with record profitability and believe we\\'re making the right moves to drive long-term profitable growth. With that, we\\'ll open the call for questions. Operator?\\nOperator: Thank you. [Operator Instructions] We\\'ll take our first question from Eric Martinuzzi with Lake Street Capital Markets. Please go ahead.\\nEric Martinuzzi: Yeah, congrats on the good numbers for the quarter, and I appreciate the examples regarding the capacity-based pricing impacts. I wanted to understand a little bit more on the risk of non-renewals. I think last quarter, you talked about four or five enterprise accounts that were in danger and that was part of the reset to the outlook for FY \\'24. Can you give us a little better color? Have we reached resolution on those four or five at-risk enterprise accounts?\\nJosh James: Yes, we\\'ve reached resolution. Good news, we were actually able to keep a couple of them just with down sales, but we still kept them as customers. So that was a little bit of a bright spot when it came to the bad news. And this quarter, we\\'ve -- given the guidance, we\\'re obviously not -- we\\'re not on a toward pace here, but we, at the same time, feel pretty good about looking out over the next three, four quarters in terms of the pacing of where customers are that are at risk. It feels like we hit the bottom of that, and we\\'re recovering from that. And like we mentioned, many of the examples with the consumption pricing, we actually end up on the consolidation, being the consolidator side of the equation versus having just being impacted by others. So, the move to consumption definitely changed the relationship with a lot of our customers and has helped us save a bunch of accounts. And we think especially as that plays out over the future, like we talked about, there\\'s so many upsells that we\\'re getting from these accounts. If you look at the cohort of customers that have been through renewal, we haven\\'t lost any customers that have signed up to consumption. And it\\'s a smaller sample size, 30 to 40, but as that number gets bigger, we\\'ll keep watching that. But it certainly is extremely encouraging looking at the 20% of our business that\\'s purely consumption and knowing that we can get that number to a vast majority just over the next 12 months.\\nEric Martinuzzi: Okay. The other comment that you made last quarter was regarding the pipeline. And you characterized the pipeline back then as soft in all stages. I\\'m wondering if you could update that view on the pipeline.\\nJosh James: Yeah, it feels -- as we look at the numbers, it seems like we\\'ve started to turn. There\\'s seven, eight, nine numbers that all feel like they\\'ve barely started to turn, but it\\'s barely. But all of our checks, it looks like things are -- hit the bottom last quarter and just are starting to slightly improve. So hopefully that\\'s how things play out. But we\\'re feeling like we have our arms wrapped around the situation much better and we feel like we\\'re in a much better position in terms of the relationship that we have with our customers and our ability to sell consumption, our ability to get our customers over to consumption, training up the reps, training up the client services folks, focusing on adoption and helping people find these additional use cases. So, we feel like we\\'re much better positioned and feel like we\\'ve got much better visibility into the customers and the contracts at this stage.\\nEric Martinuzzi: Understand. Good luck in Q4.\\nJosh James: Thank you very much.\\nOperator: [Operator Instructions] We\\'ll take our next question from Oliver Crookenden with JMP Securities.\\nPat Walravens: Actually, it\\'s Pat, but -- Pat Walravens with JMP Securities. Thank you. So, Josh, I do love the shift to consumption, and we\\'ve seen a lot of other people do it, but I was wondering if you could balance it out a little bit. I mean, there are some negatives to the consumption model, too, right? So, what do you give up when you make this shift?\\nJosh James: Yeah, we -- I think if you went around the room and e-staff and tried to figure out what the negatives are, I don\\'t -- we\\'re not seeing any negatives. The one difference in the model is you\\'re not going to sign up any seven-figure new logo deals, right? When you go start to use AWS, you don\\'t walk in and be like, \"Hey, give me a couple of million bucks worth.\" You try it out and you start spending it and if you like it, you decide that you want to commit to get lower rates, and we\\'re seeing that same thing. So, brand new seven-figure buildings walking in the door, we\\'re not going to have as many of those. They\\'ll happen, but they\\'ll happen as those customers grow. So, we\\'re seeing those relationships. We have some really big customers that are signing up right now that on the old seat model, we\\'d be signing up for $3 million, $4 million, but -- annually. But in the consumption model, you sign them up for $400,000 and then another $500,000, and then you get to a couple of million bucks. And you still get to the same spot. You get there more efficiently, more effectively. There\\'s not as many -- not as much hemming and hawing. You\\'re not going through as many use cases and approvals internally, but at the same time, you also don\\'t have the big billings hits until they\\'ve had time to bake. So, we\\'ll have to wait for some of those things to bake a little bit.\\nPat Walravens: Okay, great.\\nDavid Jolley: I think, Pat, another -- just another comment was that I think early on there was a concern, well, geez, if we\\'re giving them whole platform, is there going to be anything to sell them later on? And there was some concern about that. But the way that\\'s working is when we provide the whole platform and open up all the seats, it\\'s then just about helping the customer identify how to solve more of their data issues and more use cases. And as they do that, that\\'s the upsell. It happens very naturally.\\nPat Walravens: Okay. So, there\\'s not a near-term hit on cash flow, like you don\\'t get less cash upfront when you go to a consumption model? I mean, maybe not...\\nDavid Jolley: No. I mean still AIA...\\nJosh James: subscription.\\nDavid Jolley: Yeah, subscription AIA. So...\\nPat Walravens: Okay. Perfect. And then, my second question is, Josh, if you could go a little deeper -- actually, I\\'m able to tell you my other two question, I\\'ll put them up out front. So, for Josh, if you could go a little deeper on the architectural change and help us understand the history of that, what was it before and what is it now and how does it work? And then, maybe if you guys could address the debt in a little more detail, just sort of what\\'s the current rate, when\\'s the current -- when\\'s the maturity and what\\'s your options look like? Both of those things would be really helpful. Thank you.\\nJosh James: Yeah. So, the problem before, when we were starting to get excited about the ecosystem and we were building relationships with some folks, especially the big data providers, we\\'d walk into accounts with reps, and then their customer that they introduced us to might start pulling some compute out of our partner and putting it into Domo. And of course, that\\'s the end of those relationships, right? You\\'re taking dollars out of the pockets of the reps of the partners. And so that was DOA as soon as that started. And so, what we\\'ve done now is we said, okay, we can take that compute component, where the data lives and how it gets queried and processed, and we can run that inside Snowflake or inside AWS or inside Microsoft. So, any partner that brings us in, we can keep all the computing charges and credits that are being used up actually with the partner. And so that changes the dynamic pretty meaningfully because now we have complete alignment except for instead of 40 users that you may have at -- even a big Fortune 500 company, from 40 users or 100 users to 5,000, 10,000, when you add the Domo front-end on top of that. And that\\'s 5,000 and 10,000 people that are now querying and running reports that are all driving compute on the back-end of a Snowflake or a Databricks or a Microsoft or an AWS or Google (NASDAQ:GOOGL). So, we feel like we\\'re in a much, much better position. And that coupled with premium, there\\'s been plenty of conversations where we\\'re talking to a partner and someone that maybe we have 300 customers that have connected to their data. So, we know it\\'s popular inside our network and we know that we provide a lot of visibility into the data that\\'s inside that partner\\'s analytics and the partner will come to us and say, \"Gosh, it\\'d be great if we could give this to the other 10,000 customers that we have.\" And then we say, \"Yeah, great. You should do that. It\\'s, let\\'s see, $840,000.\" And the partners like, \"What? Well, we don\\'t know the values yet.\" We\\'re like, \"Yeah. But we will give you upside if they converted to the Domo customers.\" They\\'re like, \"Yeah, that seems like a good idea on paper, but I don\\'t know if we can lean in and make the commitment, make the investment without seeing a return. So, maybe we should just try it out with a couple of customers.\" And by then, you\\'ve lost momentum. Whereas now, we can go in that same customer, that same conversation, we say, \"Yes, we can get it to all 10,000. Let\\'s do it on freemium. Let\\'s build some quick start guides so that the second you roll this out, the customer is able to log into Domo, see all of the data from you as a partner. We\\'ll build out the dashboards. We\\'ll build out the alerts.\" They\\'re going to get this great white-glove, perfect experience replicated 10,000 times automatically. And in there, they can see, \"Hey, this is your Domo experience for this data from your partner. If you want it for everything else, here\\'s your freemium account, just keep going.\" And then, we tell the partner, \"Any upside, we\\'re going to give you guys a piece of it.\" So all of a sudden, providing data to their customers becomes not a cost center, it becomes a profit center. And we\\'ve seen a bunch of traction in just even the last month for that. So, we\\'re really excited about our ability to provide Domo Everywhere through our freemium offering.\\nPat Walravens: Great. And then, Dave, on the debt?\\nDavid Jolley: Yeah. So, our current debt, as you might recall, we raised that debt even prior to our IPO when we were still using a lot -- consuming a lot of cash. And so, it\\'s very expensive debt. It carries a cash interest component and a PIK interest component, some other payments that push the effective interest rate north of 14% is where we\\'re at today. And it\\'s got a maturity of April \\'25. And so, we\\'ve gone out with a refinancing and got a bank helping us, and we\\'re looking at cash interest component of a little over 11% right now based on where SOFR is at. We\\'re hearing some good things about where rates might go next year, but it\\'ll bring our rate down substantially and push our maturity date out to \\'29 or \\'30. So, it sort of puts it out well out into the future and it eliminates some of the other PIK interest and some of the other elements.\\nPat Walravens: All right, terrific. Thanks to both of you.\\nOperator: One moment everyone, the speaker line has disconnected. One moment we reestablish the audio. Once again, everyone, please stand by. We\\'ll establish that audio line momentarily. Please stay on the line. Once again everyone, you are on hold for the Domo, Inc. conference call. We are establishing the speaker line. Please stand by. You are now live.\\nJosh James: All right. Sounds like Pat said thanks, we heard, and there was another question. Is the question still on?\\nOperator: Yes. One moment. We\\'ll take our next question from Derrick Wood with T.D. Cowen.\\nUnidentified Analyst: Hey, guys. Thanks. This is Cole on for Derek. On the RIF that you talked about, we\\'d just like to get a little bit more color. Is that across sales, G&A? If you can just unpack that a bit, that\\'d be helpful.\\nJosh James: Yeah, we -- it was across every department. There was -- the majority of it was in sales. We\\'re in terms of growth, not where we want to be. And so for the most part, it was based on performance. Some of it was just positions that were eliminated, as we found a more optimized way of accomplishing certain things. It wasn\\'t a huge number, but it was still several dozen humans that were affected. So, a little bit of a rough day here at the office, but at the same time, I feel like the company is in a better position. And it\\'s not a dramatic effect on our ability to produce. We think in many cases, actually, taking a smaller number of leads distributed -- I mean, taking the same number of leads distributed to a smaller number of reps will actually be an improvement for the reps that are here to make sure that they\\'re being fed. So, overall, I don\\'t think it\\'s going to be too impactful to our company. It just, obviously, impacts the folks that were affected by it.\\nUnidentified Analyst: Sounds good. Helpful. Just building on that too, for the reps that you still have at Domo, how is productivity trending? Any new initiatives around helping them sell consumption better would be helpful to hear about things.\\nJosh James: Yeah, we have all kinds of initiatives around helping them to consumption. We had a Board meeting just recently and walked through our consumption deck and showed the Board members all the positive things that are happening and all the negative things that we might be able to find as well. And the resounding answer was -- from the Board was, \"Move as fast as you possibly can. We don\\'t see anything that would cause us concern about you moving as fast as you possibly can.\" So, to that end, doing as many things as we can to drive the transition to consumption, things like we\\'ve built out a brand new adoption group that\\'s focused on going into our -- especially our bigger customers, and helping them, it\\'s a great phone call. We call a customer, and we\\'re not asking for a new contract. We\\'re not asking to go get approval from procurement. We\\'re just walking in saying, \"Hey, we\\'d love to come in and show you best uses for data science,\" or \"We\\'d love to come in and talk to you about identifying opportunities inside your marketing spend.\" And they love those conversations. All we\\'re doing is walking in and helping them, but we\\'re helping them identify additional use cases, which, of course, ends up driving consumption. So those are the types of things that we\\'re doing with the customer. And then, we\\'re re-evaluating the different departments that we have and what each group does. So, reps, their job on renewal. What\\'s what is the job on renewal? Is the job to go and procure another contract, or is the job to help identify some more use cases? And so, just identifying those different ways that we can interact with our customers to help get them excited about adding additional users and adding additional use cases. It\\'s really fun for the customer, and it\\'s really fun for us. They end up happier. They end up spending more. And those are the types of things that we can really do to address transitions to consumption. There are some customers that are on seat-based licenses with us, and they don\\'t have access to all the features. We have new features rolling out all the time, and if you want to use those features, then you need to be on consumption. And so, as we go out and market the different product offerings that we have and the new product offerings that we have, those all lead to additional consumption conversation. So, we\\'re definitely laser-focused on that. We\\'ve seen this 20% cohort of consumption that\\'s in our business today. And it looks better than every other 20% cohort that you could find. So, we\\'re going to do anything and everything we can to get the entire business moved over to that because just everyone ends up happier on the consumption model.\\nUnidentified Analyst: Appreciate the color. Thanks.\\nJosh James: Yeah, you bet. Thank you.\\nOperator: Thank you. We\\'ll take our next question from Sanjit Singh with Morgan Stanley.\\nSanjit Singh: Yeah, thank you for squeezing me in. And sorry, I\\'ve been toggling between multiple calls, so if I\\'m repeating a question, I apologize upfront. But David, just given the -- sort of the refinancing of the debt and sort of the higher rate environment and given the sort of actions you guys are doing today on cost, to what extent could debt pay down be part of like the capital allocation strategy, given that the budget environment is still pretty constrained right now? How do you look at debt pay down a potential lever for increasing sort of the equity value?\\nDavid Jolley: Yeah. I mean the good thing is even sort of as is with a very modest growth expectation for next year, I think we\\'re free cash flow positive, which puts us in a great position. So, if we\\'re able to accomplish some of the things that Josh talked about in accelerating our shift to consumption, and if we get any sort of help from the macro environment, then we\\'re into producing some meaningful cash flow that then could be used to potentially retire some debt. As you know, there\\'s usually some penalties when you pay it down in the first year. So, we\\'ll look at that. But I think it\\'ll be -- probably will be a lot better position as we move into some of those succeeding years to start reducing that well in advance of maturity, certainly.\\nSanjit Singh: Great. I appreciate the thoughts. And then, Josh, on -- I was just kind of coming out of the AWS conference, and when I hear about people\\'s data strategies, one of the bigger themes is sort of English being the new programming language and English being the new SQL. And just want to get a sense of how you guys are sort of abstracting away kind of traditional BI type user interfaces to more of that generative AI modality where users are just sort of using natural language, English, to get the insights that they need from Domo?\\nJosh James: Yeah, I mean it\\'s terribly exciting because one of the most challenging things in business intelligence or in leveraging the data that you have in your company is just getting access to it. And the big part of getting access to it is connecting to it, all of the ETL that needs to go into it. And you\\'re right, that\\'s where these antiquated languages used to be a big part of it. And going forward, it is about English. And we have a bunch of stuff that we\\'ve already incorporated into our products, a bunch of AI that we\\'ve incorporated into our product, and a bunch of generative AI that we\\'re going to be incorporating over the next few months into exploring the data, into building cards, into sharing data, having a data set and having AI suggest to you what you should look at, what format it should look like, being able to pick and choose from things that it suggests to you, fully formed reports. And so, it\\'s going to dramatically alter the landscape. It\\'s going to dramatically alter the interactions and the benefits that the customers get. So, we\\'re really excited about that. We feel like we\\'re extremely well positioned. When you look at the data that powers AI, it all depends on how organized that data is. And if your data is organized like a bunch of trash, then that\\'s exactly what you\\'re going to get back. When you use Domo, you actually not only organize your data, but you have a bunch of metadata about that data. And that\\'s what helps AI really come up with great conclusions when it actually has an indication of what type of data is sitting there because in the data world it could be anything. It could be unstructured, messy data, and it might be data that\\'s coming from another data warehouse. You actually don\\'t know the root source of it, and so you need to be able to have the metadata around that and have data organized in a way. And that\\'s one of the things that we really do with our customers, is help them organize that so that they can take advantage of all the different technologies and innovations that are rapidly coming out from AI. So, we feel like we\\'re extremely well positioned for that. And that\\'s another big part of why we want to be in the middle of consumption, because again, if you\\'re using consumption, anything that is in our product, you can try out. It\\'s going to cost you a buck to try it out. It\\'s not a $50,000 commitment and you\\'ve got to buy a couple seats. It\\'s just go and click on it, see what it does, see if it produces something that\\'s effective. And then, if it does, of course, you\\'re going to start using it more. And because you already know it\\'s effective, even though you\\'re using it more, you\\'re getting a bill for it, you don\\'t care because you already proved the value. So that\\'s just another reason why consumption is such an important part of this, where we see 2x, 3x usage of the additional features and functionality we have in our product when it\\'s a consumption customer, because they\\'re able to try it out and prove that it works. So, we\\'re really excited about AI and the way it\\'s going to impact our business, broadly speaking, and feel like we\\'re really well positioned to take advantage of that.\\nSanjit Singh: Appreciate the thoughts, Josh. Thank you.\\nJosh James: You bet. Thanks.\\nOperator: And that does conclude today\\'s presentation. Thank you for your participation today, and you may now disconnect.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.',\n",
       " \"Pure Storage (NASDAQ:NYSE:PSTG) reported robust financial results for Q3 of fiscal year 2024, with a 13% YoY increase in revenue to $763 million and an operating profit of $169 million. The company saw significant demand for its offerings, specifically its Evergreen//One Storage as a Service consumption offering, which more than doubled YoY. Despite a challenging macro environment, Pure Storage remains optimistic about gaining market share.\\nKey takeaways from the earning call:\\n- Pure Storage's Evergreen//One offering saw robust demand, doubling YoY, contributing to the company's strong financial performance.\\n- The company achieved a new industry standard by offering eight service level agreement guarantees.\\n- Portworx, the company's container storage software, had a record quarter.\\n- Pure Storage expects the strong demand for its Evergreen//One offering to continue in Q4, impacting near-term revenue.\\n- The company's annual revenue outlook for FY 2024 is $2.82 billion, growing 2.5%, with Q4 revenue expected to be $782 million, declining 3.5%.\\n- Pure Storage reported strong gross margin performance of 74% in Q3, with product gross margin at 73.1% and subscription services gross margin at 75.4%.\\n- The company's headcount increased slightly to around 5,500 employees, and it plans to add to its sales force in the coming year.\\n- Pure Storage had $1.35 billion in cash and investments at the end of Q3, with cash flow from operations at $158 million and capital expenditures totaling $45 million.\\n- The company repurchased over 630,000 shares of stock in Q2, returning over $22 million to shareholders.\\nDuring the earnings call, Pure Storage executives highlighted the strong growth of their Evergreen//One program and their success in the AI segment. They also discussed a $40 million telco order for 5G applications and expressed their intention to be aggressive in pricing and gain market share in the lower price performance tiers of the disk market. Despite a tough competitive environment, the company's win rates against competitors remain steady. \\nThe company's Evergreen//One contracts typically have a duration of around 3.5 years and are consumption-based. These contracts are considered minimums, with the potential for additional consumption and contract extensions. Pure Storage's strategy of providing a unified storage platform based on a cloud operating model continues to drive its success. The company expressed confidence in their strategy and thanked their stakeholders for their support.InvestingPro InsightsPure Storage's (NASDAQ:PSTG) latest earnings report paints a picture of a company on the rise, with a particular spotlight on its Evergreen//One Storage as a Service offering. To further understand the financial health and market position of Pure Storage, a glance at InvestingPro data and tips can provide additional context.\\nInvestingPro Data shows that Pure Storage holds a market capitalization of $10.39 billion, reflecting its substantial presence in the industry. The company's P/E ratio stands at 155.2, which is adjusted to 124.21 for the last twelve months as of Q3 2024, indicating investor expectations of future earnings growth. This is further corroborated by a PEG ratio of 0.39 for the same period, suggesting that Pure Storage's earnings growth rate is favorable when pegged against its P/E ratio.\\nFrom a profitability standpoint, Pure Storage has a gross profit margin of 70.64% for the last twelve months as of Q3 2024, which is quite robust and aligns with the strong gross margin performance reported in their Q3 earnings. This level of profitability is a testament to the company's operational efficiency and pricing power in its market segment.\\nInvestingPro Tips highlight that Pure Storage is trading at a low P/E ratio relative to near-term earnings growth, which could signal an attractive investment opportunity for those who believe in the company's growth trajectory. Additionally, it's worth noting that Pure Storage holds more cash than debt on its balance sheet, providing financial flexibility and a cushion against market volatility.\\nFor readers who are considering an investment in Pure Storage or simply wish to stay informed about the company's financials, InvestingPro offers a comprehensive suite of additional tips. There are 15 more InvestingPro Tips available, which can be accessed by subscribers, providing a deep dive into the company's financials and market position.\\nInvestingPro subscription is currently on a special Cyber Monday sale, offering a discount of up to 60%. To sweeten the deal, users can use the coupon code sfy23 to get an additional 10% off a 2-year InvestingPro+ subscription. This opportunity could be particularly timely for those who value in-depth financial analysis and real-time data to inform their investment decisions.Full transcript -  Pure Storage Inc  (PSTG) Q3 2024:Operator: Good day, and welcome to the Pure Storage Third Quarter Fiscal Year 2024 Earnings Conference Call. Today's conference is being recorded. [Operator Instructions] At this time, I'd like to turn the call over to Paul Ziots, Vice President of Investor Relations. Please go ahead.\\nPaul Ziots: Thank you. Good afternoon, everyone, and welcome to Pure's third quarter fiscal 2024 earnings conference call. On the call, we have Charlie Giancarlo, Chief Executive Officer; Kevan Krysler, Chief Financial Officer; and Rob Lee, Chief Technology Officer. Following Charlie's and Kevin's prepared remarks, we will take questions. Our press release was issued after close of market and is posted on our website where this call is being simultaneously webcast. The slides that accompany this webcast can be downloaded at investor.purestorage.com. On this call today, we will be making forward-looking statements, which are subject to various risks and uncertainties. These include statements regarding our financial outlook and operations, our strategy, technology and its advantages, our current and new product offerings and competitive industry and economic trends. Any forward-looking statements that we make are based on facts and assumptions as of today, and we undertake no obligation to update them. Our actual results may differ materially from the results forecasted, and reported results should not be considered as an indication of future performance. A discussion of some of the risks and uncertainties related to our business is contained in our filings with the SEC, and we refer you to these public filings. During this call, all financial metrics and associated growth rates are non-GAAP measures other than revenue remaining performance obligations or RPO and cash and investments. Reconciliations to the most directly comparable GAAP measures are provided in our earnings press release and slides. This call is being broadcast live on the Pure Storage Investor Relations website and is being recorded for playback purposes. An archive of the webcast will be available on the IR website and is the property of Pure Storage. Our fourth quarter fiscal '24 quiet period begins at the close of business, Friday, January 19, 2024. With that, I'll turn it over to Charlie.\\nCharlie Giancarlo: Good afternoon, everyone, and welcome to our Q3 earnings call. We are pleased with Pure's Q3 financial results. We saw healthy demand for our portfolio throughout the quarter, proving that Pure's platform strategy and vision is resonating with our customers. The Pure Storage platform provides customers the ability to deploy a consistent software and management environment across the full price performance range for block, file and object workloads, for both traditional and cloud-native applications. The Pure platform guarantees customers never experienced change management downtime. It guarantees the lowest space power and e-waste in the industry and provides infrastructure and services that always get better with age without disruption. The Pure Storage platform also promises a cloud operating model for our customers, enabling them to manage their data storage like the cloud providers to reduce their storage costs in the cloud to provide tailored storage services to their developers like the cloud and to consume storage like the cloud as a service. This model is gaining traction with leading customers. Evergreen//One, our Storage as a Service consumption offering saw continued extraordinary growth, more than doubling year-over-year. Evergreen//One and Evergreen/Flex are our preferred services for providing customers data storage on a consumption basis. Although we continue to offer customers the choice of consuming storage as CapEx, we believe the continued high demand for Evergreen//One is being driven by our sales activities, new customer buying behavior, and the current macro environment. Customers are attracted to the ability to manage and consume Evergreen storage as a cloud service as they need it. But with the low cost advanced capabilities and data security of on-prem storage. The outperformance of Evergreen//One this year has been significantly above our prior expectations and we now expect this strong level of demand to continue through Q4. While this success is a long anticipated and welcome expansion of our business model, its overperformance will have an effect on near-term revenue, which Kevan will cover in detail. In Q3, Pure set a new industry standard with eight service level agreement or SLA guarantees across our Evergreen services. With the innovative new paid power and rack program, we pay our Evergreen customers power and rack space costs. This first-of-a-kind program is enabled by our ability to deliver the cloud attributes that customers desire with the most energy efficient and reliable storage technology. Paid power and rack directly addresses the increasing cost of electricity and data center space. Additional SLAs introduced in Q3 and guarantee no change related downtime, no future data migrations for hardware replacements, upgrades or expansions and zero data loss. Portworx, our Kubernetes and container storage software for cloud native applications also had a record quarter. Portworx was recently named a leader in the inaugural IDC Marketscape for container data management. We saw increased multiyear renewals from existing customers and new customers deploying the Portworx suite of products for multi-cloud databases, messaging and logging systems. Portworx was also selected by a leading global retailer to provide a single integrated platform for their machine learning researchers and analysts ensuring consistent models across multiple clouds. A large international government authority is supporting their artificial intelligence and machine learning environments with Portworx deployed in mission-critical cloud-based applications. Customer momentum in the field of artificial intelligence also continues for Pure with a double-digit number of AI wins in the quarter across our portfolio. This included era technology and AI-based decision intelligence and automation platform that chose our Portworx solution for seamless cloud integration, outperforming competitive solutions by 200%. And Olympus adopted Pure's AI-ready infrastructure based on a FlashBlade and NVIDIA (NASDAQ:NVDA) solution for a new AI development environment, ensuring performance and capacity for large-scale models to accelerate their success with transformative AI solutions. We remain our customers' preferred partner for AI deployments and have strengthened our innovation and leadership by earning NVIDIA-based pod certification. Aerie, our complete AI-ready infrastructure built on the NVIDIA DGX base pod reference architecture and the latest FlashBlade//S storage platform makes AI scaling and deployments faster and easier for our customers. We continue to see strong demand for FlashBlade//E. And this month, we announced the general availability of FlashArray//E and shipped our first orders. This past quarter, we announced our latest Gartner accolade, a leader in the Magic Quadrant for distributed file systems and object storage, and it's now three years running. This marks the tenth successive year overall that our leadership has been acknowledged by Gartner in transforming the storage industry. The early success of our E family reinforces our belief that flash is replacing this enlarging Pure's opportunity in enterprise and eventually cloud storage. In Q3, Pure scored another major win for new 5G infrastructure to be deployed in our fiscal 2025. Pure's continuing success in the 5G space is based on our superior performance, reliability, density, longevity and advanced remote management capabilities. We are seeing strong early customer interest in our expanded partnership with Microsoft (NASDAQ:MSFT) and the Pure Cloud Block store integration with Azure VMware (NYSE:VMW) Solution, also known as AVS. Pure and Microsoft announced the public preview of this service this quarter. CVS and AVS are providing customers the opportunity to reduce their cloud storage spend by half or more while providing them the advanced services that they experience with Pure's enterprise systems. Customers are also enthusiastic about managing their storage and data across data centers and clouds in a consistent hybrid environment. Despite the uncertainties of the current business environment, Pure superior low total cost of ownership and Evergreen offerings are making a difference in this challenging IT economy. We are seeing a strong positive response to our position of having a consistent, unified flash platform for all storage needs from the data center to the cloud. This position is enabling us to compete for ever larger footprints in large enterprise accounts. This, coupled with the strong overall demand for our platform gives me the confidence in our continued ability to take share and outpace the market. With that, I'll turn it over to Kevan for further commentary.\\nKevan Krysler: Thank you, Charlie. We are pleased with our financial results this quarter. We delivered strong revenue and operating profit, both above our prior guidance for the quarter. Revenue was $763 million, up 13% year-over-year, and operating profit was $169 million. The macro spending environment remains relatively consistent to what we have seen throughout this year. However, demand for our solutions was robust throughout Q3. Our business strategy continues to focus on transitioning our offerings for our customers to consumption and subscription services. These types of offerings provide more value to our customers, which is resonating as customer demand for our consumption and subscription-based offerings across our Evergreen portfolio, especially Evergreen//One and Portworx is very strong. We achieved another quarter of record sales of our FlashBlade portfolio, including FlashBlade//E, which has quickly grown to become a meaningful portion of our FlashBlade business since becoming generally available in late April. We are also seeing customers consuming our FlashBlade technology, including FlashBlade as a service, leveraging our Evergreen//One Storage as a Service offering. Our operating profit of $169 million in Q3 exceeded expectations. Our differentiated flash management technology powered by purity software operating natively with raw flash continues to be a key driver of our strong product and subscription gross margins. As we mentioned last quarter, the majority of our capacity shipped is based on QLC raw flash. Remaining Performance Obligations or RPO was very strong and exceeds $2 billion growing 30%. During Q3, we closed a large non-cancellable product order with a large telco customer totaling $41 million that is included in the RPO balance at the end of Q3. Based on the timing of the shipment schedule for this order, product revenue is not expected until next year, which impacts our revenue outlook this year, which I'll discuss further in my remarks when updating annual guidance. When excluding both the noncancelable product order from RPO in the current quarter and the outstanding commitment in Q3 of last year with one of our global system integrators, RPO growth for our subscription services was also very strong at 29%. As we mentioned last quarter, the outstanding commitment with our global system integrator was fully satisfied during Q1 of this year. In Q3, subscription services annual recurring revenue grew 26% year-over-year to nearly $1.3 billion, highlighting the strong traction for our consumption and subscription-based service offerings. As we have mentioned previously, closed Evergreen//One contracts where the effective service date has not started are excluded from the subscription services ARR calculation including closed Evergreen//One contracts where the service date has not yet started, subscription services ARR was also strong, growing 27%. Subscription services revenue of $310 million comprised 41% of total revenue. U.S. revenue for Q3 was $536 million and international revenue was $227 million. Our new customer acquisition grew sequentially as we acquired 353 new customers during the quarter. As previously mentioned, we were pleased with our strong gross margin performance of 74%. Product gross margin was 73.1%, and subscription services gross margin was 75.4%. Our headcount increased slightly to approximately 5,500 employees at the end of the quarter. Pure's balance sheet and liquidity remains very strong, including $1.35 billion in cash and investments at the end of Q3. Cash flow from operations during the quarter was $158 million and capital expenditures totaled $45 million. In Q2, we repurchased over 630,000 shares of stock, returning over $22 million to our shareholders. Consistent with our remarks last quarter, our share repurchases represent a lower level of repurchase activity as a result of the fixed trading parameters that were in place throughout the quarter. We have approximately $167 million remaining on our existing $250 million repurchase authorization. Now turning to our updated annual guidance for FY '24. A key assumption used to derive our FY '24 annual revenue guide at the beginning of the year was that the macro environment would not meaningfully improve or deteriorate throughout the year. This assumption is holding as the spending environment continues to be challenging. Though despite these challenges, we are seeing increasing demand in the second half of the year across our data storage platform especially for consumption and subscription service offerings. Although we expect the demand to increase for the second half of the year, there are two important factors that are impacting our annual revenue expectation this year, which we now expect to be $2.82 billion, growing 2.5% and Q4 revenue is expected to be $782 million, declining 3.5%. First is the impact of our Evergreen//One Storage as a Service momentum, which will be discussed in more detail. And second is the impact of a $41 million non-canceled product order with a telco customer that is not expected to be fulfilled until next year. Both factors on a combined basis represent approximately 4.5 points of incremental headwind when compared to the annual revenue guide we provided at the beginning of the year. We are very pleased with the momentum and growth of our Evergreen//One service offering, while appreciating that this momentum creates a short-term impact on revenue growth. Last quarter, we stated that sales of our Evergreen//One service offering was expected to create 1 to 2 points of headwind to the annual revenue guide we provided at the beginning of the year. Based on our Evergreen//One sales in Q3 and the opportunities in our seasonally largest quarter Q4, we now expect that annual sales of our Evergreen//One and Evergreen//Flex offerings will more than double this year, reaching nearly $400 million and expect the impact will now create 3 points of headwind to the annual guide we provided at the beginning of the year. When excluding the impacts of the increased shift to our Evergreen//One offering and a $41 million order with a telco customer, our annual revenue growth would have been 7% when compared against the annual revenue guide we provided at the beginning of the year. We expect that our consumption and subscription business models will drive improved long-term growth for Pure as our subscription and consumption business continues to grow, we will provide additional business metrics that will help measure the health of our business. This includes translating growth rates of our subscription and consumption service offerings to a growth rate under our traditional model of a CapEx sale. Finally, we are increasing our annual operating margin guidance from 15.5% to 16% driven by our continued operational discipline and gross margin strength. Q4 operating margin is expected to be approximately 19%. In closing, we are pleased with the strength and demand across our entire Pure Storage platform, including our expectation of more than doubling sales this year of our combined Evergreen//One Storage as a Service and Evergreen//Flex offerings. We could not be more excited with how our solutions resonate with our customers, delivering a consistent, nondisruptive operating and management environment, leveraging the most advanced flash technology. Our innovation across our storage platform also extends to our Evergreen business models, providing customers with increasing flexibility and business value. With that, I will turn it back to Paul for Q&A.\\nPaul Ziots: Thanks, Kevan. [Operator Instructions] Operator, let's get started.\\nOperator: Thank you. [Operator Instructions] The first question comes from the line of Amit Daryanani of Evercore. You may proceed.\\nAmit Daryanani: Yes. Thanks for taking my question. Good afternoon, everyone. I guess maybe to start the discussion, there's obviously a lot of focus on the Jan quarter guide and the delta that's there versus seasonality versus what you expected 90 days ago. I realize there are a fair number of cross currents out there. But to the extent you can maybe talk about how much of the shortfall versus your expectations 90 days ago, let's say, is micro versus macro. I'd love to get a sense if you think anything has shifted from Pure's competitive advantage of your positioning that's impacting it. And I realize Evergreen//One is a big part of it. I'd love to kind of understand why do you think customers are massively increasing the shift towards Evergreen//One versus buying storage in a traditional manner. Thank you.\\nCharlie Giancarlo: Thanks Amit. Yes. Well, you nailed it, Amit, the shift to the consumption model has just continued to be very, very strong. And as you know, this is a model that we've been investing in for well over five years. It's one that we had staked a lot of our anticipation in and moving forward with, but this year has just been very strong. And one could say that it's the macro that has driven greater interest in a consumption model, I think, for obvious reasons, for the customer. But even more than that, we've invested in it heavily to make it not just a subscription and therefore, an easier bite, if you will, for economically difficult times. But to make it a true cloud service where they manage it like the cloud, it is hands-off. And now we're even paying for their power and rack space so that it's - other than the fact that it sits on their premise, it's a true cloud service. So the uptake in this, especially in these more difficult challenging times has been tremendous. So the two are tightly aligned. The macro overall certainly hasn't helped. Better macro would have lifted all boats. But really, the change as you put it, it was driven primarily by the shift to subscription and consumption model.\\nKevan Krysler: Yes. Thanks, Charlie. This is Kevan. And how're you doing Amit? If you take a step back and really look at the change in our annual guide as well as the impact to Q4, it really does come down to two key factors that we've described. One is what Charlie has highlighted, which is the strength and momentum of Evergreen//One. And then the second, obviously, is the telco order. That was all product that's having an impact on Q4 as well.\\nPaul Ziots: Thank you, Amit. Next question, please.\\nOperator: Your next question comes from the line of Aaron Rakers of Wells Fargo. You may proceed.\\nAaron Rakers: Thanks for taking the question. Yes. Just to kind of build on Amit's question there. I guess if I'm doing the math right, it looks like you're talking about an impact of about $80 million, $85 million from this higher Evergreen//One contribution. So I guess the question is like what would that number look like just actually three months ago embedded in your expectation? And how do I think about -- you threw out a number, $400 million. Is that $400 million of ARR contribution. Just help me understand what $400 million is and how fast this necessarily could grow into the next year? Just thinking about incremental headwinds from this transition to this consumption model here as we move forward.\\nKevan Krysler: Yes. How're you doing, Arron, this is Kevan. The $400 million is really our sales of Evergreen//One and Evergreen//Flex for the year. So think about that as a TCV of sales during the year. And then if you - if we take a step back into exiting Q2 and going into Q3, we had talked about the fact that our annual guide range had contemplated strong growth. We were adding about another 1 to 2 points of headwind to that growth exiting Q2. And then with the strength that we've seen in Q3 on Evergreen//One as well as what we're seeing in terms of volume and opportunities in Q4, that headwind now has been increased to 3 points. So again, and your math is about right in terms of the effects in absolute dollars when in context to our annual revenue guide. And then tacking on to that, the telco order, which is having an impact as well.\\nPaul Ziots: Thank you, Aaron. Next question, please.\\nOperator: Next question comes from the line of Meta (NASDAQ:META) Marshall of Morgan Stanley. You may proceed.\\nMeta Marshall: Great. Thanks. Maybe a couple of questions. One, just on any update on how you're seeing kind of your - or probability of kind of Meta order coming back at least sometime in the next fiscal year? And then just a second question on - I mean, I would assume you would have said that the conditions are largely staying the same for most of this year. And so has this Evergreen transition been a headwind for kind of more than this quarter and it just culminated into something you couldn't kind of make up for given not seeing a better environment? Or just how have you seen this evergreen transition kind of progress throughout the year? Thanks.\\nCharlie Giancarlo: Let me start with the Meta question. So we continued - Meta continues to be a good customer. We have not - generally, when - on these calls, we talk about contribution by RSE. No meaningful contribution to RSE this past quarter, but we've had sales to Meta in other segments. And that relationship continues to blossom as far as we're concerned. It continues to get better, and we expect to continue to see good opportunities in Meta as we go forward. And then the second one was the...\\nKevan Krysler: You want to clarify your second question as well.\\nMeta Marshall: The second question is just it's clearly a headwind as we go into fiscal Q4, this kind of Evergreen transition. But given probably much of the macro environment has stayed stable during the year, how much of a headwind has it been in previous quarters that you were just kind of able to accommodate for? I guess I'm just trying to say or get a sense of how much of that is new this quarter versus this has been a headwind for the entire year?\\nCharlie Giancarlo: Well, almost - if I might start. One, because we've had such strong growth in Evergreen//One. It has been a bit of a headwind the whole year, but it had never been sustained the way it is this year. We had good quarters, like every other new program you have strong quarters and then it's balanced off by other quarters where the specific opportunity in this case, Evergreen//One was a bit weaker. And when we saw the growth in Evergreen//One early in the year, as we -- as you may remember, the economy turned weak around Q4 last year, and so we were expecting Evergreen//One to pick up. What's really different this time around is just the continued strength quarter-over-quarter and visibility into Q4 now as well, the strength there. So yes, but it's becoming a more meaningful number now.\\nKevan Krysler: Yes. And if I would just to add on to that, to what Charlie was saying, it really is an accumulation of effect. So starting with our annual revenue guide that we provided, obviously, there were two components that were key in that guide of mid-to high single digits. One was contemplation of the macro environment. And second was, we were considering momentum of Evergreen//One when we came into the year. Now exiting Q2, we added on 1 to 2 points of additional headwind as a result of what we were seeing, both for Q1 and Q2 and our visibility to Q3. Now Q3 outperformed even our raised expectations in Q4 looks very strong as well. And so you do have the accumulation effect going on, but lastly, when we think about Q3, there was definitely more pressure on product revenue as a result of the momentum we were seeing with Evergreen//One. And the sales team did a really nice job executing throughout the quarter. And part of this strong execution included accelerating fulfillment and probably of a subset of product orders that would have been expected to close in Q4. So there is an impact there as well. And hopefully, that adds some additional color for you there.\\nPaul Ziots: Thank you, Meta. Next question, please.\\nOperator: The next question is from the line of Tim Long of Barclays. You may proceed.\\nTim Long: Thank you. Could we just talk a little bit more about the double-digit AI wins in the quarter. Maybe just give us a little more color there on kind of how big these deals are coming out. Any product portfolio that you have that's differentiated, that's winning there kind of timing to see some revenue recognition from these deals? That would be helpful. Thank you.\\nRob Lee: Yes, absolutely, Tim. This is Rob. I'll take that one. Certainly, AI, inclusive of traditional AI and generative AI is and continues to be a strong segment for us. But more importantly, what we've seen this quarter is a variety of AI wins really across all of the areas of the AI-driven opportunity for us here at Pure. And that's number one, certainly, the training infrastructure environments. Number two, the inference and AI application environments, as well as number three, some of the broader data environments that are being connected to these AI workflows. And so if we look at the AI training infrastructure, look, this - we've been in the space for years, continues to be a strong area for us. As an example, this quarter, we had a large automaker, expand their FlashBlade footprint, really driving their autonomous driving efforts as well as number two, we saw customers really deploying both our Portworx as well as our flash array solutions as part of their inference AI application environments. And then thirdly, as we look at the opportunity around the broader data management and data preparation environments, these are equally important parts of AI deployments. And we saw some good wins in both of these - in these areas as well, serving both customers' database and data preparation applications as well as some other bulk data repositories connected to these environments. And so if I net it out continues to be a strong segment for us and look, if we look at the opportunity in terms of both the GPU connected environments as well, the larger data environments being driven by AI technology, we're well positioned across the entire portfolio to benefit from these deployments.\\nPaul Ziots: Thank you, Tim. Next question, please.\\nOperator: The next question comes from the line of Pinjalim Bora of JPMorgan. Your line is open.\\nUnidentified Analyst: Hi, guys. This is Noah on for Pinjalim. Thanks for taking our question. I guess how should we really think about fiscal year 2025? And what are really some of the puts and takes you can highlight there especially when we think about the seasonality of the business given all the moving parts going forward? Thank you.\\nKevan Krysler: Yes. Thanks for the question. And really, we won't go into a lot of detail specific to next year. We want to navigate through our seasonally largest quarter, which is Q4. But I think two important things to take away that we're seeing. One is that demand has strengthened and is expected to strengthen through the first half, and that's a good sign for us. And second is the momentum on for Evergreen//One and strength we're seeing for a variety of reasons that Charlie walked through. And so both those factors will be in play in terms of how we think about next year, but would want to get through Q4 before providing anything more specific for next year.\\nPaul Ziots: Thank you, Noah. Next question, please.\\nOperator: The next question is from the line of Krish Sankar of TD Cowen. You may proceed.\\nKrish Sankar: Hi. Thanks for taking my question. I had a question on the $40 million telco order. It seems like a pretty big number on a quarterly basis. I was under the impression you have such a high customer concentration. So can you talk a little bit about that? And also, is this being used for 5G applications what kind of application is being useful. Thank you.\\nCharlie Giancarlo: Yes. No, we highlighted that it is, in fact, a 5G application environment that this is going into. And I would say in order of this type in one quarter is not terribly unusual for us. It does happen from time to time. And 5G has been a very strong market for us. The advantages that we have in space, power and cooling, reliability and longevity of the product in its remote management capability really aligns well with telco and especially 5G, which is a very distributed environment. And I will say, although it's unusual for us in general, it's not unusual in the telco space to have scheduled shipments aligning with their upgrades to their distributed 5G environment. So our typical business in IT and other areas is book and ship within weeks. This is a scheduled shipment based on build outs - their build-out schedule of their 5G environment.\\nPaul Ziots: Thank you, Krish. Next question, please.\\nOperator: The next question comes from the line of Sidney Ho of Deutsche Bank. You may proceed.\\nSidney Ho: Thank you for the question. I want to look at the product growth - actually, gross margin is very, very good at 74%, curious about the sustainability of that, particularly if you look at the product gross margin being 73.1%, looking forward, how would you characterize the pricing environment, especially given many of your competitors have access to lower price and components now? And what specifically do you have to be more aggressive in pricing in order to keep or win businesses, especially in areas that are starting to see more competition? Thanks.\\nCharlie Giancarlo: Thank you, Sidney. We intend to be very aggressive in pricing and especially as we penetrate the lower price performance tiers of the disk market. It's a new market for flash and the opportunity there is huge, more than half of the overall enterprise storage market, and we believe we have a special opportunity that ahead of every other player in the market. So our intention is the E family grows is to be very aggressive in that segment. Frankly, as you point out, the 73 is above our intended range for product gross margins. And we're going to use that a natural cost advantage we have to continue to gain market share.\\nRob Lee: And just to add on to that, I think it's important to just highlight and remember that because of our flash management technology and direct flash, we've got a sustained and significant structural advantage over the competitive set who are trapped an SSD-based technology. Because of the direct flash, we can deliver products that are less complex, more reliable, more efficient, more performance. And at the end of the day, that translates to better products delivered in a more cost-effective way. And behind that, our Purity software really is that differentiation. And so just to - again, just to add on to Charlie's points, the advantages that we have really are structural and sustainable, driven by our significant software and hardware IP.\\nKevan Krysler: So I'm just going to add to this a little bit more, taking a step back, agreeing that the gross margins across the board were very strong, and so we're quite pleased with that and validating and agreeing with obviously Charlie and Rob and really wanting to reiterate as well that the majority of our bits shipped now continue to be QLC, which ties into the continued innovation that we're driving. And look, we really do benefit from flash pricing, both improving and weakening and we are currently seeing some improvement in the NAND pricing. And then reiterating Charlie's point, from a financial lens with our strong product gross margins, we will be aggressive in our disk takeout strategy with our E family price performance solutions. As we think about our subscription growth margins, that's going to be a combination of our scale on our Evergreen offerings, including the consumption and subscription portfolio as a key contributor, also similar to the strength we're seeing in product margins, our Evergreen subscription gross margins are also benefiting from the structural advantages that Rob alluded to. So yes, definitely pleased with what we're seeing in terms of our margin performance overall.\\nPaul Ziots: Thank you, Sidney. Next question, please.\\nOperator: The next question is from the line of Mehdi Hosseini of SIG (sic) [Susquehanna]. You may proceed.\\nMehdi Hosseini: Yes. It's actually Mehdi Hosseini. Thanks for taking my question. Just want to better understand Evergreen//One. To follow-up here, would this impact your working capital requirement? And is there anywhere in the balance sheet that we could look and better track the traction with Evergreen//One model?\\nKevan Krysler: Yes. I think the key metric, obviously, we added a new metric this quarter, which was the total sales this year expected for the combined Evergreen//One and Evergreen//Flex model and that being $400 million over doubling year-over-year. So that would be a new metric that we've added that hopefully is helpful. The other metric that I think is helpful is the RPO metric and obviously, our strength in growth in RPO is really being driven by our Evergreen//One momentum that we've seen throughout the year, including Q3. And then layering on to that would be our subscription services ARR growth, which is also quite strong. So those are the metrics I'd probably point you to in assessing the health around our consumption and subscription services business.\\nPaul Ziots: Thank you, Mehdi. Next question, please.\\nOperator: The next question is from the line of Simon Leopold of Raymond James. You may proceed.\\nSimon Leopold: Great. Thanks for taking the question. I just - first, a quick clarification. I appreciate you're not ready to guide for fiscal '25, but I think it would be helpful to get a little bit of handholding given that your April quarter, your first quarter is typically down teens double digits, but you've got kind of this tough situation in the January quarter. So just some clarification on that. And then the question I wanted to ask, and I love Charlie's description of the competitive environment as a nice fight in the phone booth. I'd love to get some updates there, in that what you're seeing and hearing in terms of competitive actions and your ability to displace both hybrid flash competitors as well as hard disk drive competitors. Thank you.\\nKevan Krysler: Yes. I'll hit the discussion for next year, again without providing a lot of details. But as you think about the setup for next year, it is critical for us to navigate through Q4 and our expectations through Q4. But this is really not, in my mind, a seasonal question. It's really about Evergreen//One and the performance of Evergreen//One. And if we take a step back, and look at the Evergreen//One momentum and the telco order, we should be for this year at 7% growth. And so what we've seen on top of that and what we're seeing in the second half is strengthening demand. And I would hope to see that as we continue through Q4 and as we move into next year. So that would be a data point for consideration. We talked about 3 points on the annual guide against the annual guide of headwind for Evergreen//One. Look, we're expecting momentum to continue on Evergreen//One. We're actually thrilled that we're seeing an inflection point with our customers with that model. And so we'll have to provide more color for you as we move through Q4, specific to the Evergreen//One momentum next year. Charlie?\\nCharlie Giancarlo: Yes. In terms of selling and what the competitive situation looks like out in the field, I'd say it's as competitive as ever, perhaps, so a little less gross margins generally have been improving. So perhaps the pricing environment has been a little bit better, but I would say, overall, the competition is as tough as ever, both in the channel and directly with customers overall. What I will say is, having perusing as we always do, our overall win rates and at bets. Our overall win rates hold very steady against our competitors. So - but it is a tough environment without doubt.\\nPaul Ziots: Thank you, Simon. Next question, please.\\nOperator: The next question comes from the line of Nehal Chokshi of Northland Capital Market. You may proceed.\\nNehal Chokshi: Yes. Thanks. I want to double click here on Slide 17. Your unbilled RPO, I've always sort of reviewed that as a good proxy for your Evergreen//One and Evergreen//Flex sales. A, is that true? And then I'll go from there.\\nKevan Krysler: Nehal, that is true. And then I would add on to that specific for this quarter would be the telco order that we've been highlighting and discussing as well.\\nNehal Chokshi: Right. Okay. So the Q-o-Q change in unbilled RPO $100 million, take out that $40 million for the Telco order, then you're talking about $60 million. That's still a significant increase. And if I just simply do it on a cumulative two-year basis, for this year relative to a year ago, that's more than a doubling on a year-over-year basis. So can you talk about the linearity that you're seeing in this acceleration in Evergreen//Flex, was there relatively sold in the first two quarters and now an explosion in the third quarter and then you expect sort of some sort of normalization in the fourth quarter here?\\nKevan Krysler: Yes. And it's primarily Evergreen//One. And we've talked about what we've seen this year is a cumulative build and momentum of Evergreen//One. So again, coming out of the year and developing our guide for the year, we had contemplated a really significant growth in Evergreen//One. And again, as we navigated through Q1 and Q2, we saw that that growth rate was impacted a little bit more 1 to 2 points against our annual guide. Now that we've been through Q3 and our visibility to Q4, now that's about 3 points incremental to what we had provided in our annual guide. So this has been a cumulative effect. And as Charlie has pointed out, really seeing an inflection point in part, I think, due to the macro, but I also think customers are really embracing the value of this business model as well.\\nPaul Ziots: Thank you, Nehal. Next question, please.\\nOperator: The next question comes from the line of Wamsi Mohan of Bank of America. You may proceed.\\nWamsi Mohan: Hi, thanks so much. I was wondering if you could maybe give us some update on how we should think about CapEx given the increased momentum of Storage as a Service. And where are you in terms of the build-out when you think about maybe this continued momentum of Evergreen//One, what sort of revenue level can the infrastructure that you currently have support? Thank you.\\nKevan Krysler: Yes. I don't think we'll see a lot of change in terms of how we're thinking about our CapEx. Obviously, we had higher CapEx as well this year due to the build-out of our headquarters. Obviously, we've got a significant amount of innovation from an R&D perspective. So we've got some CapEx requirements there. And really, the only other big CapEx requirement is supporting the momentum of Evergreen//One. And so that's how we would be thinking about it from a CapEx perspective. And then obviously, you've got a layering on in terms of the subscription services revenue effect coming on as a result of the ramping Evergreen//One sales that we're seeing.\\nPaul Ziots: Thank you, Wamsi. Next question, please.\\nOperator: Your next question comes from the line of Matt Sheerin of Stifel. You may proceed.\\nMatt Sheerin: Thank you. Commentary from one of your competitors last night pointed to a broader QLC-based adoption across the industry. So first, are you seeing any changes in the competitive landscape or market share pressure given some new product introduction. It sounds like from your previous comments that you're not seeing that. And second, can you add more color on the success you've seen so far with FlashBlade//E and expectations for the new array E? And any surprises in terms of use cases or types of customers?\\nCharlie Giancarlo: You bet. Well, I'll now refer to the E family given that we've now introduced FlashArray//E, which lowers the incoming price point, if you will, for customers from what was 4 petabytes on FlashBlade now down to 1 petabyte on flash array and even less if they take it as a service. So we really feel that it's a very, very strong product line. We have seen that entry by one of our competitors. E comes in substantially below that. We really feel the E - the competitors see offering is much closer to our C offering. So we're several years ahead of that. The competitive environment for our offering is still largely with disk. And disk - we say disk, but of course, disk comes in many flavors. And as we develop E further and further, we have to address all the different use cases that it's involved in, which is what drives its growth. That growth is still the fastest growth of any new product that we've had here at the company. So we're very pleased with the growth, but it's still at the - we're only two full quarters in. So it's - we anticipate that will be a much more meaningful part of our revenue next year.\\nPaul Ziots: Thank you, Matt. Next question, please.\\nOperator: The next question is from the line of Eric Martinuzzi of Lake Street. You may proceed.\\nEric Martinuzzi: Yes. Curious to know how you feel about your sales capacity is typically when you're evaluating your coverage for the coming year? Are you planning on adding sales?\\nCharlie Giancarlo: The answer is yes. We believe our capacity is at the level that we had planned for. And we're planning, obviously, to grow. So we are adding to our sales force have been throughout the year, but Q4 is a key time to bring in new players. And so we will be adding to our sales force.\\nPaul Ziots: Thank you, Eric. We have one more question. So next question will be the last question.\\nOperator: The next question is a follow-up question from the line of Krish Sankar of TD Cowen. You may proceed.\\nKrish Sankar: Hi. Thanks for taking my follow-up. I just wanted to follow up on the Evergreen//One. What sort of time frame are the - are these contracts, for example, just to like make it simple. You said it's a three percentage point headwind for your FY '24 outlook. It is very simply that the next two years are flat and because of this one, is it like a 1% uptake every over the next three years, if it's a three-year subscription model. I'm just kind of curious how to think about these Evergreen//One subscription.\\nKevan Krysler: Yes, it's a great question. Look, if we look at the Evergreen//One orders that we've closed to date this year, we're probably averaging a little bit over 3.5 years in duration associated with that of those orders, which really is similar to our traditional CapEx commitments as well. So that's the answer in terms of duration that we're seeing.\\nCharlie Giancarlo: I do want to point out that these are consumption contracts. And so these are, in effect, minimums. The consumption goes higher, then, of course, we'd expect more. And of course, we expect the contracts to continue even after the contract is done.\\nKevan Krysler: That's right, Charlie. And then when we calculate what the headwind is, and against - the headwind again is against our annual guide that we provided at the beginning of the year, we just basically translate the incremental growth we're seeing beyond the growth we had assumed in the annual - in the annual revenue guide that we provided, using about 70% going to product if it was a traditional CapEx sale. So that's how we're calculating the points of headwind against our annual revenue guide.\\nOperator: There are no additional questions waiting at this time. So I'll pass the conference over to the management team for closing remarks.\\nCharlie Giancarlo: Thank you, operator. Our strategy to provide customers a consistent, unified and modern storage platform for all their storage needs based on a cloud operating model continues to distinguish us in our industry and propel our success. And we believe the strength of our consumption offerings continues to outperform, benefiting both our customers and Pure. As we enter the holiday season, I want to extend our heartfelt thanks to all of our customers, our investors, partners, our suppliers and our employees. Your effort enables Pure to lead the industry in defining the next generation of data storage. Thank you.\\nOperator: That concludes the Pure Storage third quarter fiscal year 2024 earnings conference call. Thank you for your participation. You may now disconnect your lines.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " None,\n",
       " None,\n",
       " 'By Foo Yun Chee BRUSSELS (Reuters) - Chinese conglomerate ByteDance\\'s TikTok has asked Europe\\'s second highest court to suspend its designation as a gatekeeper under onerous new EU tech rules until judges rule on its challenge against the label. The Digital Markets Act (DMA) requires TikTok and other designated gatekeepers Alphabet (NASDAQ:GOOGL)\\'s Google, Meta Platforms (NASDAQ:META), Apple (NASDAQ:AAPL), Amazon (NASDAQ:AMZN) and Microsoft (NASDAQ:MSFT) to make their messaging apps interoperate with rivals and let users decide which apps to pre-install on their devices. They are not allowed to favour their own services over rivals\\' or prevent users from removing pre-installed software or apps. TikTok last month challenged the EU decision at the Luxembourg-based General Court, saying its designation risks undermining the DMA goal of protecting gatekeepers from newer competitors like itself. \"We have applied for interim measures,\" a spokesperson said. \\nThe bar for the court to approve interim measures is very high. Companies must show that situation is urgent and that they would suffer irreparable harm without an interim measure. Meta and Apple have also sued the Commission over their gatekeeper status.',\n",
       " None,\n",
       " \"UiPath (NYSE:PATH) has reported a robust third-quarter fiscal 2024, with a 24% year-over-year increase in both Annual Recurring Revenue (ARR) and revenue, reaching $1.378 billion and $326 million respectively. The company also revealed its focus on industry verticalization and strategic partnerships with SAP and Deloitte in its earnings call. In addition, UiPath introduced new capabilities, including UiPath Autopilot and Intelligent Document Processing, to boost its automation platform.\\nKey takeaways from the call include:UiPath's ARR and revenue both grew by 24% year-over-year.The company highlighted its focus on industry verticalization and partnerships with SAP and Deloitte.New capabilities, including UiPath Autopilot and Intelligent Document Processing, were introduced.UiPath aims to capitalize on the growing automation market and drive innovation as it transitions its CEO to the role of Chief Innovation Officer.The company reported a non-GAAP operating margin of 13% and generated non-GAAP adjusted free cash flow of $44 million.During the Forward VI event, UiPath announced its latest platform release, 2023.10, which includes new AI-powered capabilities to enhance user experience across the platform. Among these is UiPath Autopilot, which uses generative AI to bring automation to enterprise-grade processes, and improvements in Intelligent Document Processing (IDP) using active learning and generative AI.\\nIn terms of financials, UiPath expects fourth-quarter revenue to be in the range of $381 million to $386 million, and ARR to be between $1.450 billion and $1.455 billion. The company's dollar-based net retention rate was 121%, and its dollar-based gross retention rate was 97%.\\nRob Enslin, a representative from UiPath, discussed the company's strong quarter and highlighted the success of large deals. He emphasized the positive relationships with customers and the adoption of AI-powered automation in driving digital transformation. UiPath CEO Daniel Dines discussed the importance of powerful automation platforms in harnessing the power of AI and highlighted UiPath's position in building foundational models for automation.\\nAccording to CFO Ashim Gupta, macroeconomic variability had a greater impact on smaller businesses, while enterprise segment performance remained strong. The company also discussed the adoption of autopilot features and the acceleration of deployment through specialized models.\\nThe executives expressed positive demand and customer conversations at the C-suite level, emphasizing UiPath's relevance in IT budget growth for automation. They also mentioned partnerships with ServiceNow (NYSE:NOW) and Microsoft (NASDAQ:MSFT) and the reduction of SKUs in sales refresh.\\nIn terms of verticals, the public sector, particularly the federal business in the UK and US, had a strong quarter due to the unique capabilities UiPath offers in document understanding and attended automation. Other industries such as telcos, CPG companies, manufacturing, and retail are also looking for automation to drive efficiencies.\\nThe company reported good deal quality and execution, resulting in a record number of deals above $1 million and strong performance in the enterprise segment, particularly in North America. Enslin mentioned that they have broadened their offerings and don't see traditional RPA vendors as often.\\nWhen asked about the impact of the new bundled pricing packages on revenue, Enslin stated it was too early to provide specific data but customers like the simplicity and flexibility it offers. The call concluded with thanks and well wishes for the holidays.InvestingPro InsightsUiPath's recent earnings report has highlighted its significant growth and strategic initiatives, but what do the numbers and expert analysis from InvestingPro tell us? One critical piece of information is that UiPath holds more cash than debt on its balance sheet, an InvestingPro Tip that suggests a strong financial position to weather economic uncertainties and invest in further growth.\\nMoreover, UiPath has been demonstrating an impressive gross profit margin, with the last twelve months as of Q2 2024 showing a margin of 84.15%, according to InvestingPro Data. This metric is crucial as it indicates the company's efficiency in controlling the costs associated with its revenue.\\nAnother InvestingPro Tip that stands out is the significant return over the last week, with UiPath's price total return reaching 7.68%. This performance could reflect investor confidence following the earnings report and the announced innovations.\\nFor readers looking to dive deeper, InvestingPro offers additional insights and metrics that could be vital in making informed investment decisions. Currently, there are several more InvestingPro Tips available for UiPath, which can be accessed by subscribing to InvestingPro. With the special Cyber Monday sale, subscriptions are now up to 60% off, and using the coupon code sfy23 will grant an additional 10% off a 2-year InvestingPro+ subscription.\\nInvestingPro Data also reveals that UiPath's market capitalization stands at $11.2 billion, and while the company has a negative P/E ratio of -72.74, analysts predict the company will be profitable this year, which could indicate a potential upside for investors.\\nThe insights provided by InvestingPro, coupled with the latest earnings report, paint a picture of a company that is not just growing but also managing its finances prudently and investing in innovation that could drive future success.Full transcript -  Uipath  (PATH) Q3 2024:Operator: Greetings, and welcome to the UiPath Third Quarter Fiscal 2024 Financial Results Conference Call. At this time, all participants are in a listen-only mode. A question-and-answer session will follow the formal presentation. [Operator Instructions] As a reminder, this conference is being recorded. It is now my pleasure to introduce Kelsey Turcotte, Senior Vice President of Investor Relations for UiPath. Thank you. You may begin.\\nKelsey Turcotte: Good afternoon and thank you for joining us today to review UiPath’s third quarter fiscal 2024 financial results, which we announced in our earnings press release issued after the close of the market today. On the call with me are Daniel Dines, UiPath’s Co-Founder and Co-Chief Executive Officer; Rob Enslin, Co-Chief Executive Officer; and Ashim Gupta, Chief Financial Officer. Rob will start the discussion and then turn the call over to Daniel. After that Ashim will review our results and provide guidance. And then we will open the call for questions. Our earnings press release and financial supplemental materials are posted on the UiPath Investor Relations website: ir.uipath.com. These materials include GAAP to non-GAAP reconciliations. We will be discussing non-GAAP metrics on today’s call. This afternoon's call includes forward-looking statements about our ability to drive growth and operational efficiency and grow our platform, as well as our financial guidance for the fourth quarter fiscal 2024. Actual results may differ materially from those expressed in the forward-looking statements due to many factors and therefore, investors should not place undue reliance on these statements. For a discussion of the material risks and uncertainties that could affect our actual results, please refer to our Annual Report on Form 10-K for the year ended January 31, 2023, and our subsequent reports filed with the SEC, including our Quarterly Report on Form 10-Q for the period ended October 31, 2023 to be filed with the SEC. Forward-looking statements made on this call reflect our views as of today; we undertake no obligation to update them. I would also like to highlight that this webcast is being accompanied by slides. We will post the slides, and a copy of our prepared comments to our Investor Relations website immediately following the conclusion of this call. In addition, please note that all comparisons are year-over-year unless otherwise indicated. Now, I would like to hand the call over to Rob.\\nRob Enslin: Thank you, Kelsey. Good afternoon, everyone. Thanks for joining us. Our third quarter results underscore the compelling value our end-to-end automation platform delivers for our customers and the strength of our business model. For the quarter, ARR grew 24% to $1.378 billion, driven by a third quarter net new ARR of $70 million, while revenue was $326 million, up 24%. We continue to deliver growth while driving operational efficiencies across the organization. Non-GAAP operating margin increased more than 600 basis points year-over-year to 13%. We also delivered non-GAAP adjusted free cash flow of $44 million. As many of you know, at the beginning of this fiscal year we pivoted our go-to-market resources towards organizations that have a meaningful runway to invest in enterprise automation over the long-term. This investment has significantly increased our presence in the C-suite and helped raise our profile with partners of all sizes. You could feel our momentum at Forward VI, our annual user conference, where we hosted more than 3,000 guests, including automation practitioners, industry visionaries, and key customer decision makers. We also launched our inaugural UiPath AI10 Awards program, recognizing UiPath customers who embody what it means to be a leader in AI at Work. I left the event super energized about the tangible value our platform is delivering for customers and our leadership position in the market. FX-adjusted dollar based net retention for the quarter was 123% and we closed a record number of third quarter deals over $1 million in ARR. Customers with $1 million or more in ARR grew 31% to 264, while customers with $100,000 or more in ARR increased to 1,974. Industry verticalization continues to be a strategic priority for the company with playbooks, marketing events and enablement to support our teams. We now have 70 solution accelerators available in our marketplace, with IT Service Management Software user provisioning and two-way match invoice processing for Coupa and SAP among some of the most popular downloads. While we saw broad based strength in net new ARR across industries this quarter, there were a couple of standouts. The Federal team delivered a record quarter as agencies are increasingly standardizing on our AI-powered Business Automation Platform with a robust set of end-to-end capabilities to enrich employee experience, create mission readiness, and achieve breakthrough outcomes. Customer highlights included Veterans Affairs, Coast Guard, the IRS, and the Department of Homeland Security. We are also working with the United States Department of Agriculture to support their Future of Work initiative, delivering a new era of citizen and employee experiences. Using our full platform, USDA is driving mission impactful enterprise automations across their HR, Finance, and IT departments. The program also features a digital assistant on every USDA employee desktop, driving personal productivity for approximately 100,000 employees. Momentum continues in financial services and health care where automation delivers considerable value. This includes one of our top 25 customers, a large non-profit health system in the United States. Taking a deeper look at their automation journey, they are a great example of how customers expand with us over time. Working closely with our account executives and global systems integrators, their journey started in 2018 with core RPA and, over the last several years, they have expanded to attended automation, Document Understanding, Test Suite, and Process Mining. To date, they have achieved a return on investment of over $250 million and this quarter, in one of our largest deals in company history, they expanded to the full platform as they work to create a centralized Enterprise Automation Service department with a mandate from their EVP sponsor to deliver automation across their entire enterprise. They are also in the process of developing use cases for Communications Mining and UiPath Apps. Customers standardize on our AI-powered Automation Platform to deliver transformational outcomes that streamline processes, eliminate errors, and operate with the enterprise-quality execution needed to succeed in today's environment. A great example is  Johnson Controls  (NYSE:JCI). After starting their automation journey in 2021 with core RPA, they adopted the full UiPath platform as they work to consolidate the automation program to one end-to-end solution. They also plan to leverage our AI capabilities like Document Understanding, Test Suite, and Task Mining to drive automation across their entire business. Another example is The Department for Work and Pensions, the United Kingdom's largest public service department. They have been using core RPA since 2018 to help their most vulnerable citizens improve their quality of life by automating millions of service and support claims each year. Since their first deployment, they have scaled to over 1,000 robots in production and saved 3.1 million hours to date. During the quarter they expanded to the full platform as they look to harness AI and integrate Document Understanding, Process Mining, and Communications Mining into their automation program to improve citizen services, drive operational efficiency, and increase cost saving and capacity creation. We are also landing new logos which are adopting multiple platform products in their first purchase like Tenable and KIK Consumer Products. In their first phase of deployment KIK intends to leverage unattended robots and Document Understanding to drive efficiency across their finance department, with the long-term goal of scaling automation across the entire enterprise. Our value-based go-to-market tool, NorthStar, also continues to drive deeper customer conversations as we strategically position the differentiated and actionable benefits of AI-powered automation. A great example is Sobeys, a customer since 2020, where we created a NorthStar roadmap to help them maximize the return on investment of their automation program. And as a result, Sobeys expanded the automation footprint in the quarter to streamline processes in finance, merchandising, and supply chain and to improve their hiring process in stores. They are also investigating how they may be able to use our platform in their SAP transformation journey. Turning to our SAP partnership, while in the early stages, we are excited about the collaboration between our teams. I recently joined Scott Russell, SAP Executive Board Member, Customer Success, to co-host their sales leadership meeting and we continue to see success in signing new logos from this partnership. Key one for me The Arnott’s Group, [indiscernible] in Australia, an Australian legend, the Australian producer of biscuits and snack food, The Arnott’s Group selected the UiPath Platform to optimize business processes and reduce operating costs. Their initial focus will be on automating end-to-end sales order and invoice processing. We also announced an expanded partnership with Deloitte, one of SAP's largest and most strategic partners. Deloitte will embed the UiPath AI-powered Business Automation Platform into their Ascend service delivery platform, powering the next generation of SAP transformations. These strategic relationships are powerful for our customers, but they also drive increased engagement with GSIs, and our partner ecosystem, that has never been more invested in than they are right now. At Forward VI, we hosted an impactful session for more than 750 partners outlining our streamlined strategy that is designed to accelerate growth and revenue for our partners, while creating solutions that deliver exceptional value to our customers. Our partners and GSIs are an important element of our go-to-market motion that help us expand our reach to customers in a scalable, efficient, and cost-effective way. For example, working with Deloitte, an Australian Government agency has built a robust automation program across their finance and HR business lines where they utilize Document Understanding to streamline invoice processing, reduce errors, and enhance data accuracy. To further accelerate their automation program, they adopted Task Mining in the quarter to analyze processes and accelerate scoping activities. Broadening our technology ecosystem also makes it easier for customers to deploy automations. We recently announced several new strategic partnerships. These include Amazon (NASDAQ:AMZN) Bedrock, which enables automation developers and citizen developers to seamlessly integrate Generative AI directly into their UiPath Studio and Studio Web automations, and the availability of the AI-Powered Business Automation Platform on the Google (NASDAQ:GOOGL) Cloud Marketplace in early 2024. In summary, third quarter results are another proof point of our commitment to delivering strong top line growth with expanding profitability and non-GAAP adjusted free cash flow. I want to thank our employees and partners for their support. None of this would be possible without your relentless focus on unlocking value for our customers day in and day out. And with that, I'll turn the call over to Daniel.\\nDaniel Dines: Thanks, Rob. Good afternoon everyone, thanks for joining us. The automation market is at an inflection point, and as the market leading tool that enables organizations to derive actionable value from AI, we believe this is a huge opportunity for UiPath. At Forward VI we introduced our latest platform release, 2023.10, and delivered scores of new capabilities that seamlessly translate the potential of AI into tangible action to drive business outcomes for our customers. From my perspective one of the most exciting announcements at Forward was UiPath Autopilot, a new set of AI-powered capabilities for developers, testers, analysts, and knowledge workers, designed to enhance the user experience across the UiPath platform. We expect Autopilot, which is based on Generative AI, to bring our unique capabilities in RPA, API automation, Document Understanding, and Specialized AI to the full spectrum of enterprise-grade end-to-end processes automation. Autopilot for Studio is intended to help developers across skill levels build automations faster by leveraging natural language descriptions to generate automation workflows. For less technical developers this is a great starter tool, while an advanced developer will benefit from increased productivity. Autopilot for Test will accelerate every phase of the testing lifecycle from generation of tests to surfacing insights from test results. And, finally, as the AI companion for business users, we anticipate that Autopilot for Everyone will help users create and use personalized and intelligent micro automations on the fly. We also shared exciting announcements around Intelligent Document Processing at Forward, introducing our next-generation experience powered by active learning and Generative AI. Our next-gen IDP permits almost anyone to train Specialized AI models for specific domains and document types, and our internal benchmarking shows that our next-gen IDP experience accelerates model training time by up to 80% from a week to a day for complex scenarios, or down to minutes for simpler forms. Document Understanding is driving significant value for customers including a Chicago based public utility. Working with Deloitte, they have created a robust automation program across their organization leveraging our AI products like Document Understanding to automate their meter inspection process where they have achieved over $12 million and 163,000 hours in savings to date. With c-level sponsorship, during the quarter they expanded their UiPath deployment to incorporate Test Suite into their automation journey with a goal of saving over $70 million a year. And, we are humbled by third-party recognition of our achievements, including IDC MarketScape: Worldwide Intelligent Document Processing or IDP, 2023-2024 Vendor Assessment where UiPath was named a leader in IDP for our broad market leadership and robust IDP capabilities in UiPath Document Understanding and Communications Mining integrated into our broader enterprise automation platform. Innovation is a cornerstone of our strategy and we challenge ourselves every day to deliver market leading capabilities with a customer first mindset. I spent a lot of time with our teams in both Bellevue and Romania this quarter and am energized by our cutting edge product roadmap focused on transforming enterprise automation by harnessing the next-generation of AI technologies. Together with our world-class team of engineers I am excited to dedicate more time driving UiPath's innovation agenda as I transition into my new Chief Innovation Officer role this winter. With that, I will turn it over to Ashim.\\nAshim Gupta: Thank you, Daniel, and good afternoon, everyone. Unless otherwise indicated, I will be discussing results on a non-GAAP basis and all growth rates are year-over-year. Turning to the third quarter, ARR totaled $1.378 billion, an increase of 24%, driven by net new ARR of $70 million. Excluding the FX tailwind, net new ARR totaled $69 million. We ended the quarter with 10,865 customers, including new logos like New Relic (NYSE:NEWR), Smile Doctors, Beacon Health System, and MidWestOne Financial Group. As we said last quarter, we continue to see macro headwinds at the lower end of our market and remain focused on acquiring customers with a higher propensity to grow. Our dollar-based net retention rate for the third quarter was 121%. Normalizing for FX, dollar-based net retention rate was 123%. Dollar-based gross retention of 97% continues to be best-in-class. Revenue grew to $326 million, an increase of 24% year-over-year. Normalizing for FX, which was an approximately $3 million tailwind, revenue grew 23%. Remaining performance obligations increased to $995 million, up 31% year-over-year. Normalizing for FX, which was an approximately $16 million tailwind, RPO grew 29%. Current RPO increased to $599 million. Turning to expenses. We delivered a third quarter overall gross margin of 87%, and software gross margin was 92%. As the team continues to drive cost discipline, we now expect fiscal year 2024 gross margins to be approximately 86%. Third quarter operating expenses were $240 million, highlighting the leverage in our business and our commitment to expense management and operating discipline. GAAP operating loss of $56 million included $96 million of stock-based compensation. Non-GAAP operating income was $44 million, resulting in a third quarter non-GAAP operating margin of 13%. Third quarter non-GAAP adjusted free cash flow was $44 million. As of October 31, we had $1.8 billion in cash, cash equivalents, and marketable securities and no debt. Under our $500 million buyback program, which we announced on September 6th, we repurchased 3.2 million shares of our Class A common stock at an average price of $16.26 through October 31. Since November 1, under a 10b5-1 plan, we repurchased an additional 1.7 million shares at an average price of $17.38 through November 28, 2023. Now, let me turn to guidance, which assumes the global macroeconomic environment continues to be variable. For the fiscal fourth quarter 2024, we expect revenue in the range of $381 million to $386 million. ARR in the range of $1.450 billion to $1.455 billion. Non-GAAP operating income to be approximately $78 million. And, we expect fourth quarter basic share count to be approximately 567 million shares. And, finally, we expect fiscal year 2024 non-GAAP adjusted free cash flow of more than $250 million. Thank you for joining us today and we look forward to speaking with many of you during the quarter. With that, I will now turn the call over to the Operator. Operator, please poll for questions.\\nOperator: Thank you. [Operator Instructions] Our first question comes from the line of Keith Weiss with Morgan Stanley. Please proceed with your question.\\nUnidentified Analyst: Great. Thank you. [Indiscernible] on for Keith Weiss. Maybe to start off, quick question on sort of AI and how you see it fit into the current budget environment. Clearly, budget growth is still somewhat suppressed. And you're one of the few companies that’s sort of uniquely positioned between traditional automation and generative AI technologies. Where do you see customers kind of take the dollars from when they're spending on these AI technologies, whether that's within the product areas that you offer? Or more broadly, anything that's kind of coming out of your customer conversations in terms of how spend is getting reallocated that of AI?\\nRob Enslin: Yes, good question. What I would say is that when customers are looking at spending with us and the AI, the AI story and AI, connection, AI to automation connection really fits well to where customers want to invest. And in many cases, the investments are coming out of the business case and business value that we produced in NorthStar and are able to show the efficiencies in how to gain so they can invest in innovation. And they see us as an innovation leader today with AI and innovation, combined with our AI powered platform. So I don't feel like it's been taken away for one group to another, I feel it's really driven by the business case, we are able to drive with C-level suite and now we focus on industry solutions. And now we've connected those to the buying cycle with customers.\\nUnidentified Analyst: Great. That's helpful. And then a quick question on the margin side. It looks like sales and marketing expenses stepped up pretty markedly on a sequential basis. Obviously, you also do forward contracts in the quarter, but anything that you can highlight in terms of where you're investing, are you back to kind of growing your sales force more markedly and the second will continue into next year. Anything that you can share, there will be great.\\nRob Enslin: I would just highlight two points. First is, we are going to continue to invest in our sales team, especially in our sales capacity, especially as we head into next year. And then the second is if you recall, we also are dealing with -- we also have the reallocation of our software expenses. So first through third quarter, you'll see sales and marketing versus G&A, a little bit lopsided, and which will be caught up in fourth quarter.\\nOperator: Our next question comes from the line of Mark Murphy with JPMorgan. Please proceed with your question.\\nMark Murphy: Thank you so much, and I will add my congrats. So Rob, thinking back to the Analyst Day just over a year ago, you had mentioned there that every tech company, of course has shelfware and that UiPath does as well. If you think about it about a year later, do you sense that customers have worked down any excess inventory they might have had in terms of bots, maybe hitting a level where they might need to replenish and just start moving forward a bit faster as we get into next year? And then have a quick follow-up.\\nRob Enslin: Yes, Mark, look as I said earlier on, companies deal with shelfware, we feel good that our platform actually helps us move customers from not only bots into the broader platform as a focus on efficiencies, this marketplace that we're dealing with today. Certainly benefits efficiencies in new business model, so we feel really good that we are able to take customers to the next level with our platform and our C level activity.\\nMark Murphy: Okay. And Ashim, when we look at it arithmetically, the deceleration in ARR has been moderating, which of course is great to see. It looks like it could step down a bit, a bit closer to 20% next quarter. But is it possible to entertain the thought of holding -- kind of holding a level around 20% for a period of time? Or do you think we should kind of keep that trending down into the teens? As we've had it just -- try to be mindful of the law of large numbers and some of the some of the macro unknowns that are out there.\\nAshim Gupta: Yes, I would just say, Mark, we're really pleased with our performance and the team's execution. We are focused on closing out a good year-end here. And the teams are really focused on that. And we look at our fourth quarter guidance just putting the right prudence and the right macroeconomic variability included in our guidance, and we'll update as we get closer to next year.\\nOperator: Our next question comes from the line of Kirk Materne with Evercore. Please proceed with your question.\\nChirag Ved: Hi, thanks for taking the question and congratulations on a very strong quarter. This is Chirag on for Kirk. In your prepared remarks, you mentioned that large deals landed extremely strong. Do you have any additional commentary around what you're seeing with these large deals in terms of both new logos and renewals? And how you expect this trend to continue looking ahead to next quarter and next year? Essentially, all in all, do you believe that we're close to a more stabilized macro and spending environment? Or are you still seeing some broader choppiness out there? Thank you.\\nRob Enslin: Yes, I mean, we -- look, we feel really good about how our teams are adopting all the changes that we've made and how they're connecting with customers and the relationships we're having with customers. The expansion has really, in many ways driven through our growth products with all the AI communication that's happening in the market, it allows us to have meaningful conversations with customers, they see why the platform is relevant. And then we as I said, early on, we've been driving NorthStar quite a bit and showing where customers can really look at processes and tasks and understand how to drive value with the automation platform. So folks talk about digital transformation and connect, we believe that AI powered automation is really helping digital transformation with our customer base and our larger customers. And we're pretty happy with some of the net new companies that we've acquired over the last -- the quality of them, you really look at these kind of -- these are good companies that will expand with us over time if we do the right stuff. And so we feel good about that, we feel good about how our industry focuses is helping us change the game in this environment.\\nChirag Ved: All right, thank you.\\nOperator: Our next question comes from the line of Brad Sills with Bank of America. Please proceed with your question.\\nBrad Sills: Oh, great. Thank you so much. I wanted to ask a question around some of the departmental expansions you've seen. You talked about industries here. I think banking and public sector sounded good. But are you finding that you're seeing now with the uplift in expansion activity here with NRR, that you're getting outside of finance and accounting into IT, HR, some of these other use cases in the departments?\\nRob Enslin: Yes, so I would say we have broad machine broad industry activity, with manufacturing, with retail, with fashion retail with grocery hard goods retail. As we said, we feel really positive about public sector, and public sector globally. And we also see a significant amount of activity outside of the finance offers in procurement and in IT organization. So it's much more broad based. And we feel really good about where we are focused on that.\\nBrad Sills: Great. And then one of the forward conference, if I could please. If you could just remind us how important that event is for lead management and moving deals through the pipeline. And how do you feel coming out of the conference this year versus years past in terms of momentum? Thank you.\\nRob Enslin: Brad, I’ve only got 2 years experience. And what I would tell you, I was super excited to see the quality of customers and the quality of the event and the quality of the discussions that we had. If you look at the companies we have on stage with us and what they were doing with us, these are really transformational. If you look at the partners, partnerships with SAP and Deloitte [indiscernible] this was transformation. We spoke about 750 partners that were aligning around the channel strategy. So we feel good that we actually -- forward is actually a catalyst event to move UiPath, helped our branding, position us in the C-suite and make a difference and it certainly does help our pipeline as well.\\nOperator: Our next question comes from the line of Raimo Lenschow with Barclays. Please proceed with your question.\\nRaimo Lenschow: Thank you. Congrats from me as well. Firstly, a more bigger picture question, like if you think about the different areas of strengths that you saw this quarter, like can you specifically speak a little bit about what you saw on test, Rob? Because that's what we hear from our [indiscernible] as one of the areas where especially in the SAP ecosystem, there seems to be a lot going on there and not quite sure you're getting enough credit. And then I had a follow-up for Ashim, please.\\nRob Enslin: Yes, it's -- so if I look at the broader picture, you look at -- if you look at the market today, as I said early on customers are looking for ways to drive efficiencies to fund other activity. And there's not a significant amount of new business models that are driving growth. So that really helps us significantly, and obviously their [indiscernible] where SAP of focus where their transformation objectives, the [indiscernible] program, which is an ongoing program. And it's -- I believe that we've actually moved the needle significantly with SAP. With the SAP activity between the different sales organizations and we are involved in many transformation discussions with SAP and our GSA partners -- GSI partners, and we will see more that test is a key part of that, because it has an incredible value proposition which allows customers to drive not only -- not necessarily savings, but time benefits. And really automation of testing is one of the biggest challenges customers have in a big SAP environment. And we feel we do an excellent job helping customers achieve significant test results, which obviously, help mitigate any risk associated with a go live.\\nRaimo Lenschow: Yes, okay, perfect. Then, one for Ashim. As you think about like it does look like the demand trends are stable, and we kind of could potentially look for better times ahead. Obviously, you have the best net ARR addition for a few quarters now as well. How do you and you guys -- but also you guys have been very disciplined around cost and margins, but at some point, you need to think about lead times for sales people, et cetera. How do you think about that balance of like getting ready for eventually better times versus where you are at the moment? What are you seeing in terms of productivity gains, proceeds, et cetera, for example, to kind of drive this going forward? Thank you.\\nRob Enslin: I mean, I would start with we have a very powerful business model, Raimo, strong gross margins that allows us to invest while still generate cash and margins. And so we have been investing, whether that is within our product team as well as our sales team. I mentioned earlier, we're investing in our frontline sales team and our sales capacity. And we'll continue to invest in areas where we see the right returns and the right investments. And Rob and the team looks at that on an ongoing basis, and we do as a leadership team. At the same time, we also are looking for efficiencies. So together with that I think we're constantly in investment mode we feel very positive about our value offering and we feel like we can both drive that investment while still generate cash and margins.\\nOperator: Our next question comes from the line of Terry Tillman with Truist. Please proceed with your question.\\nTerry Tillman: Yes, congrats from me on the quarterly results, first of all. I guess, Rob, maybe the first question is, maybe I'm pretty simple here. But NorthStar roadmaps seem like a no brainer, like how much of your customer base [indiscernible] your larger enterprises. Have you actually unleashed the NorthStar roadmaps on? And the second part of my first question is, I'm hearing a lot from you about full platform adoption. Have you done anything with product or packaging to create less inertia to move all in with the platform and then add a follow-up for Ashim.\\nRob Enslin: Sure. Great questions. So I would say, we are doing a lot around pricing and packaging, and obviously pricing, packaging and solutions. And it's a constant upgrade, I would say of that. But you got to be very careful about how you actually bring that into the market. And we're focused on really simplifying SKUs and so making it very simple for customers to purchase. And also make it simple for customers to mix and match solution sets with AI units or bots in a way that's very simple, so that they can actually expand with less friction and we are doing significantly more of that. NorthStar is a combination of a great value based tool that has to be connected to the C-suite. You have to have sellers that are able to communicate and as we enable our organization more and we get more references, we are able to scale more and continue to scale the organization, so there's still plenty opportunity for us to continue driving that and continue to work on NorthStar with organizations.\\nTerry Tillman: That's great. Thank you for that, Rob. And I guess, Ashim, in terms of all the work on the go-to-market side this year, is there potentially a further drift in NRR? Could it actually improve from here? I mean, it's already best-in-class. I'm just kind of curious how you could see NRR trends potentially with more of the harvesting of all this work on the go-to-market side. Thank you.\\nAshim Gupta: I just say we continue to be pleased with where we are and how we're executing and we're going to look forward to closing out fourth quarter and we'll provide more updates next year.\\nOperator: Our next question comes from the line of Fred Havemeyer. Please proceed -- with Macquarie. Please proceed with your question.\\nFred Havemeyer: Hey, thank you very much. And I'd also like to congratulate you on a very strong quarter. I wanted to focus once again on generative AI. But firstly, looking at your product to see it being integrated throughout your portfolio and your suite, I'm looking forward to [indiscernible] what you can do there. But I wanted to take a, I guess a bigger picture, look at the longer term outlook of the market considering some of the impressive results [indiscernible] autonomous agents using for example, like GPT, for blending vision texts, et cetera. I'm curious, considering that UiPath has recently been one of the original autonomous agent companies with your attended and unattended RPA, how do you think about the market evolution over time with [indiscernible] or agents like functionality being built using generative AI now?\\nDaniel Dines: Well, I think that direction in the market that we are seeing right now, around autonomous agents prove a bit -- our approach that always said that AI plus automation is the thing that drives the biggest outcomes for our customers. Actually, this is what we are seeing a lot of our customers after the initial, a little bit of a pause around how AI is going to help me with my automation, they realize that they need powerful automation platform in order to harvest the power of AI. And going forward on a longer term basis with UiPath are in one of the best positions to build the next generation foundational models that understand processes, tasks, screens and documents, the type of multimodal that is built in, in order to drive automation. So it's -- to me, it's clear that the world is going into that direction. And again, we are really in a very good position to take advantage of it.\\nRob Enslin: Thank you for that. I'm looking forward to having an intern as a service to be able to help out autonomously and everything. I hope also just related to the topic here of generative AI, I’ve consistently heard feedback about the difficulty of organizing one's own data within an enterprise in a way that's useful. And so I'm curious what you're hearing from your customers about how they're beginning to and attempting to approach their own generative AI strategies and whether UiPath's automation platform is really able to help customers more holistically across the board with generative AI.\\nDaniel Dines: Yes, actually, this is one of the major use cases where automation is used right now in order to fine train customer [indiscernible] were even specialized AI. And it's frankly one of the hot discussion points with our partners around the world. We had interesting discussions in Japan and in the U.S with companies like Accenture (NYSE:ACN), were Deloitte that are very interested in to how they can leverage automation to cleanse the data and to accelerate custom training of [indiscernible] and specialized AI.\\nOperator: Our next question comes from the line of Siti Panigrahi with Mizuho. Please proceed with your question.\\nSiti Panigrahi: Thank you. So it's good to see this net new ARR bounce back. So are you speak with your customer? What do you think the enterprise spending train would be next year in terms of priority for automation spending?\\nRob Enslin: Yes, I mean, we've done -- we've looked at different analysts, different markets and many analysts will tell you that they see cloud or [indiscernible] data and cyber and automation is the key trends in terms of spending next year. But I would caution and say, customers are, as I said early on are very thoughtful in how they want to spend, where they want to spend and where -- how they're going to fund it, which benefits us and benefits us in the way that we actually approach customers. I don't see a significant change in that spending. I do think that customers are going to look to see how AI and AI automation and Gen AI can help them drive more efficiency and deeper levels of efficiency in the organization and look at different models when it comes to customers and our customers, how they communicate with customers, how they drive customer journeys as well. And we are focused on helping our customers with that approach in a deep way. Also connecting meaningful inputs and outputs in companies around documents and social communication emails into this process in order to drive real process orchestration at a different level.\\nSiti Panigrahi: Thank you. And, Ashim, a follow-up. If I capture your total customer count, it seems like it does come down this quarter. But you guys also had a pretty good -- on your enterprise segment spending. So wondering like, was there anything different you saw in different segments, like small versus mid to large segment?\\nAshim Gupta: Yes, look, we're very pleased with how our enterprise segment is performing. The macroeconomic variability that we've talked about has had more pronounced impact on the lower end of the market with smaller businesses. And that's where we see the majority of our churned customers, which Rob mentioned also earlier. Overall, I think our strategy remains consistent. We're focused on the quality of customers. And we define quality as customers with a high propensity to buy and we liked the way the teams are executing against that strategy.\\nSiti Panigrahi: Great. Thanks for the color.\\nOperator: Our next question comes from the line of Michael Turits with KeyBanc. Please proceed with your question.\\nMichael Turits: Hey, guys, congrats on solid execution. So two questions. One, where are we in terms of the shortening of the deployment time for the average bot? It seems like it has already improved and does the rollout of autopilot help shorten that time even further? And then have a follow-up for Ashim on [indiscernible]. Thanks.\\nDaniel Dines: This is always one of our major product focus on how can we shorten the adoption curve for our customers. And we are already seeing with our autopilot family that is in private preview some really good results with the initial set of few hundreds of customers that are testing the product. So, yes, I would say that this is going to be a significant driver for adoption. And both for advanced developers, where they will get to really increase the productivity to also citizen developers that will get started faster. And also, I would like to mention that training our specialized Document Understanding and Communication Mining models using Gen AI, it has already proven this is actually in production already. And it's been proven to accelerate the deployment quite a bit.\\nMichael Turits: Thanks, Daniel. And then, Ashim, on the NRR, it's great to see it having stabilized on an as reported basis. But could you go down another two points, as it did last quarter on an FX adjusted basis. So what are the puts and takes there? You're actually getting slightly easier comps. So what -- what's pulling that down at this point? And what are the things that are working in trying to actually increase in NRR?\\nRob Enslin: Yes, I mean, I look at our dollar base net retention adjusted for FX, it's 123%, which in our scale we're very pleased with and continues to be in the best-in-class territory. There always be a little bit of large numbers. And of course, every quarter's deal mix will change. But overall, we're really pleased with the strategy. I think we've had several marquee deals in the quarter. The penetration and the sale of the platform as Rob mentioned, we're making -- we continue to make really good progress on and we see very good strong performance in the enterprise segment, particularly in North America. A lot of the -- whatever headwinds we do see, we see really pronounced in the more pronounced in the lower end of the market. But overall our strategy is on customers with a higher propensity to buy. We feel like we are executing against that strategy and we're seeing that results in the deal quality and the customer quality though -- the end -- the customer quality in the quarter.\\nOperator: Our next question comes from the line of Scott Berg with Needham & Company. Please proceed with your question.\\nScott Berg: Hi, everyone. Nice quarter here. Thanks for taking my questions. Your net new ARR was up year-over-year for the first time, I believe since fourth quarter fiscal '22. So nice change there on the positive side. I think most of the questions have been on your execution within the quarter and have a commentary from the demand end of the market. Are you seeing any changes around your customers or the environment more mid market enterprise? Because you mentioned the down market a couple different times in the macro impact there. But are you seeing any sort of positive change upward in demand from whether it's existing customers or [indiscernible]?\\nRob Enslin: Yes, I mean, I do definitely see positive demand, positive conversations in the market. We feel like we have a significant seat at the table with the AI, open AI, chat GBT kind of discussion that's been taking place. And plus, having the platform has allowed us to have C level, sea level conversations. We have customers now that are really calling us to actually have discussions with. They've been using Core RPA since 2018, 2019 and they feel like there's an opportunity to expand it. They're not getting hold of that. They're not getting as much value as they believe they could get out of it. And those conversations are taking place. And we're able to actually showcase proof points on how to do this. So that -- yes, definitely more positive and definitely feel good about the discussion of how we can help customers and how we are more relevant to the business discussion.\\nScott Berg: Got it. Helpful. And then, Ashim, your current RPO metrics growth rate accelerated pretty meaningfully quarter-over-quarter. Was there any one-time anomalies? I don't know, early renewals or something that impacted that abnormally in the quarter? Was that a pretty clean calculation metric? Thank you.\\nAshim Gupta: No, major abnormalities like early renewals, et cetera. I think I've mentioned we had several marquee deals in the quarter and record number of deals above a $1 million for the third quarter. So it's just its execution and really good deal quality that's driving that metric.\\nOperator: Our next question comes from the line of Michael Turrin with Wells Fargo. Please proceed with your question.\\nMichael Turrin: Great. Thanks for taking the question. Two part, I'll just ask up front. Rob, given investments you mentioned in reaching the C-suite, wondering if you can provide us with your perspective around how enterprise customers are approaching IT budget growth into next year and automation as a part of that. And then, as a second part for, Ashim, can you comment on the visibility you have into 4Q forecast currently and any swing factors for us to consider in terms of potential upside there? Thank you.\\nRob Enslin: Yes, Michael, I'll take it now, let Ashim go later. Because we are having these conversations in the boardroom, customer -- customer conversations much wide to C level suite. We are much earlier in the budget cycles than we previously been. And we are actually much more important in the budget cycle than I believe we've been previously. I think there's a couple of really key points that stand out right our industry. How we've driven industry and how we made ourselves relevant in particular industries and the value proposition there. I do believe that the global systems integrators and how they've come along and support of us, the messaging around SAP has driven while and the platform proof points around, DEU and announced the customer that are $250 million savings in my earning script. This is what's really allowing us to participate. And so we feel like we are meaningful part of where the budgets are going next year. I think everybody is looking at how tight their budgets are, and trying to find out how to move from one pocket to another pocket on the innovation side. And we are definitely on the innovation side of customers thinking and then I'll hand it over to Ashim.\\nAshim Gupta: Yes, so just in terms of our guidance, we've factored in the macroeconomic variability that we've talked about, and it includes the current view of our deal mix. And we look at just executing -- the teams are focused on executing a good end to year-end, close to the year, and that's what we're focused on. That's what I've [indiscernible] on that.\\nOperator: Our next question comes from the line of Bryan Bergin with Cowen. Please proceed with your question.\\nJared Levine: Thanks. It's actually Jared on for Bryan today. In terms of the competitive environment any changes from what you're seeing from Microsoft ServiceNow or Automation Anywhere?\\nRob Enslin: Yes. I have a look at -- if I look at ServiceNow, we don't see them in many opportunities. What I will tell you, our service, our connectors, the one of the most, it is the most popular downloaded connected to UiPath and we continue to partner with them. And our partnership with Microsoft around co-pilot and [indiscernible] pilot, the discussions we're having with them continues to be robust. And we are very happy with where we going with Microsoft, and that partnership. And as for a lot of the traditional RPA vendors, I feel that we are now -- we've broadened our skill sets and what we are offering we don't see them that often anymore.\\nJared Levine: And then do you have any data points on how the new bundled pricing packages is increasing revenue, uplift per client or anything around revenue uplift from the reduction of SKUs as part of the sales refresh?\\nRob Enslin: I don’t have specific that we can point to now. I mean, we are looking at the results is probably a little bit too early for us to kind of come out with where we are on that. But we do feel good that early signs of, I would just simply say, easier for customers to understand, easier for them to move. Definitely -- they definitely like how they're able to use AI units across multiple products and interchange them as they think of new use cases to advance their business as well.\\nOperator: Our next question comes from the line of Alex Zukin with Wolfe Research. Please proceed with your question.\\nEthan Bruck: Hey, guys, this is Ethan Bruck on for Alex Zukin and thanks for taking question and congrats on the nice results. I guess, Rob, this one might be for you. If we were to like unpack a little bit and like more color, there's some stuff you saw in the quarter in terms of just either like certain verticals where we saw -- I know you mentioned on like the variable demand, but just kind of comparing from the macro, maybe by vertical basis, if something's got improved, or were more steady versus the last few quarters you've seen, and then just a bit on renewal behavior in terms of what it's like more -- buying more licenses, more [indiscernible] just kind of how was the kind of renewal conversation in 3Q compared to what you saw at the beginning of the year?\\nRob Enslin: Yes. Look, the last quarter was a very strong quarter for public sector for our federal business actually in the U.K., and in the United States. And that was really because the Federal businesses are looking at the platform in a unique way, and we offer some really incredible capabilities around Document Understanding, connecting that to attended automation, focusing on how citizens can improve their [indiscernible]. So we've seen us our -- our market move in public sector, that also is based on talent, the talent we bought into that environment and how we've driven that talent. And now we actually have some really good industry-based activity in public sector, in revenue services, in DoD, in customs, in USDA, that actually can really help all of our public sector activity globally. When you look at traditional markets, like when you look at traditional -- more traditional markets, telcos, CPG companies, manufacturing, retail companies, they're all driving efficiencies and driving efficiencies in retail, it's how fast can open up stores? How fast can I get employees on? How do I make [indiscernible] my price points are valid, and my price points are real, and I don't have any price point issues between stores and so on. And actually automation really drives some significantly capabilities in that we play a significant role. If you look at capital intensive telcos, it's all about efficiencies, and how can I get competitive advantage efficiencies because my markets are mature. And the only way I actually get growth is by competing my competition and moving subscribers from one provider to another provider. And in that space, we're playing pretty well. So it's variable in which industries you're in, it's variable in terms of how they see growth and where they are investing. But there's one common theme across the board and that's efficiencies. With renewals, it gives us an opportunity to drive a platform, to drive the platform earlier, to actually start having the discussions with customers most -- many months or many quarters earlier in terms of why move to the platform. Here's the value. Let's work on NorthStar with you. Let's get in the office. We've got an SAP transformation. We can help you with that as well. Let us bring one of our big global [indiscernible] showcase the transformation. And that really helps drive a different cadence with renewals. It's not just about renewing what you've already got, it's about renewing what you want to do in the future and how you want to add more business value in the future. And I think we're incredibly good at that right now.\\nEthan Bruck: That's super clear. And then just, Ashim, as a quick one for you, the incremental margins in the quarter, they're really good. They've been really good all year. I guess just, if you remind us just some of the puts and takes we should keep in mind around how to think about the margin profile kind of the in the out years? Thanks again.\\nAshim Gupta: Look, we -- I would refer you back to our Investor Day pitch [ph] last year, where we talked about long-term operating -- just our long-term operating model there. We're really pleased with the execution on the margin front. We've been ahead of that curve in terms of just execution. And we feel really good about our ability to operate with discipline and efficiency while still investing in growth for the company.\\nOperator: That is all the time we have for questions. I'd like to turn the call back to management for closing remarks.\\nRob Enslin: Yes, I just wanted to say thank you. Thank you to all of you for all the support. It's great to have you all on the call today. Happy holidays to you and your family. May you enjoy a great time. [Indiscernible].\\nOperator: Ladies and gentlemen, this does conclude today's teleconference. Thank you for your participation. You may disconnect your lines at this time, and have a wonderful day.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " \"Synopsys (NASDAQ:SNPS), a leader in electronic design automation (EDA), reported record-breaking revenue of $1.599 billion in the fourth quarter of fiscal year 2023. The company highlighted their sustained growth over the past five years, with a focus on AI-driven product capabilities and unique collaborations. In the earnings call, Synopsys also announced their decision to explore strategic alternatives for their software integrity business. Looking ahead, the company expects solid growth across all geographies in 2024, despite a challenging near-term growth environment in China.\\nKey takeaways from the call include:Synopsys reported an annual revenue of $5.84 billion for fiscal year 2023, a 15% increase year-over-year. The company also expanded its backlog by $1.5 billion to reach $8.6 billion.The company achieved a non-GAAP operating margin of 35.1%, a 26% increase in non-GAAP EPS, and generated $1.7 billion in operating cash flow.Synopsys emphasized their focus on expanding their AI-driven product capabilities and forming unique collaborations to strengthen customer relationships.The company provided 2024 guidance targets, including revenue between $6.57 billion and $6.63 billion, a 37% non-GAAP operating margin, and non-GAAP EPS between $13.33 and $13.41.Synopsys announced their decision to explore strategic alternatives for their software integrity business, as they aim to prioritize investments in the design automation and design IP segments.The company also highlighted their achievements and momentum in their design automation and Design IP businesses, and emphasized their commitment to driving customer success through innovation and collaboration. They anticipate record growth in their hardware-assisted verification business and are building inventory to meet customer demands.\\nSynopsys executives discussed the growth of their AI capabilities and early stages of monetization. The company is seeing varying levels of adoption, with some customers using AI on one or two projects and others being more aggressive. The executives also addressed questions about the company's operating cash flow (OCF) guidance, stating that OCF will be down next year due to a $600 million headwind from cash taxes and additional amortization.\\nDuring the call, the transition of CEO from Aart de Geus to Sassine Ghazi was also discussed, with Aart expressing confidence in Sassine's leadership and asking for support from investors. The call concluded with thanks to participants.InvestingPro InsightsSynopsys has been making waves with its record-breaking revenue and strategic business moves. To further understand the company's financial health and investment potential, let's delve into some key metrics and tips from InvestingPro.\\nInvestingPro Data presents an impressive picture with a market capitalization of $82.62 billion, reflecting the company's substantial market value. The P/E ratio, a measure of a company's current share price relative to its per-share earnings, stands at a high 68.59, indicating investors' willingness to pay a premium for Synopsys's earnings. This is slightly adjusted in the last twelve months as of Q4 2023 to 63.74. Furthermore, the company's revenue growth has been noteworthy, with a 14.98% increase in the last twelve months as of Q4 2023, which accelerated to 24.51% in Q4 2023 alone.\\nInvestingPro Tips highlight several strengths that Synopsys possesses. The company's high earnings quality is evident, with free cash flow exceeding net income, suggesting efficient operations and profitability. Additionally, Synopsys yields a high return on invested capital, demonstrating its effective use of capital to generate profits. Among the plethora of insights available on InvestingPro, seven analysts have revised their earnings upwards for the upcoming period, pointing towards a positive outlook on the company's financial future.\\nFor investors interested in a deeper dive into Synopsys's financials and performance metrics, InvestingPro offers an extensive list of additional tips. These tips provide a comprehensive analysis, from the company's impressive gross profit margins to its high return on assets and stockholders' high returns on book equity.\\nTo make the most of these insights, investors can take advantage of the special Cyber Monday sale, with discounts of up to 60% on InvestingPro subscriptions. Additionally, using the coupon code sfy23 will secure an extra 10% off a 2-year InvestingPro+ subscription. With 24 additional tips listed in InvestingPro, subscribers can gain a thorough understanding of Synopsys's investment potential and make well-informed decisions.Full transcript -  Synopsys Inc  (SNPS) Q4 2023:Operator: Ladies and gentlemen, welcome to the Synopsys Earnings Conference Call for the Fourth Quarter and Fiscal Year 2023. At this time, all participants are in a listen-only mode. [Operator Instructions] I would now like to turn the call over to Trey Campbell, Senior Vice President, Invest Relations. Please go ahead.\\nTrey Campbell: Thanks Lisa. Good afternoon everyone. With us today are Aart de Geus, Chair and CEO of Synopsys; Sassine Ghazi, President and COO; and Shelagh Glaser, CFO. Before we begin, I'd like to remind everyone that during the course of this conference call, Synopsys will discuss forecasts, targets, and other forward-looking statements regarding the company and its financial results. While these statements represent our best current judgment about future results and performance as of today, our actual results are subject to many risks and uncertainties. That could cause actual results to differ materially from what we expect. In addition to any risks that we highlight during this call, important factors that may affect our future results are described in our most recent SEC reports and today's earnings press release. In addition, we will refer to certain non-GAAP financial measures during the discussion. Reconciliations to their most directly comparable GAAP financial measures and supplemental financial information can be found in the earnings press release, financial supplement and 8-K that we released earlier today. All of these items, plus the most recent investor presentation, are available on our website at www.synopsys.com. In addition, the prepared remarks will be posted on our website at the conclusion of the call. With that, I'll turn the call over to Aart de Geus.\\nAart de Geus: Good afternoon. In Q4, we exceeded the high end of all our guidance targets and delivered another quarterly revenue high at $1.599 billion. Q4 thus capped a record year, growing revenue by 15% to $5.84 billion, with strong orders expanding backlog by $1.5 billion to $8.6 billion. We further improved non-GAAP operating margin to 35.1%, increased non-GAAP EPS by 26%, generated $1.7 billion in operating cash flow while maintaining an exceptionally strong balance sheet. Clearly, Synopsys has moved forward with sustained momentum. Over the last five years, we've grown revenue at 13% CAGR, expanded non-GAAP operating margin by 13 points, and increased non-GAAP EPS at a 23% CAGR. Through the year, we also widened our differentiation by substantially expanding our AI-driven product capabilities, but also through unique collaborations that strengthened our customers' differentiation while cementing deep long-term relationships. We thank our employees for their passion and dedication, and our customers for their business and trust in Synopsys. Meanwhile, dark geopolitical clouds are inflicting unimaginable harm in multiple conflict zones. Our hearts hurt with deep compassion for our employees, families, colleagues, customers, and all others impacted by pain, loss, and uncertainty. Yet, we will never give up believing in the positive potential of humanity. It is thus heartwarming to see how fast our teams have turned compassion into caring, caring into action, and how respect and support for each other is an active norm at Synopsys. Let me now briefly share some thoughts on the state of the industry and our company before I pass the baton to Sassine. From the first days of Synopsys 37 years ago, Synopsys has enabled and navigated the exponential ambition that came to be defining the semiconductor industry and which in turn radically impacted the world. Our initial contribution, synthesis, revolutionized digital design. We ushered in the transition from CAD, Computer Aided Design, to EDA, Electronic Design Automation, so far delivering roughly a 10 million X increase in design productivity. During Synopsys' entire existence, the vast majority of our products have been the state-of-the-art. Together with the leading foundries, we thus empowered digital age exponential, referred to as Moore's Law. And then at the very moment that the economics of classic Moore's Law are slowing down in terms of transistor performance and cost improvements, the era of big data and AI becomes real, triggering enormous compute needs on the horizon. About eight years ago, Synopsys forecasted this age of smart everything would become the driver for the semiconductor growth to a trillion dollars in this decade. And here we are. This year's Generative AI advances and furious adoption clearly fulfill our vision. As Pervasive AI is now massively underway, the classic Moore's Law era in turn is morphing into the SysMoore era, systemic complexity but still with a Moore's Law exponential ambition. SysMoore is happening in front of our eyes by reinventing architectures not based on a single chip, but on multiple chips, tightly connected, or even stacked on top of each other with extreme proximity. These so-called software-defined multi-die architectures will enable massive increases in the number of transistors available. And again, Synopsys is at the heart of the heart of catalyzing exponential impact. Our investments in multi-die design, our massive collection of IP building blocks in many silicon technology, our prototyping and electronic digital twinning technology that lets customers run software before the hardware actually exists, are all essential enablers driving the new race into AI driven computation. Adding one more spark to our technical leadership, Synopsys also pioneered the use of DeepLearning.AI in chip design. Applied to optimization, verification, and test, production results are outstanding, and adoption is broad and rapid. Most recently, our announcement of exciting GenAI capabilities adds yet another angle to driving the state-of-the-art forward. If nothing else, my enthusiasm for both the CISMO opportunity and for our technology advances should give you a sense how Synopsys is on the move. Talking about on the move, we're also well on the way in our executive leadership transition. I have great confidence, expectations, and enthusiasm for Sassine Ghazi as our next CEO. Sassine, please give us your perspective, vision and ambition for Synopsys. The floor is all yours.\\nSassine Ghazi: Thank you, Aart, for your pioneering work in our industry and for building Synopsys into one of the world's essential semiconductor ecosystem companies. I am profoundly grateful for the opportunity to succeed you as CEO, building on our strong foundation and propelling Synopsys to the next wave of growth. Let's turn to market trends. Despite global macroeconomic uncertainty, our customers continue to prioritize R&D investments and chip design starts remain robust. We leave 2023 with $8.6 billion in non-cancellable backlog, and have a time-tested business model that balances dynamic growth with macro resiliency. We expect solid growth across our geographies in 2024. But our outlook reflects a continued challenging near-term growth environment in China. China is an adaptable and large market. However, given the combination of Entity List and technology restrictions and a weaker macroeconomic outlook, we believe more pragmatism in our 2024 China forecast is appropriate. Technology trends continue to create a rising tide for our business. Chief among those trends is a new era of AI driven productivity. AI is reshaping industries and providing breakthrough solutions for intractable challenges, like the 15% to 30% percent design resource shortage the semiconductor is facing this decade. We pioneered AI driven semiconductor design and are relentlessly advancing our AI capabilities so that we can drive step-up function improvement in our customers’ productivity and thus play a greater role in their success. Recently, at Microsoft (NASDAQ:MSFT)'s Ignite conference, we announced a breakthrough Generative AI capability for accelerating chip design, Synopsys.ai Copilot. The new capability is the result of a strategic collaboration with Microsoft to integrate Azure service that brings the power of GenAI into one of the most complex engineering challenges, the design process for semiconductors. The integration of GenAI across Synopsys.ai provides chip designers with collaborative capabilities that offer expert tool guidance, generative capabilities to enable RTL and collateral creation, and fully autonomous capabilities for workflow creation from natural language. We're engaged with leading chip makers, including AMD (NASDAQ:AMD), Intel (NASDAQ:INTC), and Microsoft, to deliver the value of GenAI across the Synopsys.ai full EDA stack from design, verification, test to manufacturing. We are at a very early stage of this new AI era, but our initial customer results are exceptional. AI is key to massively unlocking customer productivity, and we are increasing our investment to accelerate the Synopsys.ai roadmap. Beyond AI, we see multiple other secular tailwinds providing our design automation and Design IP business expanding growth opportunities. With the slowing of Moore's Law, increasingly, architecture and design automation are the main levers in delivering semiconductor PPA gains, even as insatiable use case demands push the frontiers of performance and performance per watt. Multi-die implementations are accelerating as our customers seek to optimize cost and yield for these large complex designs. And our customers who rely on our critical competencies from silicon to software, now require a systems level approach, both at the semiconductor device level with multi-die and in the electronic design, software bring-up, and software validation of full systems, like today's software defined cars. Our Design IP business also has strong wind in its sails. Applications crave ever faster ingest and throughput, resulting in faster protocol migrations and increasing IP content value per device. Customers are prioritizing scarce design resources to focus on their critical architectural differentiation and turning to us as an integral part of their chip design development teams for their foundation and interface IP needs. And now, all three leading edge foundries are making Synopsys the advanced node IP vendor of choice. They are partnering with us on a broad range of IP titles to minimize risk and accelerate silicon success. Our design automation and Design IP businesses have both leadership technology and market positions with industry trends playing to our strengths. We're increasing our investment in these segments to capture more of this growing TAM. We started our investments in software integrity with the acquisition of Coverity in 2014. Software security was a pain point for every company, and risk surfaces were expanding. Customers were searching for innovative approaches in quality and security testing to help reduce the risk of software failures and security breaches. And we developed the broadest portfolio to meet that need. Flash forward to today, our software integrity business has become the leader in application security testing with industry leading team delivering over $0.5 billion in trailing 12 months revenue at mid-teens non-GAAP operating margin. We are proud of the significant progress we've made over the last nine years and believe the future opportunity remains attractive. At the same time, we have compelling investment opportunities in design automation and Design IT with much higher expected growth and return profiles. Following our strategic portfolio review and in consultation with the company's Board of Directors, we have decided to explore strategic alternatives for the software integrity business. As part of this process, we're considering full range of strategic opportunities. We will provide an update after we conclude that process. Based on these market and technology trends and with high confidence in our business, here are our 2024 guidance targets. We expect 2024 revenue between $6.57 and $6.63 billion. We expect to deliver 37% non-GAAP operating margin, a 200 basis point improvement versus last year. We expect full year non-GAAP EPS between $13.33 and $13.41. Shelagh will discuss the financials in more detail. Now I'll share some segment highlights starting with design automation. This quarter, Synopsys.ai was selected by AspenCore to receive the World Electronics Achievement Award for EDA Software of the Year. We're proud of the recognition, but even more excited by the strong customer adoption for Synopsys.ai across the design flow. A major North American hyperscaler made a major commitment to use DSO.ai after demonstrating PPA and productivity benefits on consecutive HPC projects. In verification, we engaged with over 20 customers in Q4, demonstrating up to 10x faster turnaround time. While in test, we added eight new customer engagements with [KIOXIA] (ph) publicly highlighting more than 50% pattern reduction. Finally, we and TSMC announced that our analog migration flow through Synopsys.ai is enabled across TSMC's advanced process technologies. We are also seeing great results deploying Synopsys.ai internally with our IP teams. Internal IP teams are seeing 10x turnaround time improvements in time to target verification coverage and have deployed analog design migration flows for TSMC 2 nanometer. Fusion Compiler continues to win key designs including the leading edge Arm mobile core for the industry's first implementation for a gate-all-around based mobile SoC. In combination with DSO.ai, Fusion Compiler also delivered 10% better power on gate-all-around based mobile GPU and modem designs. We saw continued momentum in sign-off, delivered by our leadership family of prime tools. We won multiple engagements with PrimeTime, PrimeClosure, and PrimeShield, and saw the world's top three data center providers adopt PrimeClosure to get the fastest ECO closure time for five 3-nanometer SoCs. Expanding our multi-die ecosystem, we received the prestigious leadership award from TSMC, OIP 2023 Partner of the Year, for developing the industry's first 3D IC design prototyping solution, supporting the new industry standard 3D blocks. Verification, product momentum also remains strong. This quarter, we announced our AI-driven next generation Verdi solution, which continues its lead in functional debug with deployment already at more than top semiconductor companies. In hardware assisted verification, we delivered another record year. In Q4, ZeBu won against competition at two large North American hyperscalers, and we expanded our HAPS footprint with a large North American systems company and a large Asian semiconductor company. Now turning to Design IP. This quarter, we won our first 2-nanometer interface IP engagement with a leading mobile company and are now in production at 3 nanometer with foundation IP for a high-volume PC chip. We delivered a key multi-die proof point in concert with Intel and TSMC on UCIe interoperability. The demonstration at Intel Innovation showed die-to-die interconnect over UCIe between Synopsys IP on TSMC and 3E and Intel Foundry Silicon. We saw two other key technology proof points this quarter. We demonstrated interoperability for our 224 gig Ethernet PHY IP and PCIe 6.0 IP, both industry firsts. On the processor IP side, we announced a new addition to the ARC processor IP portfolio, the RISC-V ARC-V processor IP. This product allows customers to choose from a broad range of flexible, extensible processor options that deliver optimal power performance efficiency for their target applications. Finally, we delivered a significant win in automotive, displacing competition at a marquee customer in a multi-generation, multiple project agreement. Now, to the software integrity segment, which delivered solid growth against the backdrop of continued macro headwinds for enterprise software. In Q4, we saw over 50% year-over-year growth in our Polaris (NYSE:PII) software integrity platform. Polaris is a SaaS based application security testing solution optimized for the needs of development and DevSecOps teams. We were also recently recognized as a leader in the Forrester Wave for Software Composition Analysis. This was based on an evaluation of Black Duck, our software composition analysis solution. In summary, we had an outstanding Q4 and FY 2023 financial results and operational execution and take tremendous forward momentum into 2024. We have a resilient business model and our customers continue to prioritize investments in the chips and systems that position them for future growth. We are aligning our portfolio investment with the greatest return potential to accelerate our growth, deepest thanks to our employees, partners, and customers for their passion and commitment. With that, I'll turn it over to Shelagh.\\nShelagh Glaser: Thank you, Sassine. 2023 was an excellent year highlighted by record revenue, record non-GAAP operating margin, and record earnings. We continue our strong execution with financial discipline and are confident in our business heading into 2024, driven by our execution and leadership position across our segment, robust chip and system design activity by our customers who continue to invest through semiconductor cycles and with $8.6 billion in non-cancellable backlog, the stability and resilience of our time-based business model. As a result, while the macro environment is uncertain. We expect to grow revenue 12.4% to 13.5%, expand non-GAAP operating margin by approximately 2 percentage points, and drive non-GAAP PPS growth of 19% to 20% in 2024. Let me provide some highlights of our full year 2023 results. We generated total revenue of $5.84 billion, up 15% over the prior year, with double digit growth across all key products and geographies. Total GAAP costs and expenses were $4.6 billion and total non-GAAP costs and expenses were $3.8 billion, resulting in non-GAAP operating margin of 35.1%. GAAP earnings per share were $7.92 and non-GAAP earnings per share were $11.19, up 26% year-over-year. Now onto our segment. Design automation segment revenue was $3.78 billion, up 14% driven by strengths in EDA software and hardware. Design automation adjusted operating margin was 38.1%. Design IP segment revenue was $1.54 billion, up 17%, driven by broad-based strength. Design IP adjusted operating margin was 34.5%. Software integrity revenue was $525 million, up 13%, and adjusted operating margin was 14.5%. Turning to cash. Operating cash flow for the year was $1.7 billion. We ended the year with cash and short-term investment of $1.59 billion and total debt of $18 million. During the year we completed buybacks of $1.2 billion or 80% of free cash. Now to targets which reflect the impact from export control regulations and assume no further changes for the year. Based on our current assessment of timing of hardware and IP deliveries, we expect the first half, second half split of approximately 48% to 52% for revenue and non-GAAP EPS. For fiscal year 2024, the full year targets are revenue of $6.57 billion to $6.63 billion, total GAAP costs and expenses between $5.0 billion and $5.05 billion, total non-GAAP cost and expenses between $4.14 billion and $4.18 billion, resulting in non-GAAP operating margin improvement of roughly 2 percentage points, non-GAAP tax rate of 15%, GAAP earnings of $9.07 to $9.25 per share, non-GAAP earnings of $13.33 to $13.41. Cash flow from operations of approximately $1.4 billion, which includes an impact of approximately $200 million of 2023 taxes that we will pay in ‘24 and approximately $400 million of higher cash taxes due to the amortization of R&D expense. Following 2024, we expect cash tax growth rate to be approximately in line with operating income growth over a multi-year period. Now to targets for the first quarter, which includes an extra week compared to the first quarter of fiscal 2023. Revenue between $1.63 billion and $1.66 billion, which includes approximately $70 million from the extra week. Total GAAP cost and expenses between $1.22 billion and $1.24 billion, total non-GAAP cost and expenses between $1.02 billion and $1.03 billion, GAAP earnings of $2.40 to $2.50 per share, non-GAAP earnings of $3.40 to $3.45 per share, including approximately $0.14 from the extra week. Our press release and financial supplement include additional targets and GAAP to non-GAAP reconciliation. I also want to highlight that we will be hosting our Investor Day on March 20th, which will be held in conjunction with our Synopsys Users Group event in Santa Clara. We look forward to seeing many of you there. In conclusion, we entered 2024 with momentum and confidence, reflecting our leadership position across our segments, robust design activity by our customers who continue to invest through semiconductor cycles, and the stability and resiliency of our time-based business line. With that, I'll turn it over to the operator for questions.\\nOperator: Thank you. [Operator Instructions] We'll take our first question from Harlan Sur with JP Morgan.\\nHarlan Sur: Good afternoon and congratulations on the strong execution and fiscal ‘24 guidance.\\nAart de Geus: Thank you.\\nHarlan Sur: You're welcome. Hardware verification, emulation, prototyping, I mean, this has been a big contributor to your double digits revenue growth profile over the past number of years. Lots of customers focusing on verification on these very complex chip designs, a lot of them wanting to get a head start on the embedded and application software development. Is the team anticipating another strong year next year for hardware? Will it be growing at a double-digit sort of growth rate within your fiscal ‘24 guidance? And if I look at inventories, right, they're up 53% year-over-year to kind of record levels, which I think is a good indicator of a strong hardware pipeline, but wanted to get your views.\\nSassine Ghazi: Yeah, thanks for the question. In terms of the need, exactly the way you highlighted it, the requirement to develop software early and having a hardware assisted verification to enable that step in the process is not only continuing, is accelerating with many system companies are trying to design their chip or even if they're getting a chip from a semiconductor company, they want to start their software development and verification of the system early. And this is where our HAPS and ZeBu platform comes in to enable that part of the solution. We're anticipating that to continue into 2024. As you saw as well, we had a record year for our hardware-assisted verification in ‘23, and we don't anticipate anything different that will change going into 2024. As for the inventory comment, I'll turn it to Shelagh for comments.\\nShelagh Glaser: Sure, as Sassine said, we had a record in 2023. We expect another record in 2024, and we're building inventory to be able to fulfill our customer demands.\\nHarlan Sur: Perfect. And maybe a similar question on your IP business, strong growth last year, right, up 17% going forward. I mean just continued tailwinds, right? Chip design complexity, driving more reliance on off-the-shelf IP licensing. And just as importantly, right, there's some pretty big transitions on the interface and connectivity side, PCIe Gen 5, CXL, DDR5, HBM3e. Looking at your pipeline backlog, customer programs, will the IP business be growing slower, faster in line with the fiscal '24 sort of total revenue profile?\\nSassine Ghazi: So again, exactly the way you're outlining the requirement, there is two factors. There's the complexity and then there is the different methods of designing an SoC or chip with multi-die is opening up the door for new protocols, UCIE, the PCIe, the CXL, the one that you've listed. There are different protocols for various markets for automotive. As they expand their sophistication in developing the electronic system, it requires different requirements for the IP, interface IP for an automotive application. What we outlined in the script as well is today, we are the leading supplier for TSMC, Samsung (KS:005930) and Intel Foundry business. And that puts us in a great position because most of these complex chip developments are on these advanced foundries. And today, we are in a very fortunate position to be the partner and the leader in providing the IP for that business. So we expect that growth to continue, given the market demand for the sophisticated chips will continue.\\nHarlan Sur: Great insights. Thank you.\\nSassine Ghazi: Thank you.\\nOperator: We will take our next question from Joe Vruwink with Baird.\\nJoe Vruwink: Great. Hi, everyone. I maybe want to start by parsing out some of the demand commentary. So first, there was a big step-up in backlog. I'd imagine that bodes well for 2024 and 2025 positive for revenue. At the same time, maybe the incremental moderation you're signaling in China, as I think about just the last 12 months, I think China contributed about 2 points worth of revenue growth, just the incremental revenue contribution from China. So arriving at the 13% growth guidance, are you really seeing kind of good strength, something better than 13% and then China is maybe just a more neutral factor in your outlook?\\nSassine Ghazi: We really took a balanced approach as we look at the guidance and the forecast for FY '24. We took into account some of the headwinds that we're seeing and of course, the number of the tailwinds that we are observing in the market. A couple of the headwinds, as we mentioned, one of them is China. And the two factors. One is the continued export restrictions and the other one that I'm sure all of you are observing, which is the macro economy inside China. The other headwind we took into account is the continued stress and pressure from enterprise software spending that does impact our software integrity business. From tailwinds, and this is where we get excited about is exactly what we're seeing, the AI as a megatrend is driving amazing silicon demand and that silicon demand is complex. It's on the most advanced nodes and it's giving us an amazing opportunity for our design automation and design IP. So as we took into account the both headwinds and the tailwinds. We came with a balanced view of the guidance around 13% midpoint for FY '24.\\nJoe Vruwink: Okay. That's clear. Thank you. And then just on the topic of Generative AI, I did want to maybe get your take, if it's possible to contrast just the pace of product development incorporating such a technology relative to what you've done in the past? And I guess the context for this question, as I think back to DAC in July, there were actually members of the Microsoft verification team speaking about how they wanted more robust tools from their EDA vendors that incorporated GenAI and we're not very far from July. And here you are with a product in conjunction with Microsoft and now Microsoft verification is using this in their workflows. So it seems like a very quick turnaround. Is it that much quicker than maybe what Synopsys was able to do in the past?\\nSassine Ghazi: Maybe we made it look too easy, Joe, but there was a lot of work that started well ahead of the DAC time frame. If I take a quick step back, when we talk about Synopys.ai, there are really four pillars under it. The one that we -- the first one we did produce was DSO.ai in the 2020 time frame. That was focused mostly on optimizing the -- our product and using every opportunity to leverage machine learning and AI in the product and around the product. The second pillar, which is what we're calling a collaborative capability using Gen AI, and that's what we announced with Microsoft, which is using a CoPilot approach for supporting our users for knowledge-based or workflow-based or results co-pilot and assistance. And that's in our announcement where AMD, Intel, Microsoft were some of the early users of the technology. Then we talked a little bit about what's coming down the path, which is around both generative and autonomous capability using Gen AI and natural language. We're super excited actually about the early results we're seeing with the co-pilot in terms of the productivity of our customers and users.\\nJoe Vruwink: Great. Thank you very much.\\nSassine Ghazi: Thanks, Joe.\\nOperator: We'll take our next question from Vivek Arya with Bank of America Securities.\\nVivek Arya: Thank you for taking my questions. If I remove the -- I think you said about $17 million or so impact of the extra week, you're guiding to roughly 12% sales growth right, which seems to be very conservative relative to the strong backlog that you have built up. So I'm curious, Sassine, I think you mentioned the impact of China and enterprise software and AI is kind of neutral, but then when I look at the 12% growth that is lower than what you had in the last three years, even though there is all this AI excitement that's starting now, so is it conservative? How should we kind of put this 12% in the context of the kind of growth rates we are used to seeing from Synopsys over the last three years?\\nShelagh Glaser: So, Vik, thanks for the question. This is Shelagh. I'll jump in on that. So we're obviously taking a balanced stance as Sassine had talked about at the we're at a 13% growth year-over-year. And we're seeing design automation, Design IP have incredibly strong momentum. You mentioned the $8.6 billion backlog. We're seeing that growth in line with our long-term targets. And we're seeing AI as a catalyst, but it's in its early inning. So we think it's a long-term growth catalyst. And we're looking forward to having it be able to help drive our business over the horizon. But the two headwinds are China, as we've talked about the macro situation in China and the restrictions that have been imposed on China, are having some dampening effect. We expect China to grow in '24, but at a lesser rate than it grew in '23. And the other headwind that we have is we are expecting a muted environment for enterprise software spend to impact our software integrity business, and we're assuming that, that business will be single-digit growth in 2024. So it's the combination of those headwinds and tailwinds that's leading to our 13% growth rate at the midpoint.\\nVivek Arya: Got it. And for my follow-up, the AI Copilot seems like a very interesting new product. I'm curious what is the right way to track how you are able to monetize it? Are you selling it as incremental feature? Are you selling it as a separate tool? What is the right way to just kind of track how successful you are with this capability?\\nSassine Ghazi: So the way we're thinking and we expect this over the last few calls, which is AI will be offered to our customers through a subscription license, meaning, if a customer wants to use an AI capability, be it a DSO.ai or VSO.ai, it will be offered as an incremental spend for the customer to get access to that technology. And in some early adopter cases where they’re still not ready to make the long-term commitment, we're offering it as well in terms of consumption base. They may be looking for one project to use it, et cetera. And the Copilot in particular case, it's still be early, fairly maybe early stage in terms of us to talk about monetization and how is it contributing overall. But those are the flexibility tools per se, we're providing the customer to get access to it.\\nVivek Arya: Thank you.\\nSassine Ghazi: Thank you, Vivek.\\nOperator: We'll take our next question from Jason Celino with KeyBanc.\\nJason Celino: Great. Thanks for taking my question. The backlog number, it's quite impressive. Looking back at the last couple of years, it looks like it's sequentially the biggest quarter-over-quarter increase we've ever seen, up 20% year-over-year. Just wanted to check how much of this has been driven just from the AI design activity or the AI tool revenue. It sounds like it was more broad-based, but just wanted to check.\\nSassine Ghazi: With AI, we communicated last quarter that what we're observing on average and that average emphasizing the average because in some cases, we were seeing a much bigger number or a lesser number. About 20% contract-over-contract growth when a customer is renewing their EDA agreement and asking for the AI capability to be added. The other point that I made earlier with Vivek, I'd like to emphasize as well, that we are in early stages of that monetization with AI. We still have customers that they are in early adoption, meaning if they have x number of projects, they may be using it on one or two projects and other customers, they started early and they're going more aggressively. But to give you a sense, it's roughly 20% on the EDA side contract-over-contract growth we're observing.\\nJason Celino: Okay. Interesting. Thank you. And then one question on the OCF guidance. It will be down next year. Are you saying that there's a $600 million headwind from cash taxes and then additional amortization and then are you saying it's going to grow just in line with net income growth that you're after? Can you clarify?\\nShelagh Glaser: Yeah, Jason, thanks for the question. So the total impact from taxes is $600 million as we're moving to the new tax guidance where R&D is capitalized. Out of that $600 million, $200 million of it was payable in November. We've already paid it, which was for 2023 and then $400 million of that $600 million is for 2024. And again, both of those are impacted by the amortization of R&D. As we move forward, as we move into 2025 and beyond, we anticipate that the growth of the cash tax rate will align with the growth the growth of our operating income. So there is a bit of a big step up this year. We do not anticipate that same level step up as we move forward.\\nJason Celino: Got it. But this is the new base level, right?\\nShelagh Glaser: It is because we're now going forward for the foreseeable future, we will be amortizing R&D. And for us, about half of our R&D is in the US and about half of our R&D is outside the US. So that new scheme is with us for the foreseeable future.\\nJason Celino: Got you. Okay, great. Thank you.\\nSassine Ghazi: Thank you.\\nShelagh Glaser: Thank you, Jason.\\nOperator: We'll take our next question from Jay Vleeschhouwer with Griffin Securities.\\nJay Vleeschhouwer: Thank you. Good evening. For Aart and Sassine, first and a financial follow-up for Shelagh. For Aart and Sassine, one of the tailwinds that you've had for the last number of years, and we've spoken about this often is what you've referred to as domain-specific architectures on the part of your customers or specialty chips. And AI is probably a very special case of that. The question is, how do you think the whole phenomenon of the main specific design at either the chip or system level is going to be fundamentally altered by the AI phenomenon? And in turn, what does it mean for any additional investments you need to make outside of R&D, for example, in AE capacity services, et cetera?\\nSassine Ghazi: Yeah. Thank you, Jay, for the question. You're right. The whole domain-specific architecture I want to say, about six, seven years ago, we started seeing a number of customers investing in it, mostly hyperscalers. And you can argue before that couple of the mobile companies started optimizing based on their own system software optimizing their silicon. With the hyperscalers, initially, the target was -- can they -- based on the workload -- on a specific workload, can they develop a chip that is more effective for power, performance cost, et cetera. And the answer is yes, and they made a number of these investments. Now you're seeing another wave of expanded investment around AI and how can they train the models that they are creating for, again, their specific applications. And I'm sure you've noticed in the last couple of weeks almost the top three hyperscalers announced their own silicon investments and chips for AI specific training to drive more optimization and efficiency for their work loans. What that means for Synopsys is not only it's another chip that our customer base is investing in, which drives both EDA and IP, typically, they are the most advanced node and different methodology in many cases, they're pushing towards multi-die and chiplets, which opens up the door for IP and the comments I made earlier. From our solution point of view, yes, we're expanding the solution offering to enable our customers to design these complex chips. On IP, I want to say it's fairly straightforward. You deliver [to] (ph) standards to connect these chips. But the work is not straightforward, but the road map is fairly straightforward what you need to do. On the EDA side, we have a number of those customers using 3DIC Compiler, which gives them the ability to architect that chip for that system in that case. SLM (NASDAQ:SLM), the silicon life cycle management to give them the ability to trace the health and the connectivity of these dives into the system and all the way in the field or in the hyperscale case as it sits into their data center and it's up and running. So we have a number of expansions in our portfolio in order to support these opportunities. And that's why we are bullish and excited about the opportunity to continue the growth for design, automation and design IP.\\nJay Vleeschhouwer: For Shelagh, one of the largest components of your backlog has been FSAs which are, I would assume, largely related to IP. Is there any reason to believe that in future, the pull down of FSAs for IP consumption would be faster or larger than has been the case to date? And for that matter, were FSAs a large component of the $1.5 billion sequential increase in backlog that you noted?\\nShelagh Glaser: There's not a -- I wouldn't note a different percent of FSAs. It's certainly something that many of our customers who are purchasing IP prefer that model, but there's not a different mix.\\nSassine Ghazi: Thanks, Jay.\\nShelagh Glaser: Thank you, Jay.\\nOperator: We'll take our next question from Charles Shi with Needham.\\nCharles Shi: Hi, good afternoon and congrats on the very strong results, guidance and the backlog number. I want to ask a little bit -- I think a little bit more into the backlog number because you have relatively consistent, I mean, ratios in terms of how much backlog goes into RPO, how much RPO goes into current RPO and how much the current RPO covers next year's revenue. With that, at the same ratio as in the past few years, I thought you would have guided a little bit higher given that $8.6 billion backlog. So how should I reconcile this? Is it something like at this time, the average contract duration is a little bit longer within that $8.6 billion backlog? Or there's some conservatism around what will you want to guide for '24? Thanks.\\nShelagh Glaser: Yeah. Thanks for the question. So the backlog was broad-based. It was across multiple customers. And as Sassine said, some of them was larger renewals and new deals that we booked. There was -- there's no change in the duration in our contracts. So there's no change from our typical duration. As we were putting the forecast together for the year, we are really balancing the headwinds and tailwinds that we're seeing in the business, and we're seeing because of the backlog, we're seeing very strong momentum in the core business in design automation and Design IP where we're seeing the headwinds is on China, in particular, which obviously has been a large growth driver for us for the last several years. We're seeing that growth rate slower and then the other one is SIG, which the Software Integrity business is still -- we anticipate going to be impacted by a difficult software enterprise purchasing environment. And so that's why we've got that business forecast at single-digit growth. So it's really the balancing of those things. But we are seeing design automation and design IP aligned with our long-term goals for those businesses.\\nCharles Shi: Got it.\\nShelagh Glaser: Thanks for the question.\\nCharles Shi: Yeah. Maybe a quick follow-up on the half-over-half profile for next year because I mean, I would think that EDA revenue, I mean, because of its time-based nature and you guys keep signing bigger contracts, you tend to be like up to the right kind of profile through the year, but your relatively flattish half-over-half profile seems to suggest that the IP, maybe some of the hardware is going to be a little bit front half loaded. Is that the case? And why is that a thing?\\nShelagh Glaser: Yeah. It's really just the balance of how we see the customers wanting to ingest our hardware and our IP business. So it's fairly aligned also with what we saw in '23 and fairly aligned with what we saw in '21.\\nSassine Ghazi: Maybe, Charles, if I -- I'll add a couple of comments to what Shelagh just said. Rough numbers, our design automation is about 65% of our business and then Design IP, 25% and software integrity 10%. And the $8.6 billion is mostly those agreements in EDA and IP. And given the large percentage of the overall revenue we have, which is 25% is IP, there's a different pull down and development of the IP, especially on the advanced nodes, when we're talking about we're developing our IP portfolio on the most advanced foundries, from the day you find an agreement to the day you deliver, there's a time lapse by when that you deliver the IP and you get the pull down. So while the backlog number, the $8.6 billion is large, we need to get, I would say, used to that the consumption for EDA and IP would be very different across that backlog number. And the timing of the renewal et cetera, et cetera.\\nTrey Campbell: Thanks, Charles. Let's do one more question and then if you could turn it back over to me, Lisa.\\nOperator: Thank you. We'll take our next question from Gianmarco Conti with Deutsche Bank.\\nGianmarco Conti: Hi, there. Thank you for taking the questions. So I guess the first one would be, how should we think about the recurring revenue rate into 2024? Is it fair to assume that given that we're expecting another record [hard year] (ph) into '24, it will inch closer to 80%. And secondly, could you provide some more color on the SIG strategic initiatives that you're considering?\\nSassine Ghazi: Maybe I'll talk about the SIG initiative, and Shelagh will comment on the backlog. The -- what we've said in the remarks that we went through a strategic portfolio review, just to put some context around what we went through. As I stated earlier, we have the three business segments and we are truly fortunate that we have a leading position in each one of those segments. As we look at the opportunities over the next five to 10 years of these market segments, I cannot express how excited we are about the opportunities we have in design automation and design IP. And the more we can expand our portfolio within these market segments. So that led us to discuss and make a decision to explore the strategic alternatives for software integrity to put priority of where to make our investments and where we believe there's a higher ROI for the investments based on the 90% of our portfolio between the design automation and design IP business segments. So that's really the process that we went through in order to get to that point.\\nShelagh Glaser: Yeah. And I would add that on the recurring revenue question, we're not expecting a substantial change in that. We've had a record hardware year in '22 and '23, and we're expecting another one in '24, but the overall business is growing too. So it won't substantially change the mix of recurring revenue. Thank you for the question.\\nTrey Campbell: Thanks, Johnny. Let me turn it over to Aart for some closing remarks.\\nAart de Geus: Thank you. A couple of personal words in closure here. Since the IPO in 1992, this is my 128th earnings call. I did not miss a single one, but I suspect that literally only Jay Vleeschhouwer remembers exactly what was said at each one of these calls. So a big thank you to you, Jay. But really huge gratitude to all of you for your feedback, you have write-ups, your advocacy on the market, but also for never giving up on asking questions where you perfectly well know that we will never answer them. So I really hope that you will keep that up with Sassine. But more importantly, a gratitude for having been travel made and family members in raising this fragile Synopsys start-up child into the strong worldwide market and technology leader that is today. Thank you. Now some CEO transitions are hard, but I think we're doing very well. Sassine is the perfect choice for CEO. He already leads with ambition, heart and smart and of course, his 25 years of experience at Synopsys is comprehended is complemented, I should say, by great customer relationships and very importantly, trust. Sassine has my respect, my enthusiasm and my full support. So please give him your support as well. With that, I look forward to from time to time, touching base with you, so until then, be well and thank you very much.\\nTrey Campbell: Thanks, everyone, for joining the call. Lisa, can you close this out?\\nOperator: Thank you. And that does conclude today's presentation. Thank you for your participation today. You may now disconnect.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " 'CrowdStrike Holdings (NASDAQ:CRWD) announced a record-breaking third quarter in its Fiscal Year 2024 Earnings Conference Call, with an ending Annual Recurring Revenue (ARR) of $3.15 billion, marking a 35% year-over-year growth. The cybersecurity firm also reported record net new ARR, operating profitability, and gross margin, while emphasizing its position as a leading security platform.\\nKey takeaways from the call include:\\n- CrowdStrike\\'s LogScale next-gen SIEM business surpassed the $100 million ARR milestone, demonstrating the growing demand for its Identity Threat Protection and unified architecture.\\n- The company reported significant growth in its Cloud Security and Identity Threat Protection businesses, with notable wins across various sectors.\\n- CrowdStrike expects to achieve its target model of 82-85% subscription gross margin, 28-32% operating margin, and 34-38% free cash flow margin within the next three to five years.\\n- The company\\'s financial outlook for Q4 and the full fiscal year 2024 reflects strong revenue growth and profitability expectations.\\n- CrowdStrike ended the quarter with a strong balance sheet, with cash and cash equivalents totaling $3.17 billion.\\nCrowdStrike\\'s CEO, George Kurtz, acknowledged the challenging macro environment for deals, citing longer sales cycles and cautious buyers. However, he also highlighted the company\\'s strong October performance. The company\\'s focus remains on annual recurring revenue (ARR) as a key metric for the health of the business, rather than billings.\\nKurtz expressed excitement for Falcon for IT, a product that promises better security and unification with IT, leading to potential cost savings and automation. He also mentioned the potential of Falcon for IT to drive ROI-based sales, as customers can use it to identify and fix problems, eliminating the need to rely on IT teams with less advanced tools.\\nDespite potential challenges in the market, CrowdStrike remains confident about its growth and profitability outlook. The company expects Q4 FY \\'24 total revenue to be in the range of $836.6 million to $840.0 million, with a year-over-year growth rate of 31% to 32%. Full-year revenue for 2024 is projected to be between $3,046.8 million and $3,050.2 million, reflecting a growth rate of 36% over the prior fiscal year.\\nKurtz also discussed the advantages of using CrowdStrike\\'s AI engine and proprietary endpoint data to capture the next-gen SIEM opportunity. He emphasized the capabilities of LogScale to ingest third-party data without creating an index, making it a versatile platform for security and performance management purposes. Despite the uncertain macro environment, the company remains focused on innovation, go-to-market strategies, and product development. CrowdStrike\\'s mission to stop breaches continues to drive strong demand for its Falcon platform, extending its reach to businesses of all sizes.InvestingPro InsightsCrowdStrike Holdings\\' recent Fiscal Year 2024 Earnings Conference Call highlighted their impressive financial performance and strategic achievements, painting a picture of a company with strong forward momentum. To further understand the financial landscape of CrowdStrike, let\\'s delve into some key metrics and insights from InvestingPro.\\nInvestingPro Data shows that CrowdStrike boasts a robust market capitalization of $56.91 billion USD, underlining its significant presence in the cybersecurity sector. The company\\'s revenue growth over the last twelve months as of Q3 2024 stands at an impressive 39.94%, which is consistent with the strong revenue figures and growth rates mentioned in their earnings call. Additionally, CrowdStrike\\'s gross profit margin during the same period is 74.62%, reflecting the company\\'s ability to maintain profitability and manage costs effectively.\\nFrom the perspective of InvestingPro Tips, two particularly relevant insights for CrowdStrike investors include:\\n1. The company holds more cash than debt on its balance sheet, which is a strong indicator of financial health and provides flexibility for future investments and operations.\\n2. Analysts are optimistic about the company\\'s future, with 34 analysts having revised their earnings upwards for the upcoming period, signaling confidence in CrowdStrike\\'s growth trajectory and profitability potential.\\nFor those seeking a more comprehensive analysis, InvestingPro offers additional tips and insights. Currently, there are 21 more InvestingPro Tips available for CrowdStrike, which subscribers can access to make more informed investment decisions.\\nAs a special offer, InvestingPro subscription is now on a Cyber Monday sale with a discount of up to 60%. To further enhance this deal, use coupon code sfy23 to get an additional 10% off a 2-year InvestingPro+ subscription. This is an opportune time to gain access to a wealth of investment knowledge and analytics to navigate the markets with confidence.Full transcript -  Crowdstrike Holdings Inc  (CRWD) Q3 2024:Operator: Thank you for standing by, and welcome to CrowdStrike Holdings Third Quarter Fiscal Year 2024 Earnings Conference Call. At this time, all participants are in a listen-only mode. After the speakers\\' presentation, there will be a question-and-answer session. [Operator Instructions] I would now like to hand the call over to VP of Investor Relations, Maria Riley. Please go ahead.\\nMaria Riley: Good afternoon, and thank you for your participation today. With me on the call are George Kurtz, President and Chief Executive Officer and Co-Founder of CrowdStrike; and Burt Podbere, Chief Financial Officer. Before we get started, I would like to note that certain statements made during this conference call that are not historical facts, including those regarding our future plans, objectives, growth including projections, and expected performance, including our outlook for the fourth quarter and fiscal year 2024, and any assumptions for fiscal periods beyond that are forward-looking statements within the meaning of the Private Securities Litigation Reform Act of 1995. These forward-looking statements represent our outlook only as of the date of this call. While we believe any forward-looking statements we make are reasonable, actual results could differ materially because the statements are based on current expectations and are subject to risks and uncertainties. We do not undertake and expressly disclaim any obligation to update or alter our forward-looking statements, whether as a result of new information, future events or otherwise. Further information on these and other factors that could protect the company\\'s financial results is included in the filings we make with the SEC from time to time, including the section titled Risk Factors in the company\\'s quarterly and annual reports. Additionally, unless otherwise stated, excluding revenue, all financial measures disclosed on this call will be non-GAAP. A discussion of why we use non-GAAP financial measures and a reconciliation schedule showing GAAP versus non-GAAP results is currently available in our earnings press release, which may be found on our Investor Relations website at ir.crowdstrike.com or on our Form 8-K filed with the SEC today. With that, I will now turn the call over to George.\\nGeorge Kurtz: Thank you, Maria, and thank you all for joining us today. I would like to begin my remarks today by expressing gratitude to our customers, who proudly trust CrowdStrike as their cyber security platform consolidator for the AI era; gratitude to our partners who win with CrowdStrike, taking our joint customers on Falcon Platform transformation journeys from device to cloud, to identity, to data and beyond; and to our team for their passion and dedication to our mission, stopping breaches, finding adversaries, and delivering the very best cyber security outcomes. Moving onto Q3. Despite a challenging macro environment and geopolitical tension, I am extremely proud of the resilience of our business and that we delivered a record Q3. CrowdStrike surpassed the $3 billion ARR milestone with an ending ARR of $3.15 billion, growing 35% year-over-year. CrowdStrike is the fastest and only pure-play cyber security software vendor in history to achieve this milestone. In Q3, we delivered double-digit net new ARR acceleration at scale, powered by customer demand for the depth and breadth of CrowdStrike\\'s AI native XDR platform, terrific execution exemplified by rising win rate against our competitors, and acceleration in our cloud security and identity businesses, as well as a record quarter in our LogScale Next-gen SIEM business. Our standout top-line performance came in tandem with P&L discipline as our profitability soared to record heights. Q3 was indeed a quarter of records. Let me share several of our financial highlights. Record net new ARR of $223 million, representing a 13% year-over-year growth from acceleration in new and expansion business; record non-GAAP subscription gross margin; record GAAP and non-GAAP operating profitability; record free cash flow of $239 million, representing 30% free cash flow margin and achieving free cash flow Rule of 66, up from 63 last quarter. Burt will share more color on our financials and platform adoption stats, following my remarks. This was a standout quarter and places us well on the path to $10 billion in ARR that we outlined in our latest investor briefing. The market continues to validate CrowdStrike\\'s widening leadership position. While others scaled back R&D, we are increasing our investments as innovation is the lifeblood of our company. We continue to make meaningful investments in go-to-market and profitable growth. To this end, we received multiple industry awards and accolades over the course of this quarter. Across industry analyst firms, CrowdStrike is consistently top-rated. In Q3, CrowdStrike was awarded a perfect 100% across protection, visibility and analytics detections in MITRE\\'s latest ATT&CK testing in industry-first; recognized as Gartner Customers\\' Choice and one of the highest-rated in their latest Peer Insights Voice of the Customer for EPP report; named a Leader in the Forrester Wave for Endpoint Security; and positioned as a leader in the IDC Vulnerability Management MarketScape. Our leadership position is translating into record demand for our Exposure Management solution in its first quarter on the market. Our market presence in open platform approach to XDR is a uniting force in cyber security. In Q3, we hosted Fal.Con, our flagship customer and ecosystem event with more than 4,000 attendees and 70 sponsors. This was fast followed by our Fal.Con On the Road series, where we have already hosted Fal.Con Tokyo and Fal.Con Sao Paulo with thousands in attendance. The drumbeat of innovation was loud and clear with multiple releases and announcements showcasing CrowdStrike as the XDR leader, including the Falcon Platform Raptor release, which standardizes all of our customers on LogScale. This builds our platform data gravity, coupling native Falcon data with third-party data ingest, further enabling our customers to realize SIEM and XDR use cases on Falcon. Falcon for IT, a new module to unify IT and SecOps. From hygiene to patching, Falcon for IT lets customers consolidate multiple use cases and replace legacy products with our single-agent architecture. Our new Falcon data protection module that liberates customers from legacy DLP products with modern frictionless data security, which prevents data exfiltration. Falcon Foundry, a no-code application development solution, enabling both customers and technology partners to build directly on Falcon. Foundry epitomizes the true definition of a platform offering limitless apps. We announced the acquisition of Bionic, bringing application security posture management to the Falcon Cloud Security Suite. Falcon Cloud Security is the industry\\'s most comprehensive and innovative cloud security offering, with integrated agent and agentless CSPM, CIEM, CWP, and now, ASPM. We released the beta and pricing for Charlotte AI, our AI-powered [stock] (ph) analyst, which was incredibly well received. CrowdStrike\\'s generative AI leveraging multiple foundational models can turn hours of work into minutes, while democratizing cybersecurity, unlocking value, and adoption across the entire breadth of the Falcon platform. Enthusiasm for Falcon for IT, our automation and hygiene module for IT teams, is exciting to witness because it is something so intuitive that our customers want. Like data protection, Falcon for IT saw a clear and long-neglected pain point, where customers are forced to rely on multiple legacy products that have long outstayed their welcome. Customers are eager to consolidate their agent estate while reducing cost and complexity. Falcon for IT illustrates the power of CrowdStrike\\'s single-platform approach. We have the real estate and data foundation to solve ever-evolving use cases, delighting our customers, while disrupting vendors that have failed to evolve. The resounding takeaway from my customer engagements at Fal.Con and throughout the quarter is that CrowdStrike is the right choice for CISOs, CIOs, executive teams and Boards. Here\\'s why? Our build-by-design, single-platform architecture requires no integration, no stitching, and no platformization. Our unified cloud-native platform and critical beachfront real estate within the customer estate naturally create cybersecurity data gravity, solving for today\\'s challenges, while preparing for the unknowns of tomorrow. Organizations are looking for a trusted cybersecurity consolidator. CrowdStrike is cybersecurity\\'s AI consolidator, liberating organizations from a litany of increasingly ineffective legacy tools, multiple agents, point products, and fragmented pseudo platforms. Illustrating this point, deals with eight or more modules increased 78% year-over-year in the quarter. This is what consolidation looks like. And third, stopping the breach matters more today than ever. Adversaries continue to evolve, moving faster and increasing their use of dark AI, turning AI into a weapon for evil. At the same time, legislative and SEC regulatory oversight pressures Boards and Executives to prioritize cybersecurity. With these three takeaways in mind, let\\'s start by expanding on the first. Why CrowdStrike is the definitive single security platform? Multiple fragmented platforms, consoles, and data silos force customers to focus on integrating, not security outcomes. Our relentless focus on innovation and commitment to a single built-by-design platform creates cybersecurity\\'s source of truth. Built on this platform foundation, our cloud identity and LogScale next-gen SIEM products are examples of IPO-worthy hypergrowth businesses. Let\\'s take a look at our momentum in cloud security. Growth accelerated in the quarter and we\\'re entering Q4 with a record pipeline. Noteworthy wins in the quarter included: an eight-figure total deal value with a new Falcon customer in the hospitality vertical, where we now secure their vast multi-cloud estate as part of a broader Microsoft (NASDAQ:MSFT) replacement; a seven-figure cloud security expansion with one of the largest enterprise SaaS providers, where we replaced multiple existing point products with Falcon Cloud Security; a seven-figure cloud security expansion with a major apparel brand, where we replaced a firewall hardware vendor\\'s cloud security with our unified Falcon Cloud Security offering. CrowdStrike has one of the largest and fastest-growing cloud security businesses by ARR and customer count. The number of customers we protect in the public cloud has increased 45% from last year as we rapidly replace other cloud security vendors in the ecosystem. Point product API-based CSPMs have emerged as the easiest to replace, as the market rapidly adopts our consolidated view that together agent and agentless security stops the breach. Combined with our recently completed Bionic acquisition, we\\'re the only vendor offering ASPM, CSPM, CWP and CIEM under a single CNAPP umbrella. Moving to Identity Threat Protection, where we delivered a record quarter. More and more companies are investing in protection from the growing number of identity-based attacks with highly visible breaches prominently covered in the news over the past few months. Identity Threat Protection wins in the quarter included: an eight-figure total deal value win in the federal government, where Falcon Identity landed as the identity security solution of choice; and multiple seven-figure wins across diverse verticals, including financial services, consumer packaged goods and manufacturing to name a few. We see a long runway for guiding organizations on their zero-trust transformations with our Identity Threat Protection, given a rapidly growing threat vector, a largely untapped market where most organizations aren\\'t protected, and our frictionless unified architecture that secures identity across devices, clouds, domain controllers, and active directory. CrowdStrike pioneered the Identity Threat Protection category, and we are unmatched in the market as the only company with a single-agent solution for both on-prem and cloud environments. Moving to LogScale. Our LogScale next-gen SIEM business achieved new records in Q3, surpassing the $100 million ARR milestone. The combination of search speeds, data gravity, and cost efficiencies, all integrated within the Falcon Platform, sets LogScale miles apart from its competitors. However, it is the future of this business that really excites us. Our LogScale next-gen SIEM opportunity is supercharged by pervasive discontent with legacy SIEMs, recent M&A activity, and CrowdStrike\\'s growing position as cybersecurity\\'s platform of record. We have seen a significant and pronounced increase in interest among customers looking to leapfrog their expensive, cumbersome, and slow legacy SIEM. Our next-gen SIEM offering is the right technology in the right place, at the right time to benefit from market dynamics and the scale of CrowdStrike. Q3 wins include: a seven-figure expansion in a major consumer staples company, selected LogScale to ingest data from third parties, correlate alerts and benefit from long-term data retention; a seven-figure financial services, new customer land, where CrowdStrike was called in to stop a breach, with the desire to operationalize their high-fidelity security data without compromising on cost, Falcon LogScale was an easy choice to replace the incumbent Microsoft; a seven-figure new logo land in a business process outsourcing firm that had decided it was time to replace their antiquated SIEM with LogScale, while also adopting the Falcon Platform for XDR, Identity and Cloud Security. Our advancement into the next-gen SIEM market is accelerated by the Raptor release, which brings LogScale to the forefront of the Falcon Platform experience. With Raptor, we\\'ve made it significantly easier for customers to build their security and data lake efforts on top of LogScale next-gen SIEM across our native first-party data and that of any third-party product. Our single-platform approach evolves and broadens the aperture of cybersecurity. CrowdStrike is cybersecurity\\'s AI consolidator, liberating organizations from legacy AV, subpar EDRs, and a hodgepodge of hygiene, compliance, vulnerability, device management tools, costly and clunky SIEMs, and a confusing alphabet soup of immature cloud point products. This lengthy list of costly point products can be left behind, saving tens of millions annually for businesses. Most importantly, stopping the breach matters more today than ever before. Adversaries don\\'t discriminate, no matter the industry vertical, the geography, or the business size, stopping the breach is non-negotiable. The business disruption and financial losses from breaches are growing. [Cutting corners] (ph) on security is one of the most costly choices a business can make. The cost associated with cleaning up a breach can exceed $100 million. Our seminal cybersecurity platform innovation, as well as threat intelligence and service expertise, delivers the best security outcomes. This is why we win. Stopping the breach is the security outcome that CrowdStrike delivers. This makes our business incredibly durable in diverse economic cycles. We are a business necessity. A win this quarter exemplifying our unmatched capabilities is a large multinational, who was using legacy EDR until they were breached. They called CrowdStrike and we quickly remediated the breach. Seeing the power of the Falcon platform, this customer purchased Falcon Complete, Identity Complete, and LogScale next-gen SIEM for hundreds of thousands of workloads. Helping this large multinational in their time of need showcases CrowdStrike\\'s differentiation. Our AI trained on cybersecurity\\'s richest data set drives the industry\\'s most comprehensive protection and automation. We couple the right technology platform with cybersecurity\\'s best and brightest incident responders and the expertise that comes from curating the industry\\'s leading threat intelligence. The need for stopping breaches is not just an enterprise concern. The problem is just as pronounced in the SMB market, especially the S of SMB, where businesses remain stuck on legacy signature-based AV, oftentimes not knowing that better exists for them. Falcon Go is fit for purpose, delivering big protection for their small business. With our latest release of Falcon Go, we deliver a whole new user experience, creating the easy button for cybersecurity, removing the need for CrowdStrike users to have cyber or IT skills. The new console is intuitive and pre-configured for immediate SMB success. We\\'re always looking to remove the friction from the customer experience, and this new release makes Falcon Go as easy as one-click to accelerate customer adoption. Falcon Go is a major step for SMB cybersecurity, and we are now investing in go-to-market. We launched sales of Falcon Go on Amazon (NASDAQ:AMZN) Business, a marketplace where over 6 million SMBs purchase everything for their businesses. With Falcon Go, our mission of stopping breaches extends to businesses of all sizes, large and small. Our go-to-market is propelled by our partner ecosystem. Partners bring CrowdStrike into new accounts and drive platform adoption in existing customers. Year-to-date, 62% of all our new logo wins were partner-sourced. The efficacy and success of our channel-led growth was reported by Canalys, the channel\\'s leading industry analyst firm, in their most recent cybersecurity research. Canalys reported CrowdStrike\\'s endpoint market share as number one, ahead of Microsoft. In addition, the research highlights CrowdStrike as the fastest-growing cybersecurity vendor in the channel ahead of Microsoft and Palo Alto networks, among others. We are setting new records. We announced surpassing $1 billion in AWS Marketplace sales. Being the first and fastest cybersecurity vendor to reach this milestone exemplifies CrowdStrike\\'s leadership position as the cloud\\'s cybersecurity platform of choice. And our AWS momentum is not slowing. This past quarter was a CrowdStrike record AWS marketplace quarter. Our global systems integrator alliance partners are increasingly engaging with us across the entire Falcon platform. For example, EY built a 150-person strong and growing global LogScale next-gen SIEM practice, based on the demand they are seeing for SIEM transformation and our technological superiority. In addition, EY refers their clients to CrowdStrike for incident response engagements and secure digital transformation projects. To date, we have done over $160 million in business with EY across 24 countries. In closing, in a challenging macroeconomic environment, which impacted even the largest cybersecurity vendors, we delivered a record Q3, accelerating double-digit net new ARR growth, record free cash flow, and record profitability. Surpassing $3 billion of ARR as the fastest and only pure-play cybersecurity software vendor in history to achieve this milestone validates CrowdStrike\\'s market leadership. Our Q4 setup is strong with a record pipeline, and the competitive gap between CrowdStrike and other players in the market continues to widen. I\\'m excited about the path we are on and the progress we are making to $10 billion in ARR. Today\\'s cyber environment is exactly why I built CrowdStrike. The adversary is moving faster and the rise of dark AI significantly increases attacker sophistication. Falcon has made cybersecurity easy and effective for small businesses to the world\\'s largest enterprises. Simply put, CrowdStrike is cybersecurity\\'s platform consolidator of choice. The platform is winning at scale. Our customers are looking to Falcon for more, more workloads, more use cases and more security outcomes. CrowdStrike remains unrelenting in our focus and promise, stopping the breach. With that, we have a busy and exciting Q4 ahead of us. And I will now turn it over to Burt for commentary on our financial performance.\\nBurt Podbere: Thank you, George, and good afternoon, everyone. As a quick reminder, unless otherwise noted, all numbers except revenue mentioned during my remarks today are non-GAAP. Additionally, the results we are reporting today include the acquisition of Bionic, which closed during the quarter and was de minimis to revenue and ARR. Outstanding execution from the CrowdStrike team resulted in a record third quarter, despite a continued challenging macro environment. Financial highlights in the quarter included: an acceleration in net new ARR growth, driving ending ARR well above the $3 billion milestone; 96% year-over-year growth in operating income and record operating margin; record net income, which more than doubled year-over-year; record GAAP profitability for the third consecutive quarter; and record free cash flow. In the third quarter, we achieved record net new ARR of $223.1 million, with year-over-year growth accelerating to 13%. Demand in the quarter was broad-based, as we expanded our leadership across the market from large enterprises to small businesses. New customer acquisition, strong expansion business within existing customers and a record quarter with the U.S. federal government contributed to the acceleration in net new ARR growth and ending ARR reached $3.15 billion, up 35% year-over-year. We continue to be very pleased with our platform strategy as subscription customers with five or more, six or more and seven or more modules now represent 63%, 42% and 26% of subscription customers, respectively. Our gross retention rate remained high and our dollar-based net retention rate was slightly below our benchmark in Q3, as the mix of net new ARR from new customers has remained above our expectations and we continue to land bigger deals. Moving to the P&L. Total revenue grew 35% over Q3 of last year to reach $786.0 million. Subscription revenue grew 34% over Q3 of last year to reach $733.5 million. Professional services revenue was a record $52.6 million, representing 57% year-over-year growth. The geographic mix of third quarter revenue consisted of approximately 69% from the U.S., 15% from Europe, Middle East and Africa, 10% from the Asia-Pacific region, and 6% from other markets. Our investments in data center and workload optimization that we outlined in our Fal.Con Investor Briefing in September continued to bear fruit, resulting in record subscription gross margin performance above 80% in the third quarter. Total non-GAAP operating expenses in the third quarter were $436.1 million, or 55% of revenue, versus $348.6 million last year, or 60% of revenue. Q3 sales and marketing and R&D expenses grew 23% and 31% year-over-year, respectively. We continue to invest aggressively in innovation and growth, while exceeding our profitability expectations. Third quarter non-GAAP operating income grew 96% year-over-year to reach a record $175.7 million and operating margin increased by 7 percentage points year-over-year to reach a record 22%. As we outlined during our Fal.Con Investor Briefing in September, we have once again raised our target model, which now calls for subscription gross margin of 82% to 85%, operating margin of 28% to 32%, and free cash flow margin of 34% to 38%. We expect to achieve our new target model within the next three to five years, as we also flight the company to reach $10 billion of ARR over the next five to seven years. Non-GAAP net income attributable to CrowdStrike in Q3 grew to a record $199.2 million or $0.82 on a diluted per share basis, each more than doubling year-over-year. Our weighted average common shares used to calculate third quarter non-GAAP EPS attributed to CrowdStrike was on a diluted basis and totaled approximately 244 million shares. We ended the third quarter with a strong balance sheet. Cash and cash equivalents and short-term investments totaled $3.17 billion and reflects the $238.7 million payment, net of cash acquired for the acquisition of Bionic. Cash flow from operations was a Q3 record of $273.5 million. Free cash flow reached a record $239.0 million or 30% of revenue, achieving a Rule of 66 on a free cash flow basis. Moving to our outlook. As George outlined, strong demand for the Falcon platform is driving our pipeline to new heights. However, the macro environment remains challenging with continued increased budget scrutiny, and as a result, we are not expecting to see the typical Q4 budget flush. Balancing these factors, we are maintaining our net new ARR assumptions, which call for in-line to modestly up net new ARR for the full year and double digit year-over-year net new ARR growth in the second half. As our guidance reflects, we are raising our full year revenue and profitability expectations for the year. Additionally, we are maintaining our target of 30% free cash flow margin for the full fiscal year. More specifically, for the fourth quarter of FY \\'24, we expect total revenue to be in the range of $836.6 million to $840.0 million, reflecting a year-over-year growth rate of 31% to 32%. We expect non-GAAP income from operations to be in the range of $186.5 million to $189.0 million, and non-GAAP net income attributable to CrowdStrike to be in the range of $199.6 million to $202.1 million. We expect diluted non GAAP net income per share attributable to CrowdStrike to be approximately $0.81 to $0.82, utilizing a weighted average share count of 245 million shares on a diluted basis. For the full fiscal year 2024, we currently expect total revenue to be in the range of $3,046.8 million to $3,050.2 million, reflecting a growth rate of 36% over the prior fiscal year. Non-GAAP income from operations is expected to be between $633.6 million and $636.2 million. We expect fiscal 2024 non-GAAP net income attributable to CrowdStrike to be between $715.2 million and $717.7 million. Utilizing 243 million weighted average shares on a diluted basis, we expect non-GAAP net income per share attributable to CrowdStrike to be in the range of $2.95 to $2.96. George and I will now take your questions.\\nOperator: Thank you. [Operator Instructions] Our first question comes from the line of Saket Kalia of Barclays.\\nSaket Kalia: Okay, great. Hey, guys, thanks for taking my question here, and great to see the net new ARR return to growth in the quarter. George, maybe for you, a lot of great stuff in your prepared remarks to run through, but maybe I\\'ll just stick to one question here. You\\'ve talked about how the legacy SIEM market is starting to feel kind of like the legacy AV market did. And of course, we\\'ve recently seen some M&A in that market. It\\'s probably too early, but maybe the question is, what are prospective SIEM customers saying to you about LogScale and their willingness to explore other solutions besides their traditional SIEM? Does that make sense?\\nGeorge Kurtz: It does, yeah. It\\'s a great question. So, when I look at the market today and I compare that to when I started CrowdStrike in 2011 and really talking to customers in 2012 and 2013 about replacing their legacy AV, it feels like it\\'s the same conversation, just with a different context of replacing their legacy SIEM. What I found, and I tend to like businesses and markets where this is in play, is where the incumbents are entrenched, but have a high degree of dissatisfaction, where the technology is legacy and where the complexity is just high of patching all this stuff together. And in talking to customers, they want better, faster, cheaper, and they want something that works at cloud scale. If you look at what we\\'ve done with LogScale, it\\'s a much more modern version of a SIEM, a next-gen SIEM. We got ahead of the curve a couple of years ago when we bought Humio, which is now LogScale, we\\'ve now integrated into our platform, and it\\'s extended out our XDR technology and it allows us to do log and SIEM in the same platform. So, from our perspective, the feedback that we\\'ve gotten not only from customers but partners like EY, who are building practices around this, this is a massive opportunity for CrowdStrike, and now is the right time for us, given the level of dissatisfaction M&A in the environment and the customer\\'s willingness to look for a much better solution, which also gives us data gravity in the platform.\\nSaket Kalia: Makes a lot of sense. Thanks, guys.\\nOperator: Thank you. Our next question, please standby, comes from the line of Joel Fishbein of Truist.\\nJoel Fishbein: Thanks for taking my question, and a good quarter on the net new ARR side. George, another question along Saket\\'s lines, but around the comments you made with regard to data protection. The same could be said about the legacy DLP market. And as you know, I would say that\\'s even longer in the tooth than some of the SIEM market players out there. Love to hear how you are -- you didn\\'t talk about it like you did cloud security, identity protection, LogScale, et cetera. How far are we in that displacement market? And how big do you think they are? And what\\'s the competitive dynamics there would be really helpful. Thank you.\\nGeorge Kurtz: Sure. Well, we actually showed our data protection technology at Fal.Con a couple of months ago. We\\'re actively working with some of the largest enterprises in the world to make sure that we account for all their requirements and their ability to actually move off their legacy DLP product. There\\'s not one customer that I\\'ve talked to that says they like their DLP product, period. And in today\\'s environment, with data in motion, particularly through cloud services, the ability to actually be able to track from endpoint through cloud where data moves, what type of data is out there, leveraging our AI to understand, is it something that should be moving in certain places is going to be incredibly important. And the way we look at it is the same way we looked at legacy AV. There\\'s a better way to reimagine DLP into Falcon Data Protection, and we\\'re very, very close to being able to release that as we continue to put the fine touches on what customers are looking for and what they\\'re willing to write a check for. So stay tuned, but another massive market opportunity for us.\\nOperator: Thank you. Our next question comes from the line of Rob Owens of Piper Sandler.\\nRob Owens: Great. Good afternoon. George, I guess staying on theme here with these extensible adjacencies, maybe you can touch on Falcon for IT. What early response has been since, I guess, showing it at your recent user conference and where you see the opportunity on that front?\\nGeorge Kurtz: Well, coming out of Falcon, I would say that was the number one requested feedback item across all the great announcements that we had. I think it\\'s so overdue in the technology space to be able to unify IT and security. Sometimes the technology isn\\'t the harder part, it\\'s the org structure. So, having a single agent that both security and IT can use, a home for IT, I think is very important. We\\'re actually working with large customers right now, gathering their feedback, and then we\\'ll look to have a product out probably in Q1. But so far the feedback has been amazing in terms of what we\\'ve already delivered, and now we\\'re capturing additional capabilities for our first release. So, I think that one has a lot of legs to it, and certainly opens up a massive TAM opportunity for us in IT, well beyond anything in security, given the agent cloud architecture that we built. We\\'ve got the real estate and we plan to monetize it.\\nOperator: Thank you. Our next question comes from the line of Matthew Hedberg of RBC.\\nMatthew Hedberg: Great, thanks for taking my question. Burt, I\\'ve got one for you. Obviously, you didn\\'t comment on fiscal \\'25. I believe at this point last year, you did give a couple of breadcrumbs on how to think about ARR. I\\'m curious as we as we sort of sharpen our pencils on \\'25, are there any guidepost or things that we should think about whether its growth or profitability?\\nBurt Podbere: Hey, Matt. Great question. So for us, we are always looking to maximize our growth while at the same time paying attention to the bottom-line. So that\\'s never changed. We are absolutely going to continue to invest in innovation. We got -- we announced so much at our last Fal.Con. We\\'ve got a lot of momentum. We\\'re going to continue to invest to continue that momentum. But also at the same time, we recognize that we want to make sure that we\\'re a very well-run company and people are seeing the output of that certainly on this quarter and we don\\'t plan on changing that at all for next year.\\nOperator: Thank you. Our next question comes from the line of John DiFucci of Guggenheim.\\nJohn DiFucci: Thank you. George, it\\'s impressive to really see the consistency of your team at scale here. But I\\'m just curious, how did traction continue into the end of the October quarter and then again into -- we\\'re almost through November, relative to the October period. And the reason I\\'m asking this is that, we\\'ve generally heard of a fade in business activity through October across the industry, not necessarily with you, I mean, actually we didn\\'t hear with you, but across most others. And it\\'s actually sort of shown up in results of some of your security peers. So, I\\'m just wondering, maybe -- what are you seeing. What did you see into the end of the quarter? And what are you seeing at the beginning of the January period?\\nGeorge Kurtz: Yeah, we certainly had a strong October. I think, as I said in my prepared remarks, the macro environment is still challenging. I mean, make no mistake about that. Deals take longer, a lot more scrutiny, a lot of sign offs, and there\\'s a lot more work that goes into these larger enterprise deals. Getting deals done even like Falcon Flex (NASDAQ:FLEX), which are more enterprise-like in their nature, takes time. So, we had a great October, but in general, buyers are still cautious. And I think the fact that we\\'re able to provide a real platform play that allows them to consolidate in other technologies and ultimately save money accrues values to us, but it certainly takes a lot of time and effort to get the deal over the goal line. But the team did a great job and October was strong for us.\\nOperator: Thank you. Our next question comes from the line of Tal Liani of Bank of America.\\nTal Liani: Hi, guys. The first question is Bionic contribution, you said was de minimis. Can you define what de minimis for net new -- the contribution to net new ARR? And then, quarter was phenomenal, but I do see some weakness across the board of cyber companies with billings. In your case, it was down about 2%. I also see some weakness in deferred revenues. How do I reconcile what I see with billings, with deferred, with the underlying drivers that are very strong and your strong execution? Why is -- why are we seeing weakness not just with you, maybe with the entire space, but why are we seeing weakness with billings across the board? Thanks.\\nBurt Podbere: Thanks, Tal. Good question. I\\'m going to start with billings. Yeah, you\\'re correct. For us specifically, we don\\'t manage the business to billings and we feel that ARR gives you the absolute best proxy to revenue and we felt that that\\'s the right metric, as you know, since we went public, to give you more transparency into the health of our business. And that\\'s the metric that really guides you on health. With respect to billings ourselves, I mean, we think about billings similar to probably most of you out there, is that billings, typically for hardware companies, use that when they don\\'t disclose any sort of bookings metric, and that\\'s not us, right. We think that billings has certain things that just are not as relevant as a metric like ARR. You\\'re comparing a balance sheet item to a P&L item. For us, the P&L is going to dictate the health of the business. So for us, you know, billings obviously is going to be impacted by duration and there are many things that go into that. And remember also that when you think about on a year-on-year basis, we\\'re still up on billings. And I think that\\'s the one thing that you want to take away. For us, when we think about how we want to continue to be transparent, ARR really gives you that notion of where we\\'re going and how we\\'re doing. And I think that that\\'s the focus. And it has been, by the way, since we went public, even as a private company, that\\'s the one that we manage the business to, that\\'s how we look at how to give out quotas to our reps, et cetera, et cetera. So for us, that\\'s not going to change. And I hope that answers that question.\\nOperator: Thank you. Our next question comes from the line of Adam Borg of Stifel.\\nAdam Borg: Awesome. Thanks so much for taking the question. Maybe just for George on CNAPP, it\\'s obviously great to hear the traction. As we look ahead, should we still view this more of an upsell opportunity to existing customers or what needs to happen for this to be more of a tip of spear to drive net new logos? Thanks so much.\\nMaria Riley: I\\'m sorry, would you mind repeating your question?\\nAdam Borg: Great, can you hear me okay now?\\nMaria Riley: Yeah, I can -- we can hear you now.\\nAdam Borg: Great. Sorry about that. Maybe just on CNAPP, it\\'s great to see the continued traction there. And as the market matures, should we view this as more of an upsell opportunity or what needs to happen for this to be more of a tip of spear to drive net new logos?\\nGeorge Kurtz: Yeah, it\\'s really a bright star for us when you look at our CNAPP capabilities of cloud workload protection, CSPM, CIEM, ASPM now with Bionic. We really have, I think one of the most complete cloud security offerings in the industry, and we\\'re winning deals with it. We\\'re leading, in many cases, particularly forward-leaning cloud companies that we\\'re selling to. If you look at the latest threat environment and what\\'s happening and how identities are being abused and two-factor authentication systems being bypassed, those sort of things, it\\'s important to have a full suite of cloud protection. And I think this is sort of the awakening time for that industry and that companies can\\'t just say, hey, we\\'ve got something from a cloud provider or we\\'re going to go at it alone. I remember many, many, many years ago, companies didn\\'t run antivirus, right? And you would say that\\'s silly. Today, everybody runs an antivirus-type technology. It wasn\\'t so silly 30, 35-plus years ago. And I think this is kind of the inflection point in the cloud world, where you\\'re going to have multiple levels of protection, both agent and agentless. And the fact that we\\'ve been able to harmonize those two together and deliver a very robust agentless technology combined with the best cloud workload protection in the industry gives us a real advantage.\\nOperator: Thank you. Our next question comes from the line of Mike Walkley of Canaccord Genuity.\\nMichael Walkley: Great. Thanks. George, you touched on the new release of Falcon Go to better target SMB customers with more concise packaging. Can you just give us some color on how this segment of your business is trending, as some of your competitors have called out soft SMB spending due to the macro? And also, just how is this new release helping address pain points for these smaller customers? It seems like you\\'re starting to accelerate win rates in this segment.\\nGeorge Kurtz: Well, we saw a very strong SMB segment. We\\'ve done a lot of work, A, starting with our partners like Pax8 and others, Dell (NYSE:DELL), in that market. So meeting the customers, where they like to buy is really, really important. And then, coming up with a very innovative and easy-to-use technology designed for the SMB, our latest release of Falcon Go is literally one-click install. And the feedback that we\\'ve gotten from customers are like, \"Wow, this is the easiest thing I\\'ve seen. It just works.\" And I think you\\'re going to see even more broad adoption because we\\'ve made it so easy to install. You don\\'t have to be an IT professional. And then, we\\'ve got our partner network that are building services around it. So they\\'re able to generate dollars. We\\'re able to sell our product through their channels where they meet the customers. And I think we\\'ve taken a very innovative and effective approach in our partnering strategy. This is something that we\\'ve really worked on over the last year, 18 months, and we\\'re bearing the fruit of it right now.\\nOperator: Thank you. Our next question comes from the line of Gray Powell of BTIG.\\nGray Powell: Okay, great. Thank you for taking the question. Yeah, I was just wondering if you could repeat the commentary on the potential for a year-end budget flush. I think you said that it was not going to be a typical year. And if I remember correctly, last year, the commentary is more like there\\'s going to be little or no budget flush. So, I guess my question is like how does this year look in comparison? Is it better, same, worse? Just sort of what\\'s your confidence level on trends?\\nGeorge Kurtz: I think we\\'re looking at it saying, it may not happen, right. So, we\\'re not necessarily counting on that. And if it does happen, fantastic. And you have to again keep in mind that our fiscal year end is the beginning of a new budget cycle as well. So, we have year-end budget if it comes in December and then new budgets in January. But we -- the macro environment, I would say has remained steady. As I said earlier, it\\'s still challenging and we\\'ll see if a budget flush comes, but that\\'s not something that we\\'re counting on.\\nOperator: Thank you. Our next question comes from the line of Jonathan Ho of William Blair.\\nJonathan Ho: Hi. Congratulations on the strong quarter. I just wanted to get a little bit more color around the Charlotte AI revenue potential and maybe what customer use cases are seeing the most interest so far? Thank you.\\nGeorge Kurtz: Well, we announced our pricing at Fal.Con. That was actually well received with customers. And in terms of the overall used cases, as I\\'ve talked about in the past, really it\\'s helping a Tier 1 SOC analyst up level themselves to be a Tier 3, right? It\\'s really driving SOC automation, which is a big focus for us with things like Charlotte and things like Falcon Foundry, how do you drive automation into organizations to give them better outcomes. At the end of the day, they would be willing to pay for something if it\\'s going to save them time or money or make their job easier. And what we\\'ve seen in the customers who are using Charlotte right now in its preview mode is, they\\'re able to do things way faster than they ever could and they\\'re able to explore and ask questions that they haven\\'t been able to do in the past, right? It just makes it so easy to have Charlotte gather up all the information and then take an action on their behalf. So, A, the pricing has resonated well with customers. We\\'ll see how it shakes out as we get into full swing of things. But overall, what we\\'ve put together and the fact that it isn\\'t an independent chatbot, it\\'s really a foundational platform service that\\'s wired into everything that we do and allows automation through all the modules, I think is a home run for us.\\nOperator: Thank you. Our next question comes from the line of Gregg Moskowitz of Mizuho.\\nGregg Moskowitz: Okay, thank you for taking the question. George, in your prepared remarks, you expressed a lot of excitement for Falcon for IT. But in addition to better security, better unification with IT, it also strikes me as having the potential to be a real ROI-driven sale for many customers. Do you agree with that? And if so, is that going to be an angle that you really go after from a go-to-market standpoint?\\nGeorge Kurtz: Well, it\\'s exactly what we\\'re hearing from customers. I was with a customer last night and they were saying, we\\'re so excited for Falcon for IT, because we\\'ve got to work. We find the problems, we document them, and then we\\'ve got to work with the IT teams to get them fixed, and a lot of times their tooling is well behind what we can do with Falcon, and they sort of end up fixing it themselves with IT because we just have the capabilities to do that. So, to be able to carve out a home for IT and seamlessly help them understand their exposures on their assets that matter and then drive the automation to actually remediate those is incredible. And it goes beyond just kind of simple remediation. If you think about all the internal use cases in HR, an employee might have an issue, they might have to investigate it, they might have to do forensics on systems. Systems are remote all over the world. We\\'ve got many, many airlines that use our technology. They don\\'t want to send out an IT person to go fix a kiosk that has a Microsoft blue screen. So what can they do? They can use Falcon for IT to bring it back to health. Tremendous savings in terms of cost and travel and complexity. So this one, I\\'m really, really excited about. And again, we\\'re working to get all the components of what customers need, at least for this first release. And then, I think sort of the sky is the limit of what we can build out there.\\nOperator: Thank you. Our next question comes from the line of Gabriela Borges of Goldman Sachs.\\nGabriela Borges: Good evening. Thank you. George and Burt, I have a question for you. As you think about your fiscal year \\'25 planning assumptions. You mentioned a couple of times that the macro environment is very consistent. Clearly, the execution on growth and profitability is very consistent. I\\'d love your observations on what\\'s incrementally changing. Meaning, as you think about your priorities into next year, what\\'s new versus this time last year? Thank you.\\nGeorge Kurtz: Well, I\\'ll start and then I\\'ll let Burt jump in. I think from a priority perspective just on go-to-market and product-related. You talk about the fiscal piece, Burt. But we continue to drive innovation. Our focus has been on innovation, not integration. And I think that\\'s shown in what we\\'ve been able to deliver in the market, how fast we\\'ve been able to come out with products. Falcon was our richest release of technology. So, we want to continue to be the innovation leader in security, and I think customers will recognize that. From a go-to-market perspective, there\\'s a lot of work that we\\'ve done in terms of really, really making sure we\\'ve got the right hygiene across the enterprise and making sure that we can compress these, basically, time in sales cycles to get deals done when we need to and vector them in, and then also as we go down market into SMB, creating the right products for that segment and then delivering it through the right channels. So, what we have is working. So, it\\'s going to continue to drive innovation, continue to remove friction in go-to-market motion, and continue operational excellence up and down the sales and the partner organization. Burt?\\nBurt Podbere: Thanks, George. So, in terms of how we think about the macro next year, we\\'re not going to predict the change in the macro until we actually see it, right? I think that\\'s going to be the wait-and-see game for most companies and that\\'s certainly the one that we\\'re going to take, but that\\'s how I see it.\\nOperator: Thank you. Our next question comes from the line of Alex Henderson of Needham.\\nAlex Henderson: Great. Thank you so much. Thanks for a great print. But I was hoping you could give us some clarity around some of the key metrics that people use to track conditions, closure rates, whether they\\'re stable, improving or eroding cycle time. I know you\\'ve said your deal size is up and I think at Fal.Con you talked about a significant increase in pipeline build. I think the comment was made on the stage that your pipeline target that they had tried to build was more than doubled the target. So, I was hoping you could talk about what the pipeline looks like as we go into this quarter and into the new year.\\nGeorge Kurtz: Great. Well, when we look at the current macro environment, it is stable. But as I talked about in the Q&A and in the prepared remarks, it\\'s still a challenging environment, it takes a lot of effort to get deals done. And again, it depends on the segments. Some segments are going to be a little bit longer in the enterprise and SMB a little bit shorter. But that being said, when we look out and we look at things like our pipeline, we have a record pipeline and we have that because I think we\\'ve got the best product suite in the industry. We\\'ve got customers that are truly understanding the platform capabilities of what we\\'ve built. They\\'re coming to us and saying, \"Hey, let\\'s sit down and go through your roadmap and our roadmap and let\\'s figure out what we can consolidate.\" So, they\\'re coming to us with the openness of wanting to buy more and leveraging things like Falcon Flex to be able to have bigger commits at CrowdStrike, which is important. And overall, it\\'s, as I said, still a tough environment. But I think we\\'ve got some key metrics like the pipeline that is encouraging to us. And when you look at LogScale and our cloud business and our identity business, I mean, these are absolutely bright stars across the entire platform. And again, keep in mind, we\\'re not selling individual piece parts. People are buying the platform and that\\'s what\\'s driving the growth.\\nOperator: Thank you. Our last question comes from the line of Eric Heath of KeyBanc Capital Markets.\\nEric Heath: Great. Thanks for fitting me in here. George, I guess I\\'ll come back to the SIEM conversation. So, appreciate the color on the synergies of using LogScale to do the analytics on top of your first-party data that you\\'re already capturing, makes a ton of sense. But I guess, can you flush out a bit more as to why your AI engine and your proprietary endpoint data is a better approach to capture the next-gen SIEM opportunity that many seem to be targeting, including those that claim to have more first-party data sources?\\nGeorge Kurtz: Well, when you look at just kind of the data gravity and what customers care about, from a security perspective, I would say 85% of really the value of the data and the data that\\'s generated comes from the endpoint. That really is the richest source of data because that\\'s where it\\'s generated from. As it goes across the network, it gets down-selected and you lose a lot of the fidelity. So, we already have a lot of that data. So let\\'s start there. Then, when you look at LogScale, LogScale is very robust. It has the capabilities to ingest just about any type of third-party data into the product and very effectively without creating an index, which is its unique capabilities, index-free ingestion, which means you get immediate results. So, from a platform perspective, we sort of have the data gravity already. We\\'ve got that 85% of the data. So customers would rather give us 15% that we don\\'t have and leverage LogScale to replace their SIEM. And beyond replacing their SIEM, that\\'s the security use case, we have customers -- many, many customers that use it for all kinds of performance management, logging their infrastructure, logging their Kubernetes clusters. So, when you combine the fact that we\\'re coming out with Falcon for IT, now with LogScale, which can log any data above and beyond security, we think that\\'s a much broader opportunity than just SIEM itself. It opens up the entire data architecture of an organization to CrowdStrike.\\nOperator: Thank you. I would now like to turn the conference back to George Kurtz for closing remarks. Sir?\\nGeorge Kurtz: Well, thank you. And I first want to close by acknowledging the heroic dedication of our team in Israel, who continue to remain focused on CrowdStrike\\'s mission. Our thoughts are with each and every one of you in these trying times of war. Thanks all of you for your time today. We appreciate your interest and look forward to seeing you at our upcoming investor event and happy holidays.\\nOperator: This concludes today\\'s conference call. Thank you for participating. You may now disconnect.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.',\n",
       " \"Snowflake Inc . (NYSE:SNOW) reported its Q3 fiscal 2024 earnings, with product revenue reaching $698 million, marking a 34% year-over-year (YoY) growth. The company added 35 $1 million plus customers in the quarter, and nine of its top 10 customers experienced sequential growth. Snowflake also introduced new technologies such as Snowflake Cortex for AI and machine learning, Snowpark Container Services, and Dynamic Tables. The company's Q4 product revenue is projected to be between $716 million and $721 million, with full-year guidance increased to approximately $2.65 billion.\\nKey takeaways from the earnings call include:Snowflake executives discussed the stabilization of their customer base and evolving sales approach, highlighting the potential impact of new products on consumption and future growth.The company is focusing on enabling AI and machine learning technologies, with a shift in sales approach towards more specialization and teams of specialists.Snowflake announced an expanded partnership with Microsoft (NASDAQ:MSFT), which has positively impacted their business. The company's largest hyperscaler customer is AWS, followed by Microsoft Azure and Google (NASDAQ:GOOGL) Cloud Platform (GCP).The company added two Global 2000 customers last quarter and expects more in the pipeline for Q4.Snowflake expects to see increased consumption as more customers migrate to their platform, particularly in relation to the adoption of Snowpark.The company's product gross margins may face headwinds in Q4 due to the costs associated with new product development and software fine-tuning.During the earnings call, Snowflake executives highlighted the importance of AI and machine learning technologies in driving their business. The company expects the effects of AI to unfold gradually, with incremental workload utilization and consumption growth. The executives also mentioned the upcoming FedRAMP high authorization and the importance of unstructured data and Document AI in driving their business.\\nSnowflake also discussed the utilization of higher tiers by large enterprises, the impact of AI on the business, consumption trends, container services, go-to-market strategies, unstructured data support, and legacy replatforming. They highlighted the continued migration of on-premise workloads to the cloud and the expansion of workloads within existing customers.\\nIn terms of payment models, Snowflake continues to push for three-year contracts but may offer more flexible payment terms in the future. The company's top 10 customers have shown strong quarter-over-quarter growth, driven by large data estates and workload migrations. The company's product gross margins may face headwinds in Q4 due to the costs associated with new product development and software fine-tuning. However, Snowflake does not anticipate significant upside in their long-term margin guidance.InvestingPro InsightsIn light of Snowflake Inc.'s recent Q3 fiscal 2024 earnings report, several key metrics from InvestingPro provide additional insight into the company's financial health and market position. Snowflake's market capitalization stands at a robust $61.88 billion, reflecting investor confidence in its business model and growth prospects. Despite a negative P/E ratio of -69.95, analysts remain optimistic about the company's potential, as indicated by the 20 analysts who have revised their earnings upwards for the upcoming period—suggesting an expectation of improved financial performance.\\nFrom a growth perspective, Snowflake has demonstrated a significant revenue increase of 40.87% over the last twelve months as of Q3 2024, with a gross profit margin of 67.09%. This growth is a testament to the company's ability to scale effectively and maintain profitability at the gross level. However, it is important to note that the company has not been profitable over the last twelve months, with an operating income margin of -40.21%.\\nInvestingPro Tips for Snowflake indicate that the company holds more cash than debt on its balance sheet and analysts anticipate sales growth in the current year. This financial stability, coupled with expected sales growth, may provide a solid foundation for future expansion and innovation.\\nFor readers interested in deeper analysis and additional insights, InvestingPro offers more tips on Snowflake, including its recent strong returns over the last week, month, and three months. Subscribers to InvestingPro can access these valuable insights, and with the special Cyber Monday sale, there's an opportunity to save up to 60% on a subscription. Plus, using the coupon code sfy23, users can get an additional 10% off a 2-year InvestingPro+ subscription. With a total of 15 additional InvestingPro Tips listed for Snowflake, investors can gain a comprehensive understanding of the company's financial standing and market potential.Full transcript - Snowflake Inc (SNOW) Q3 2024:Operator: Good afternoon. Thank you for attending today's Q3 FY 2024 Snowflake Earnings Conference Call. My name is Hannah and I will be your moderator for today's call. All lines will be muted during the presentation portion of the call with an opportunity for questions-and-answers at the end. [Operator Instructions] I would now like to pass the conference over to our host, Jimmy Sexton, Head of Investor Relations at Snowflake. You may go ahead.\\nJimmy Sexton: Good afternoon and thanks for joining us on Snowflake's Q3 fiscal 2024 earnings call. With me in Bozeman, Montana are Frank Slootman, our Chairman and Chief Executive Officer; v, our Chief Financial Officer, and Christian Kleinerman, our Senior Vice President of Product, who will join us for the Q&A session. During today's call, we will review our financial results for the third quarter of fiscal 2024 and discuss our guidance for the fourth quarter and full year fiscal 2024. During today's call, we will make forward-looking statements, including statements related to the expected performance of our business, future financial results, strategy, products and features, long-term growth, our stock repurchase program and overall future prospects. These statements are subject to risks and uncertainties, which could cause them to differ materially from actual results. Information concerning those risks is available in our earnings press release distributed after market close today and in our SEC filings, including our most recently filed Form 10-Q for the fiscal quarter ended July 31, 2023, and the Form 10-Q for the quarter ended of 2023 that we will file with the SEC. We caution you to not place undue reliance on forward-looking statements and undertake no duty or obligation to update any forward-looking statements as a result of new information, future events or changes in our expectations. We'd also like to point out that on today's call, we will report both GAAP and non-GAAP results. We use these non-GAAP financial measures internally for financial and operational decision-making purposes and as a means to evaluate period-to-period comparisons. Non-GAAP financial measures are presented in addition to and not as a substitute for financial measures calculated in accordance with GAAP. To see the reconciliations of these non-GAAP financial measures, please refer to our earnings press release distributed earlier today and our investor presentation, which are posted at investors.snowflake.com. A replay of today's call will also be posted on the website. With that, I would now like to turn the call over to Frank.\\nFrank Slootman: Thanks, Jimmy. Welcome and good afternoon. Q3 product revenue grew 34% year-over-year to reach $698 million. Non-GAAP adjusted free cash flow was $111 million, representing 7% year-over-year growth. Results reflect strong execution in a broadly stabilizing macro environment. while Snowflake's global revenue mix is highly diverse in terms of industries and geographies, the company derives an ever larger revenue share from mainstream enterprises and institutions. This, as compared to a newer crowd of digital natives, have made up many of Snowflake's early adopters. We added 35 $1 million plus customers during the quarter, 9 of our top 10 customers grew sequentially. Generative AI is at the forefront of customer conversations, which in turn drives renewed emphasis on data strategy in preparation of these new technologies. We said it many times, there's no AI strategy without a data strategy. The intelligence we're all aiming for results in the data, hence the quality of that underpinning is critical. Meanwhile, Snowflake has announced and showcased the plethora of new technologies that let customers mobilize AI. We've introduced Snowflake Cortex to leverage AI and machine learning on Snowflake. Cortex is a managed service for inferencing large language models. This opens up direct access to models and specialized operations by translation, sentiment and vector functions. Business analysts and data engineers can now use AI functionality without the fractured highly technical challenges of the AI landscape. Last summer, we introduced Snowpark Container Services, which also serves as the second pillar of our AI enablement strategy. Developers can access any language, any library and flexible hardware inside the governance boundary of Snowflake. More than 70 customers are already using container services in preview with many more waiting in line. Snowflake makes the common AI use case is easy and the advanced use case is possible. We are well positioned for AI based on the scale and scope of our data cloud programmability and governance framework. There are hurdles challenging enterprise adoption of AI and ML. The first is broad access to quality data. Snowflake addresses this challenge through its data sharing architecture. 28% of all our customer share data, up from 22% a year ago and 73% of our $1 million-plus customers are data sharing up from 67% a year ago. AI models can only be as smart as data they are trained on. Security and governance present another challenge for enterprise adoption of AI and the now Snowflake Horizon offers a unified security and governance solution built for AI. Horizon strictly and consistently enforces user privileges on data across use cases, including large language model applications, traditional ML models and ad hoc queries. As part of Horizon, we introduced universal search, which enables customers to search the data cloud. Customers can now discover data and metadata that exists across their accounts and in the Snowflake marketplace. Snowflake continues to win new workloads outside of its traditional. Snowpark's consumption grew 47% quarter-over-quarter. Consumption in October was up over 500% since last year. Over 30% of customers use Snowflake to process unstructured data in October. Consumption of unstructured data was up 17 times year-over-year. Our newest streaming capability, Dynamic Tables entered public preview earlier this year. Approximately 1,500 customers are using the feature and initial adoption is outpacing expectations. We have a number of major new capabilities becoming broadly available in Q4. Our native apps framework will go GA, UniStore for transaction processing, Snowpark Container Services and Apache Iceberg Tables will all enter public preview. These products unlock substantial new workload expansion opportunities. We are campaigning globally to expand our audience. This fall, our Data Cloud World Tour traveled to 26 cities worldwide. In-person attendance at these events reached 23,000 nearly double from last year. Next up is our Build Developer Conference in early December, where we anticipate 35,000 registrations. Build is focused on building apps, data pipelines and AI/ML workflows. We hope to see you there. With that, I will turn the call over to Mike.\\nMike Scarpelli: Thank you, Frank. In Q3, we saw strong consumption from a broad base of customers. Our performance was evenly split between large and small accounts largest customer stabilized as expected. Migrations drove growth in Q3. Our two fastest-growing customers who are both migrating workloads from a legacy vendor. One of these accounts is in their second year on the platform, the other in their eighth year on the platform. We added four customers with more than $5 million and two customers with more than $10 million in trailing 12-month product revenue in the quarter. Growth in September exceeded expectations. For three weeks, consumption grew faster than any other period in the past two years. Consumption continued to grow in the month of October. Q3 represented a strong quarter for bookings execution. Remaining performance obligations grew 23% year-over-year to $3.7 billion. Of the $3.7 billion in RPO, we expect 57% to be recognized as revenue in the next 12 months. APJ and SMB drove growth in net new bookings. We are making significant progress in delivering margin expansion. Non-GAAP product gross margin of 78% was up approximately 300 basis points year-over-year. Improved terms from the cloud service providers have contributed to margin expansion. We also benefit from increasing consumption of higher-priced additions of Snowflake. In Q3, price per credit increased 4% year-over-year. Non-GAAP operating margin of 10% was ahead of expectations. Operating margin benefited from revenue outperformance and increased hiring scrutiny. Non-GAAP adjusted free cash flow margin was 15%, benefiting from favorable timing of collections. We ended the quarter with $4.5 billion in cash, cash equivalents and short-term and long-term investments. Our strong cash position allows us to opportunistically repurchase shares. In Q3, we used $400 million to repurchase 2.6 million shares. Year-to-date, we have used $592 million to repurchase 4 million shares at an average price of $147.5. Now let's turn to guidance. For the fourth quarter, we expect product revenue between $716 million and $721 million, representing year-over-year growth between 29% and 30%. We're increasing our full-year guidance to approximately $2.65 billion, representing 37% year-over-year growth. Consumption trends have improved. We are seeing stability in customer expansion patterns. Our guidance is based on observed patterns and assumes continued stability of consumption. For the fourth quarter, we expect operating margin of 4% and 360 million diluted weighted average shares outstanding. For the full-year, we are increasing our non-GAAP product gross margin guidance. We now expect non-GAAP gross margin of 77%. We still expect a product gross margin headwind in the fourth quarter associated with new products. We are increasing our fiscal 2024 non-GAAP operating margin guidance. We now expect non-GAAP operating margin of 7%. We are increasing our fiscal 2024 non-GAAP adjusted free cash flow margin. We now expect non-GAAP adjusted free cash flow margin of 27%. For the full-year, we expect diluted weighted average shares outstanding of 361 million. We are on track to add more than 1,000 employees this year, inclusive of M&A. With that, operator, you can now open up the line for questions.\\nOperator: Absolutely. [Operator Instructions] Our first question is from the line of Mark Murphy with JPMorgan. Please proceed.\\nMark Murphy: Thank you so much. I love the 11-minute earnings call, and congrats on a fantastic result. So Frank, we are hearing broadly that conversations are starting with generative AI and they're stopping at data, because they find their data estates aren't in good enough shape. Are you sensing more tangible uplift there around that concept that Snowflake might be on the front edge of AI projects and perhaps seeing that spill over into customer conversations or drive more pipeline for some of your other products like Snowpipes and Snowpark and data sharing?\\nFrank Slootman: Generally speaking, yes, one of the interesting things is that customers are now getting preoccupied with their data estates because they have to get them into shape where they can productively take advantage of the newer technologies, which we are now also showcasing and delivering where they can just turn it on and have well-governed frameworks to run them with all the things that they're used to from Snowflake. So it's definitely true that the frenzy and the high degree of interest in AI has a knock-on effect on the interest in data strategy, data platforms. And people are also not just looking at the quality of their data and the optimization of the organization curation of data but also what kind of data they need to be able to have access to. So people are taking a much broader view of their data estates as well in terms of what's in it and what should be in it.\\nMark Murphy: Thank you very much.\\nOperator: Thank you, Mr. Murphy. Our next question is from Keith Weiss with Morgan Stanley. Please proceed.\\nUnidentified Analyst: Great, thank you guys. [Indiscernible] on for Keith. Maybe just start off with a quick question on sort of you mentioned several times on the call, the stabilization that you are seeing in your -- with the new growth. And we've obviously heard that from sort of your hyperscaler peers as well. When you're looking at an account level, could you give us sort of an idea like how much of the stabilization is coming from cost optimization, scrutiny alleviating versus sort of existing customers who slow what migrations leaning in more versus customers leaning more into new products? Is there any way that you can kind of give us more color, what parts are playing out already and which are yet to come maybe into the next year?\\nFrank Slootman: Keith, one of the things, it is Frank. What I mentioned is our customer base has evolved in recent years to include much larger enterprises and institutions who are typically not prone to over consumption and unbridled expansion that they then later have to reset and rationalize. Because of that, the exposure to these drastic resets and optimization that we saw earlier in the year is getting less and less with each incremental quarter. Secondly, people have really driven themselves through these processes and rationalized themselves and are now in a good place to move forward. You can only optimize and rationalize so much. At some point, it's diminishing returns. People get tired of us and they're moving on to things that are now new and interesting, namely preparing for enabling AI and ML technologies.\\nMike Scarpelli: I'll add to that, too, that why we see that stabilization is nine out of our top 10 customers all grew quarterly sequentially. And the other point I'll make is we are seeing a shift, as Frank mentioned. Our biggest customers are mature enterprises we're seeing now and mature enterprises have always scrutinized cost. They always will. And so there's nothing new there, and that will continue. And that's just the way anyone should run a business.\\nUnidentified Analyst: Got it. That's helpful. And then maybe one quick question on your sales and marketing headcount. Obviously, we noticed it's basically flat quarter-over-quarter this quarter. But then again, it's only sort of one number. Can you shed some light in terms of like what regions are you may be leaning more into versus getting more efficiencies or any sort of areas that you're investing in still? Because obviously, your growth seems pretty healthy. So just some color on where you're investing, would be helpful.\\nMike Scarpelli: Yes. So first of all, in general, with sales and marketing heads, most of those typically are added right at the end of Q4 or even more so at the beginning of Q1, so people can get involved in our sales kickoff. And what I would say is we are continuing to invest very heavily in our sales and marketing function, in particular, in Europe. We talked about six months ago or so, we added a new leader there. We have been changing out some people and investing, and we're continuing to, and we will continue to prune underperformers globally and invest more in the right people as we go forward. And APJ is another one that we continue to invest in.\\nUnidentified Analyst: Excellent. Thank you.\\nOperator: Thank you, Mr. Weiss. Our next question is from the line of Raimo Lenschow with Barclays. You may proceed.\\nRaimo Lenschow: Thank you. Congrats from me as well. Guys, you have like a crazy amount of new exciting products coming out. How do you think about the sales setup here now going forward? Because you're going to be able to address lots of different areas from like classic data to kind of more AI. Does that mean your sales approach needs to change, Frank, here going forward? And it doesn't sound like you are going to have a crazy sales force expansion here. Like how do you want to do that going forward and ensure that all these new products are actually finding the market? Thank you.\\nFrank Slootman: Yes that's actually an excellent observation. We have historically had sort of one dominant selling motion that we sort of deployed everywhere and anywhere. It has served us well. But as you correctly observed, the market has really changed. When you go back to 2015, Snowflake really swam in swim lanes that were very narrowly defined and very well understood. Now we're in the mega market, right? These are very, very broad-based platforms that are incredibly capable in many directions. And we've been working very hard, as you've seen in recent years, in delivering just an absolute ton of capabilities to enable these platforms in all these different directions. I mean, our drive towards applications and the whole programmability framework, the onslaught of AI and AM and our capabilities, all of that is coming to fruition. Now we have a ton of irons in the fire, and they're now all getting hot. So from a sales standpoint, we have much more specialization happening and that is going to literally going all over the world because it is impossible for a single person to be to be expert in all these technologies and all these disciplines. So we're going to have people that have general purpose capabilities, sort of core skills, if you will, and then we will have teams of specialists that will augment these groups wherever they're needed. So our basic selling motions and how they are supported will evolve rapidly in the coming year.\\nRaimo Lenschow: Okay, perfect. Makes sense. Thank you.\\nOperator: Thank you, Mr. Lenschow. Our next question is from the line of Kirk Materne with Evercore ISI. Please proceed.\\nKirk Materne: Yes, thanks very much. Congrats on the quarter. I guess, Mike, could you just talk about the impact that some of these newer unstructured data workloads are having on consumption at all, meaning I assume it's still a very, very small part of overall consumption when you look at it, but is that helping with the stabilization? Can they turn into sort of catalyst for acceleration as we get into '24? Anything you to sort of dimensionalize the impact that has as we think out to next year? Thanks.\\nMike Scarpelli: Well, they're definitely helping with stabilization. I can't quantify exactly what unstructured is doing, but it's not just that. It's also we're starting to see the effects more of Snowpark that's doing very well for us right now. Some of the new things we're already seeing very early uptick in streaming and dynamic tables. We really, I would say, Streamlit is built into our forecast for next year. The other products because they're new, we really don't build much in for that because we need history before we can do that. And clearly, we think a lot of these new products that are just going into public preview now and GA next year will be a catalyst for growth for us in 2025.\\nFrank Slootman: One follow-on, Kirk. Document AI is a technology that we've been working on for quite a while, and it really was on the basis of an acquisition we did over a year ago. That is now going into preview. And it is incredibly popular out there and what Document AI does returns to an unstructured document into a semi-structure document. So it can become a full participant in analytical processing. There is a ton of interest in that. And so that really brings the entire estate of unstructured data, which is 80% of the world's data into the analytical sphere. So we do expect that to become very important, and especially because AI and LLMs are so heavily focused on unstructured data, textual data, at least today, this will be something that will be a driver in our business.\\nKirk Materne: Thanks. And Mike, can you just give us an update on where the federal sort of opportunity stands? I know you guys were waiting to hear back on FedRAMP. Any update on that? Thanks.\\nMike Scarpelli: Federal is a huge opportunity, and I would say it's upside for us because it's such a small piece, but we should have our FedRAMP high authorization literally any day. I actually thought I might have had it today. So stay tuned. You'll see an announcement on that very soon.\\nKirk Materne: Thanks. Appreciate it.\\nOperator: Thank you, Mr. Materne. Our next question is from the line of Brad Reback with Stifel. You may proceed.\\nBrad Reback: Great, thanks very much. Just a quick technical question with Graviton 4 now being announced by Amazon (NASDAQ:AMZN), should we think of any potential headwinds there?\\nMike Scarpelli: Well, as we told you before, every year, we count on a 5% headwind associated with software hardware improvements. Graviton 4 was just announced recently. We really have not tested that and not all hardware improvements benefit our software. We expect there will be some, but we just don't know. It's too early, stay tuned and we'll update you when we have more information.\\nBrad Reback: Great, thanks very much.\\nOperator: Thank you, Mr. Reback. Our next question is from the line of Alex Zukin with Wolfe Research. Please proceed.\\nEthan Bruck: Hey, guys. This is Ethan Bruck for Alex Zukin. Congrats on the nice quarter. My question is, if you're assuming the consumption trends persist through 4Q, and that's how you're guiding, I guess, what would be the biggest puts and takes to accelerate growth next year? And in the same vein, when we think about NRR, just given the stabilizing consumption trends, I guess, directionally, when should we think about when would we expect NRR to stabilize trough and around like what level would you expect?\\nMike Scarpelli: So I'll start with -- I'm not going to guide to NRR. I do see it stabilizing. It could dip a little bit more. I do expect over time, NRR, as we've said, we'll converge with our revenue growth. at the size we're at. And clearly, the biggest puts and takes for next year is going to be the -- to continue to see the stabilization we have we're seeing right now and what the impact of a lot of our new initiatives are going to be next year, and it's just too early to tell right now and to guide to that. So stay tuned for our February call when we give guidance for next year.\\nEthan Bruck: Great. Thank you very much, Scarpelli.\\nOperator: Thank you, Mr. Zukin. Our next question is from the line of Brent Thill with Jefferies. You may proceed.\\nBrent Thill: Thanks, Mike. I think you mentioned there is a higher utilization on the higher tiers that you're offering. Can you just extrapolate what you're seeing there?\\nMike Scarpelli: Well, typically, large enterprises are the ones using our business critical in BPS, and those guys are becoming bigger and bigger customers. And as a result, we do see more of our revenue being derived from these very large companies who are using our higher SKU, which has higher margin for us, and that's what we're seeing there.\\nBrent Thill: Okay. And quickly for Frank, when you think about just the overall AI impact, do you feel that this is a bigger tailwind in the back half of '24? Do you think you'll see it coming in early '24? How are your thought process in terms of adoption in this way, helping you? When does that impact hit from a monetization perspective?\\nFrank Slootman: There's going to be a lot of nuances to how AI is going to unfold and translate itself into the effects into our business. It's not just like turning on the switch and all of a sudden, you see incremental consumption happening. As we said during the prepared remarks, we're already seeing that the interest in AI is also driving interest in the data strategy, which then has a knock-on effect on consumption. It's also the expansion of the data universe that people are bringing in quality of the data initiatives. All of that is going to bring incremental workload utilization to Snowflake, even though you would normally not necessarily characterize that as AI, but these are things that are going to enable AI, and it might well be that a ton of the workload of AI is actually the proverbial iceberg where only the tip of it that's ticking above the water is really AI, but everything that has to happen to support us is happening below the service then we're going to be a huge beneficiary of that. We think that Snowflake is super well prepared because our data estates are in a very, very advanced state because our customers have spent years, years and years. And in some cases, literally decades from prior legacy platforms to building these estates and curing the data and organizing and optimizing for the data to be completely trusted and sanctioned in their environment, and that is a huge value when you start tackling AI and ML models.\\nBrent Thill: Thank you.\\nOperator: Thank you, Mr. Thill. Our next question is from the line of Patrick Colville with Scotiabank. You may proceed.\\nPatrick Colville: All right, thank you so much for taking my question. In your prepared remarks, you talked of very strong consumption in September. Consumption continued to grow in October. We're now 29 days through November. Can you just talk about trends thus far this month?\\nMike Scarpelli: What I would say is trends are good, but you have to remember, it also has a big holiday in the U.S. and the week of Thanksgiving is typically a slow week. With that said, I'm happy with the consumption we're seeing, and that's reflected in our guidance.\\nPatrick Colville: Perfect, thank you so much.\\nOperator: Thank you, Mr. Colville Our next question is from the line of Tyler Radke with Citi. You may proceed.\\nTyler Radke: Hi, good afternoon. Thanks for taking the question. I wanted to ask you about container services. So it sounds like you're seeing some good early momentum there, more than 70 customers in private preview. I'm curious how many of those are actually deploying large language models directly on the Snowflake data? And for those customers that are doing that, what type of uplift or how you kind of characterize that consumption when they do that?\\nChristian Kleinerman: Yes. This is Christian. Yes, we have seen a variety of use cases, but a good number of those prior preview customers are leveraging GPU instances in silver and payer services for use of large language models. So for sure, it's a meaningful part of the early adoption use cases and we've got lots of encouragement and excitement to continue the rollout of the preview.\\nTyler Radke: Great. And just a question on go-to-market. So I think earlier this year, you talked about some execution issues and it seems like that's been partially resolved to get good results in APAC. I guess, do you feel like you have all the right key sales leaders in place at this point? Or are there still some roles you're looking for? And I guess, ultimately, do you feel like you've turned the corner on those execution issues as well?\\nFrank Slootman: Yes. It's Frank. Look, in a global sales organization that we're running and we're running all the way from an on-demand selling motion that's unattended, if you will, by salespeople to SMB, to an ISB to mid-range large and then the extremely large customers. The -- it's always a work in process. There's always opportunity for improvement. That's been true for as long as I've been here. That said, I feel we're getting incrementally better, stronger, deeper in terms of our ability to sell. We're becoming much more consistent and more and more productive every day. So a lot of stability and a lot of progress. But this is something that's never over. Anybody who's been in this business knows how this works. It's a very dynamic mix of things. But on the whole, we have to, as a company, a $3 billion or something of that order of magnitude run rate, we lean hard on our organization to show up and deliver those results every quarter. So it's becoming a formidable enterprise in terms of our go-to-market capability.\\nTyler Radke: Great, thank you.\\nOperator: Thank you Mr. Radke. Our next question is from the line of Brent Bracelin with Piper Sandler. You may proceed.\\nBrent Bracelin: Thank you and good afternoon. Frank, we'll start with you here. I think last quarter, unstructured data was mentioned twice. You've called it out here more than a dozen times. You talked about October trends being up, I think, 17x increase year-over-year. How much of a game changer is this, particularly as we think about Document AI and Snowpark Containers coming out next year? Clearly, a focus here. It certainly hasn't been an area you supported in the past, but it feels like there's a sea change movement here. How much of a game changer is unstructured data support relative to the growth opportunity going forward?\\nFrank Slootman: Look, unstructured data, as I said earlier, is the majority of the world's data. And until relatively recently, it's been borderline unusable for analytical purposes, because it is unstructured and you can't reference it in analytical workloads. It's ironic that both through the on slide of large language models that is also extraordinarily of dealing with textual data as well as things just Document AI that Snowflake developed, and it's bringing to the market that this data is going to become a full participant in these types of workloads. It's super exciting. It's going to really enriched and really unlock insights and outcomes and all of that we haven't had before. So this is going to be a driver of our world as we know it in terms of this type of computing.\\nBrent Bracelin: That's helpful. And Mike, if I look at average consumption revenue per customer, those metrics improved on a growth perspective, slightly year-over-year for the first time in over a year. What are the driving factors there? How much of this is just the optimization headwinds now starting to ease versus net new workloads coming on the platform?\\nMike Scarpelli: Well, what I would say, and I'll repeat what I've said at numerous times to investors, optimizations are part of our life. They've been happening at Snowflake from day 1, they will continue to happen. Nothing is new with optimization. I don't see any big ones happening now, but that's not to say they won't happen in the future, because history has shown they happen all the time. Most of the growth that we're seeing within our customers is we talked about two of our biggest growth customers was on-prem legacy migrations into Snowflake. So there's initial migrations, but we're also seeing new workload expansion within existing customers as well, too. So it's -- there's no one thing that's driving it. It's just general consumption we're seeing. And I will say Snowpark is starting to kick in for us. Still not 10% of our revenue. That's a long ways to get there, but it's still meaningful for us.\\nBrent Bracelin: Great to hear, thank you.\\nOperator: Thank you, Mr. Bracelin. Our next question is from the line of Derrick Wood with TD Cowen. You may proceed.\\nDerrick Wood: Great. Thanks. I guess I wanted to dovetail off of that on-premise legacy migrations. And I guess for Frank, I mean, when we saw the macro hit, I think it did cause a slowdown in customers looking to re-platform from on-prem to cloud. Just curious, I mean, I know you guys are kind of highlighting the consumption trends improving here. But wondering what you're seeing in the pipeline for new Global 2000 accounts? And if you're seeing legacy replatforming initiatives start to kick back up now that the macro environment has gotten a little bit more stable?\\nFrank Slootman: Well, I mean, there's no doubt that the legacy replatforming is sort of the hardcore of our business. And almost surprisingly there is just an enormous amount of workloads still sitting on-premise that is still waiting to get migrated to the cloud. So I expect this to continue for a very long period of time. But as what Mike's said is very important. Once you get those data states into the cloud, our new architectures and our new technologies are now enabling opportunities that people haven't had before. And so that drives workload expansion. So it's not just like, okay, we're going to be doing in the cloud where we used to be doing on-premise, and that's the foundation of the business. It is definitely foundational but the opportunity is really what grows from that. That's been the Snowflake story from day one because of what's now possible. We don't have the capacity constraints and people can run unlimited numbers of workloads. And now with all the new technologies in terms of programmability, AI and sky is the limit. I mean, in our conversation with customers, I was telling us is that your problem is no longer the technology. Your problem is your imagination and your budget, right? Not that those are easy things, but technology is not holding us back anymore. It's our ability to harness the technology. And then, of course, you have to pay for it as well.\\nMike Scarpelli: Yes. I'll just add to that, Derrick, too, on your question about Global 2000. Yes, we added two Global 2000 last quarter. And as I've said many times before, selling into a Global 2000 as a campaign, it's a one to two, sometimes three year sales cycle. With that said, we have a number of Global 2000 in our pipeline for Q4. And what I also want to see, too, is not every Global 2000 starts with an on-prem migration. Many of them start with a first-gen cloud solution they had purchased to migrate to Snowflake. Yes, almost all of them have an on-prem estate, but that doesn't always happen at first. It really varies by customer. But with that said, we still see a lot of on-prem migration to be done over the coming years, and it's going to be for many, many years.\\nDerrick Wood: Got it. And Mike, a follow-on for you, really around RPO. And we've heard of more companies looking to take a pay-as-you-go approach. So could you speak to what extent customers are shifting to that approach versus annual or multiyear commits and how that may play into your RPO growth trends looking forward?\\nMike Scarpelli: No. I don't see that, that much. The only time you see pay-as-you-go are ones who had signed a 3-year contract and then they run out of capacity. And then they just pay as they go. Actually, one of our top 10 customers is like that. They have until April to continue before they have to do another contract, if they want to get the same pricing that they have. I actually expect Q4 is going to be a pretty significant bookings quarter with a number of renewals that are up or customers are going to do something. And we continue to push for three-year contracts with our customers. Payment terms, I do expect though that is one of the things I'd rather give up on payment terms and discount to price per credit. And that's really, I've said it all along, I anticipate longer term, customers will want to do more monthly in arrears payment terms, and that is available to customers. It all comes down to what price you want to pay per credit. And to date, most people want the lower price per credit and are willing to pay annually in advance.\\nDerrick Wood: Helpful. Thank you.\\nOperator: Thank you, Mr. Wood. Our next question is from the line of Joel Fishbein with Truist. You may proceed.\\nJoel Fishbein: Thanks for my question and congrats on the great quarter. I guess, Frank, this one is for you. Snowflake Summit, you announced an expanded partnership with Microsoft. And I'm just curious how that partnership is going, and you also announced some joint product integration. So I'd love to get an update how that's going and how you feel about that going forward?\\nFrank Slootman: This is Frank. Yes, we actually saw quite a bit of energy coming from the Azure platform this quarter. The things that we worked on in the renewed relationship with Microsoft is really much better alignment in the field from a compensation standpoint that is just super, super important in our world. And we're seeing the effects of that. And the Microsoft platform really upticked during the quarter. I don't know the exact numbers at hand, but it measurably ticked up. So we're encouraged by the behavior we're seeing in the field, and we're encouraged in the overall sentiment that's developing in the field. So it's definitely healthier than the relationship with Microsoft has been historically. So we're pleased and optimistic about it.\\nJoel Fishbein: Any milestones we should be looking for there from the partnership?\\nFrank Slootman: Not really. We'll let you know.\\nJoel Fishbein: Thanks.\\nOperator: Thank you, Mr. Fishbein. Our next question is from the line of Mike Cikos -- with Mike Cikos. Please proceed.\\nMike Cikos: Thank you for getting me on here. And good to hear some of the earlier comments around Snowpark, which is consistent with some of the growing momentum we've heard in some of our checks. I think the question is probably more for maybe Mike or Christian. But is there enough empirical data or a sizable enough customer base yet for Snowpark to start talking about how these Snowpark customers, I guess, what adoption of Snowpark does for consumption versus non-Snowpark customers? Or maybe how we should think about usage building over time as Snowpark becomes a larger part of those customers' workflows?\\nMike Scarpelli: I think it's still too early to tell. I will say -- we have one customer doing a significant migration, which will increase their consumption on Snowflake to roughly $1.5 million a year. And there are a number of those that we've identified in POCs, but we haven't done the migrations. I see that 1 customer going into production right now, which, by the way, has saved them significantly on their legacy vendor. So clearly, the -- with some customers, it's quite meaningful, the consumption we're starting to see, and we expect that trend will continue into next year.\\nMike Cikos: Understood. Thanks for calling that out, Mike.\\nOperator: Thank you, Mr. Sikos. Our next question is from Patrick Walravens with JMP. You may proceed.\\nOwen Hobbs: Hi, this is Owen Hobbs on for Pat. Thanks for taking the question. Congrats on the strong quarter. So starting off, I guess, how much consumption comes from the different hyperscalers? Is there 1 that kind of is it split evenly between the three or does kind of have the majority of compute share there?\\nMike Scarpelli: No. AWS by far is our biggest, followed by Azure and then GCP. GCP is up to 3% right now. Microsoft Azure is the fastest-growing one, but AWS is still 76% of our business with Microsoft being 21%. As I said, GCP is 3%. And I will tell you, 1 of the reasons why GCP is not as big as just so much more expensive for our customers to operate in GCP than it is in AWS and Azure. And as a result, our salespeople are really not inclined to do much in GCP.\\nOwen Hobbs: Thank you. And then going bigger picture, maybe this is a question for Christian as well. How is the role of the data scientist you sort of kind of changed as we move through the era of big data, machine learning and now into AI? And I guess where do you see that going in the future?\\nChristian Kleinerman: Yes. We see actual interest on both the traditional data science and ML platform. And we've had a number of announcements at Snowflake for those such use cases, but we obviously see lots of interest on generative AI and large language models where we have also expanded obviously capability to do both hosting via Snowpark Container Services, but more seamless inferencing via the new Snowflake Cortex. So we see strong demand from our broad customer base for both types of use cases.\\nOwen Hobbs: Good. Thanks so much.\\nOperator: Thank you, Mr. Walravens. Our next question is from the line of Gregg Moskowitz with Mizuho. You may proceed.\\nGregg Moskowitz: All right. Thank you very much. Frank, since you speak with a lot of large company execs, I'm curious to hear any thoughts you have on IT budgets next year as well as whether there will be incremental budget dollars approved for AI in calendar '24 or if the AI spend will primarily or maybe even entirely come from other areas of IT?\\nFrank Slootman: I don't even hear the words AI and budget in the same sentence. In other words, they're going to make resources available to enable it. But if anything is holding them back is really understanding how to do it. It takes time in tech to mature and evolve deployment architectures where everybody involved is fully confident and comfortable that these are the right ways to do it. So you see a lot of benchmark and compare and contrast experimentation, testing all these kinds of things. And they're going to go through many, many iterations of that. We will as well. And I think that the field will become very proliferated with large foundational models and many, many, many subsector specialized models that are very, very, very deep, but also are very, very narrow in purpose. So it is going to become an enormous field. I will tell you that when you talk to the C-suite in large enterprises, people are looking for a reset of economics, like, for example, in contact centers, pricing optimization, supply chain management, I mean, really very, very big impact opportunities. These are not sort of marginal incremental and that has the attention. Data has always had a promise, but it's really on steroids now under the influence of the newer technologies that we're all excited about.\\nGregg Moskowitz: Very helpful. And then for Mike, really encouraging, of course, to hear that for three weeks, consumption grew faster than any other period over the past two years. Over which three weeks though, did you see that particularly strong consumption? Was that post Labor Day? Or did it span a different time period?\\nMike Scarpelli: Post Labor Day.\\nGregg Moskowitz: Right, Perfect. Thank you.\\nOperator: Thank you, Mr. Moskowitz. Our last question is from the line of Will Power with Baird. You may proceed.\\nWill Power: Okay, great. Thanks. Mike, you referenced that nine of your top 10 customers, it sounds like we're nicely quarter-over-quarter. I guess I wonder, were there any common use cases or products or common threads kind of driving that? Any other color behind the strength there?\\nMike Scarpelli: No, these are just all very large customers with massive data estates that continue to -- one was doing a big migration, but the others are just continuing to move workloads to Snowflake.\\nFrank Slootman: That's very industry-specific as well.\\nWill Power: Okay. And maybe just a quick question on product gross margins. Last couple of quarters, you're kind of already at your longer-term target, I think. It sounds like there may be a couple of headwinds there in Q4, but is this kind of the general level to expect going forward? And maybe any color on kind of the upside surprise, I guess, if you will, given it seems like you've gotten there faster than you might have expected?\\nMike Scarpelli: Yes. I don't really see a lot of upside in our long-term guidance and for a number of reasons. The big headwind that we're seeing this quarter to our margin is with all these new products, that are going into public preview. We have to start to amortize the costs associated with the software development costs that we're required to capitalize under GAAP that are now going to start to be amortized on top of that. In particular, something like Unistar, a lot of times when we introduce new products, many times, we actually have negative contribution margins until it takes us usually up to half a year to nine months to actually fine-tune the software to get the -- to take costs out of operating those new features. And so we just have so many new products that are coming out this quarter that is going to have a headwind on the margin.\\nWill Power: That's helpful. Thank you.\\nOperator: Thank you, Mr. Power. That concludes the question-and-answer session as well as today's call. Thank you for your participation. You may now disconnect your lines.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " 'Investing.com – U.S. stocks were higher after the close on Friday, as gains in the Industrials, Utilities and Basic Materials sectors led shares higher.\\nAt the close in NYSE, the Dow Jones Industrial Average rose 0.82% to hit a new 52-week high, while the S&P 500 index climbed 0.59%, and the NASDAQ Composite index climbed 0.55%.\\nThe best performers of the session on the Dow Jones Industrial Average were Walgreens Boots Alliance Inc (NASDAQ:WBA), which rose 4.26% or 0.85 points to trade at 20.79 at the close. Meanwhile,  Nike  Inc (NYSE:NKE) added 3.26% or 3.58 points to end at 113.48 and Salesforce Inc (NYSE:CRM) was up 3.21% or 8.09 points to 259.99 in late trade.\\nThe worst performers of the session were Intel Corporation (NASDAQ:INTC), which fell 2.15% or 0.96 points to trade at 43.74 at the close. Microsoft Corporation (NASDAQ:MSFT) declined 1.16% or 4.40 points to end at 374.51 and Walmart Inc (NYSE:WMT) was down 0.86% or 1.34 points to 154.35.\\nThe top performers on the S&P 500 were Boston Properties Inc (NYSE:BXP) which rose 11.15% to 63.28, Ulta Beauty Inc (NASDAQ:ULTA) which was up 10.81% to settle at 472.03 and Paramount Global Class B (NASDAQ:PARA) which gained 9.81% to close at 15.78.\\nThe worst performers were  Pfizer  Inc (NYSE:PFE) which was down 5.17% to 28.89 in late trade, Intel Corporation (NASDAQ:INTC) which lost 2.15% to settle at 43.74 and Netflix Inc (NASDAQ:NFLX) which was down 1.74% to 465.74 at the close.\\nThe top performers on the NASDAQ Composite were  Neximmune  Inc (NASDAQ:NEXI) which rose 144.94% to 6.05, Green Giant Inc (NASDAQ:GGE) which was up 66.43% to settle at 0.31 and View Inc (NASDAQ:VIEW) which gained 63.26% to close at 1.03.\\nThe worst performers were  Polar Power Inc  (NASDAQ:POLA) which was down 46.34% to 0.46 in late trade,  Smart for Life  Inc (NASDAQ:SMFL) which lost 43.57% to settle at 1.36 and Incannex Healthcare Ltd ADR (NASDAQ:IXHL) which was down 33.79% to 5.29 at the close.\\nRising stocks outnumbered declining ones on the New York Stock Exchange by 2469 to 466 and 40 ended unchanged; on the Nasdaq Stock Exchange, 2641 rose and 840 declined, while 77 ended unchanged.\\nShares in Pfizer Inc (NYSE:PFE) fell to 3-years lows; losing 5.17% or 1.58 to 28.89. Shares in Salesforce Inc (NYSE:CRM) rose to 52-week highs; gaining 3.21% or 8.09 to 259.99. Shares in Polar Power Inc (NASDAQ:POLA) fell to all time lows; falling 46.34% or 0.39 to 0.46. Shares in Smart for Life Inc (NASDAQ:SMFL) fell to all time lows; losing 43.57% or 1.05 to 1.36. \\nThe CBOE Volatility Index, which measures the implied volatility of S&P 500 options, was down 2.24% to 12.63.\\nGold Futures for December delivery was up 1.62% or 33.00 to $2,071.10 a troy ounce. Elsewhere in commodities trading, Crude oil for delivery in January fell 2.17% or 1.65 to hit $74.31 a barrel, while the February Brent oil contract fell 2.18% or 1.76 to trade at $79.10 a barrel.\\nEUR/USD was unchanged 0.06% to 1.09, while USD/JPY fell 0.91% to 146.85.\\nThe US Dollar Index Futures was down 0.26% at 103.15.',\n",
       " 'Sam Altman, one of the founders of OpenAI, has officially returned to office as the firm’s CEO, ending a whirlwind few weeks caused by his abrupt and unexpected temporary departure. \\nAddressing OpenAI employees in a company memo made public on Nov. 29, Altman confirmed that interim CEO Mira Murati will step down from her position and return to her previous role as chief technology officer.\\nMicrosoft (NASDAQ:MSFT) gets non-voting seat on OpenAI board\\nContinue Reading on Cointelegraph',\n",
       " 'By Akanksha Khushi, Sayantani Ghosh and Krystal Hu (Reuters) -Microsoft will take a non-voting, observer position on OpenAI\\'s board, CEO Sam Altman said in his first official missive after taking back the reins of the company on Wednesday. The observer position means Microsoft (NASDAQ:MSFT)\\'s representative can attend OpenAI\\'s board meetings and access confidential information, but it does not have voting rights on matters including electing or choosing directors.  Microsoft CEO Satya Nadella, who had recruited Altman to Microsoft after his ouster from OpenAI, had said earlier that governance at the ChatGPT maker needs to change. OpenAI said last week announced a new initial board that consists of former Salesforce (NYSE:CRM) co-CEO Bret Taylor as chair and Larry Summers, former U.S. Treasury Secretary. Quora CEO Adam D\\'Angelo, who was part of the board who fired Altman, also stayed on for the new one.  The new OpenAI board is on an active search for six new members with expertise in fields from technology to safety and policy. OpenAI investors are unlikely to get a seat on the non-profit board, sources told Reuters. Microsoft has committed to invest over $10 billion into OpenAI and owns 49% of the company. It did not immediately respond to requests for comment. Mira Murati, who had been OpenAI\\'s chief technology officer and was briefly named interim CEO after Altman\\'s ouster, is once again the company\\'s CTO. OpenAI ousted Altman on Nov. 17 without any detailed cause, setting off alarm bells among investors and employees. He was reinstated four days later with the promise of a new board. Altman\\'s exit sparked confusion about the future of the startup at the center of an artificial intelligence boom. His co-founder Greg Brockman, who had followed Altman out of the company, would return as president, Altman said on Wednesday. \"Greg and I are partners in running this company. We have never quite figured out how to communicate that on the org chart, but we will,\" Altman said.  OpenAI\\'s chief scientist Ilya Sutskever will no longer be part of the board, Altman said.  Sutskever had joined in the effort to fire Altman but later signed an employee letter demanding his return, expressing regret for his \"participation in the board\\'s actions.\" \\n\"I love and respect Ilya, I think he\\'s a guiding light of the field and a gem of a human being. I harbor zero ill will towards him,\" Altman said, adding the company was discussing how Sutskever could continue his work at OpenAI. Apart from Altman, Brockman, Sutskever, D\\'Angelo, OpenAI\\'s previous board consisted of entrepreneur Tasha McCauley, Helen Toner, director of strategy at Georgetown\\'s Center for Security and Emerging Technology.',\n",
       " 'By Muvija M and Martin Coulter LONDON (Reuters) -The president of tech giant Microsoft (NASDAQ:MSFT) said there is no chance of super-intelligent artificial intelligence being created within the next 12 months, and cautioned that the technology could be decades away.  OpenAI cofounder Sam Altman earlier this month was removed as CEO by the company’s board of directors, but was swiftly reinstated after a weekend of outcry from employees and shareholders.  Reuters last week exclusively reported that the ouster came shortly after researchers had contacted the board, warning of a dangerous discovery they feared could have unintended consequences.  The internal project named Q* (pronounced Q-Star) could be a breakthrough in the startup\\'s search for what\\'s known as artificial general intelligence (AGI), one source told Reuters. OpenAI defines AGI as autonomous systems that surpass humans in most economically valuable tasks. However, Microsoft President Brad Smith, speaking to reporters in Britain on Thursday, rejected claims of a dangerous breakthrough. \"There\\'s absolutely no probability that you\\'re going to see this so-called AGI, where computers are more powerful than people, in the next 12 months. It\\'s going to take years, if not many decades, but I still think the time to focus on safety is now,\" he said.  Sources told Reuters that the warning to OpenAI\\'s board was one factor among a longer list of grievances that led to Altman\\'s firing, as well as concerns over commercializing advances before assessing their risks.  \\nAsked if such a discovery contributed to Altman\\'s removal, Smith said: \"I don\\'t think that is the case at all. I think there obviously was a divergence between the board and others, but it wasn\\'t fundamentally about a concern like that.  “What we really need are safety brakes. Just like you have a safety break in an elevator, a circuit breaker for electricity, an emergency brake for a bus – there ought to be safety breaks in AI systems that control critical infrastructure, so that they always remain under human control,” Smith added.',\n",
       " 'By Martin Coulter LONDON (Reuters) -Google has called on Britain’s antitrust regulator to take action against Microsoft (NASDAQ:MSFT), claiming its business practices had left rivals at a significant disadvantage, according to a letter seen by Reuters. Microsoft and Amazon (NASDAQ:AMZN) have faced mounting scrutiny around the world over their dominance of the cloud computing industry, with regulators in Britain, the European Union, and the U.S. probing their market power. The CMA (Competition and Markets Authority) launched an investigation into Britain’s cloud computing industry in October, following a referral from media regulator Ofcom which highlighted Amazon and Microsoft’s dominance of the market. In 2022, Amazon Web Services (AWS) and Microsoft\\'s Azure had a combined 70-80% share of Britain\\'s public cloud infrastructure services market, Ofcom said. Google’s cloud division was their closest competitor, at around 5-10%. In a letter submitted to the CMA, Google (NASDAQ:GOOGL) said Microsoft’s licensing practices unfairly discouraged customers from using competitor services, even as a secondary provider alongside Azure. “With Microsoft’s licensing restrictions in particular, UK customers are left with no economically reasonable alternative but to use Azure as their cloud services provider, even if they prefer the prices, quality, security, innovations, and features of rivals,” Google said in its letter to the CMA. Such practices directly harmed customers, and were the only significant barrier to competition in Britain’s cloud computing market, the company said. The CMA declined to comment. TACKLING CONCERNS  Microsoft last year updated its licensing rules to address such concerns and promote competition, though the changes did not satisfy rivals.  A Microsoft spokesperson said the company had worked with independent cloud providers to address concerns and provide opportunity and that more than 100 worldwide had taken advantage of the changes. \"As the latest independent data shows, competition between cloud hyperscalers remains healthy. In the second quarter of 2023 Microsoft and Google made equally small gains on AWS, which continues to remain the global market leader by a significant margin,\" the Microsoft spokesperson said. Speaking to Reuters, Google Cloud Vice President Amit Zavery criticised Microsoft’s practices, and said his company was committed to a multi-cloud approach, in which customers could easily move between providers depending on their needs. \"A lot of our software and cloud services interoperate, and can run on AWS or on Azure as well, so you\\'re not restricted,\" he said. \"If you don\\'t fix this, eventually you will have fewer cloud providers, and then innovation will not really happen, and investments will start shrinking.\" At issue was Microsoft\\'s decision to update the terms for when customers wanted to use their Windows or other software licenses in the cloud, effectively resulting in higher costs if they used Google or AWS instead of Microsoft\\'s Azure. Asked why Amazon, which boasts a larger share of the cloud market than Microsoft, did not pose a similarly anticompetitive risk, Zavery said AWS consumers were not facing the same restrictions. “There are some issues, in terms of cloud interoperability, but we can fix that. That\\'s a discussion between providers, which is much understood, and customers are forcing that conversation,” he said. \\n“The problem we run into with Microsoft is that there\\'s no technical issue, but you have licensing restrictions which means we are now being prevented from competing.” \\xa0 Google made six recommendations to the CMA, including forcing Microsoft to improve interoperability for customers using Azure and alongside other cloud services, and banning it from withholding security updates from those that switch.',\n",
       " 'Sony  (NYSE:SONY) and Microsoft (NASDAQ:MSFT), two of the biggest names in gaming, have been making significant moves towards blockchain technology in recent years. This is an important development, as it has the potential to not only revolutionize the gaming industry but it could be the Trojan horse that makes blockchain a mainstream possibility.\\nSony has filed a patent for a system allowing players to transfer digital assets between PlayStation titles using blockchain technology. This would enable unified progress and asset ownership across different games rather than having assets locked to individual games. Sony has also explored tokenizing in-game assets, allowing players to sell and trade them on secondary markets.The gaming market could grow by more than 10% between 2022 and 2025, according to some estimates. Source: Newzoo.Olga Vorobyeva is the founder of Vox Consulting, a blockchain advisory firm, and a former head of marketing at SwissBorg, a crypto wealth management platform. She is a founding member of the Swiss Blockchain Association and advisor to startups in Switzerland\\'s \"Crypto Valley.\" She also serves as a mentor to the Alchemist startup accelerator. She holds a master\\'s degree from the Plekhanov University of Economics and a certificate in management from The Wharton School of Business.Continue Reading on Cointelegraph',\n",
       " None,\n",
       " '(Reuters) - We asked ChatGPT, OpenAI\\'s viral chatbot, how it felt on its first birthday. This was its reply:  \"Thank you for the birthday wishes! However, it\\'s important to note that as a computer program, I don\\'t have feelings or consciousness, so I don\\'t experience emotions like humans do.\" Still, its uncannily human-like responses have taken the world by storm in the past year. And while it has answered millions of user prompts, its growing influence has raised questions about the role of AI in society.  Its parent, OpenAI, was also jolted this month by a tumultuous boardroom battle that saw the sudden ouster and return of CEO Sam Altman. ChatGPT became the fastest-growing software application in the world within six months of its launch. It also sparked the launch of rival chatbots from Microsoft (NASDAQ:MSFT), Alphabet (NASDAQ:GOOGL) and a bevy of startups that tapped the hype to secure billions in funding. The generative AI craze has disrupted several industries from cloud computing and customer service to movie editing and screenplay writing. Here are four charts on ChatGPT and the impact of generative AI: CHATGPT DOMINATES DESPITE RISE OF COMPETITORS ChatGPT\\'s competitors include Bard, Anthropic\\'s Claude, Character.AI and Microsoft\\'s CoPilot, which have seen a surge in users. ChatGPT, however, commands the lion\\'s share of the market. CHATGPT APP DOWNLOADS Six months after ChatGPT\\'s website launch, OpenAI introduced the chatbot application to Apple (NASDAQ:AAPL)\\'s iOS in May and later on Android in July. Downloads of the app on both platforms have steadily increased on both platforms, with OpenAI seeing revenue from in-app purchases, according to data analytics firm Apptopia. WINNERS OF THE AI BOOM Nvidia (NASDAQ:NVDA) became the first and the only chip company to join the $1 trillion valuation club and is widely considered the biggest winner of the AI boom due to its position as the key supplier of the chips used to power ChatGPT and other generative AI applications. With these applications running mostly on the cloud, vendors of cloud computing services, including Microsoft, Amazon (NASDAQ:AMZN) and Alphabet, have also seen their shares surge. BIG TECH POURS BILLIONS INTO AI ChatGPT\\'s launch sparked massive investments from the top tech players. Microsoft and Alphabet have invested billions to improve their cloud computing capabilities and take on more AI workloads as businesses embrace such tools. \\nCONTROVERSIES OpenAI and its backer Microsoft have been slapped with several lawsuits that have been brought by groups of copyright owners, including authors John Grisham, George R.R. Martin and Jonathan Franzen, over the alleged misuse of their work to train AI systems. The companies have denied the allegations.',\n",
       " 'By Stefania Spezzati and Karin Strohecker LONDON (Reuters) - Software firm SUSE, whose clients include Microsoft (NASDAQ:MSFT) and BMW (ETR:BMWG), told investors in January it was tightening its scrutiny of commercial deals.  SUSE, whose software helps large companies run servers,\\u202fsaid in its annual report for 2022 that to ensure sales were \"appropriately authorized\" it had created a deal desk – a department commonly used by software businesses to vet commercial and contractual terms.  Founded in 1992, SUSE had been publicly traded since 2021 and creating a deal desk aligned it with other listed companies in the sector.  \"SUSE may enter into high-risk or commercially inappropriate deals if it does not exercise effective control over the Sales organization,\" it said in the report. The company noted that \"\\'Commercial governance\\' is a newly identified risk this year\" and a heatmap in the report ranked the risk as \"possible\" and its impact on the business as \"high\". Reuters wasn\\'t able to establish what prompted that warning. Nine company \\u202fdocuments seen\\u202fby Reuters relating to two of SUSE\\'s recent sales, as well as interviews with\\u202f three people with knowledge of the transactions, shed light on some examples of the company\\'s decision-making as it has tried to \\u202ftighten commercial controls. Lawyers for SUSE said in a letter to Reuters on Sept. 22 that the creation of the deal desk was \"nothing more than a natural governance maturity exercise, undertaken by a scaling and young listed business\".  SUSE\\'s majority shareholder, EQT (ST:EQTAB), in August offered to take the software company private\\u202f after it slashed sales targets this year, its CEO and CFO departed, and its shares plunged. SUSE held an extraordinary meeting on Nov. 13 at which shareholders approved to delist the stock from the exchange.  \"Execution challenges and several changes during the past twelve months have impacted SUSE\\'s operating performance and market valuation,\" EQT said in a statement on the public offer in August. It added that leaving the public stock market would ease pressure from \"short-term demands\" on the new management. EQT declined to comment for this article.  Its offer for SUSE shares, which were traded in Frankfurt, valued\\u202f SUSE\\u202f at 2.72 billion euros ($3 billion), some 47% below the level of the 2021 IPO. SUSE shares last traded at 10.89 euros on Nov. 13 before the delisting, according to the exchange Deutsche Boerse (ETR:DB1Gn), 64% below its IPO price.  SUSE\\'s revenue in the financial year to October 31, 2022 was $653 million. Its 2022 annual report, filed on January 19,\\u202f said the deal desk should review any deals greater than $500,000. Lawyers for SUSE\\'s former CEO Melissa Di Donato, who stood down on March 21, told Reuters the deal desk was created after a joint decision by the executive team and the company\\'s board.  According to four documents reviewed by Reuters and two of the people with knowledge of the situation, Di Donato greenlighted the commercial terms of a roughly $1.4 million sale to South African utility Eskom in late January, bypassing the deal desk\\'s scrutiny in order to speed the process, less than two weeks after SUSE told investors the desk would help to improve controls.  On January 30,\\u202f after commercial terms of the Eskom deal had been agreed, a senior SUSE executive said in an email to other company officials that the terms had not been submitted to the deal desk.  In late February, an executive told colleagues in an email that the sale had not been scrutinized by the deal desk before being agreed but the team would review the terms to familiarize itself with the transaction, according to the documents and two people with knowledge of the transaction.  The deal would still be logged into SUSE\\'s systems as approved by Di Donato without sign-off from the deal desk, the executive said. Reuters could not determine if this was done. There was no suggestion in the documents reviewed by the news agency of any problematic issues that arose from the terms of the deal.  A lawyer for Di Donato said in the letter to Reuters that it was \"highly misleading\" to suggest she had reached the Eskom agreement without submitting it for the deal desk\\'s approval: \"the deal desk was aware of the deal. Nothing was skipped.\" SUSE lawyers didn\\'t address directly whether approval from the deal desk had been skipped on the Eskom transaction.  A spokesperson for Eskom did not comment on SUSE\\'s due diligence but said that \"Eskom\\'s established processes and governance structures were adhered to during this procurement.\"  DISCOUNTED DEAL  As part of the new risks identified in the annual report, SUSE warned in January that: \"quarter-end pressure may exacerbate the risk as sales staff try to get deals over the line.\" The documents \\u202freviewed by Reuters show the efforts SUSE made to\\u202f clinch one deal: in December 2022 SUSE discounted a sale to Bank of New York Mellon (NYSE:BK) by more than 90% from the listed price.\\u202f  SUSE sales growth was slowing at the time. Di Donato agreed to offer BNY Mellon the discount on a package of services originally listed at several million dollars, according \\u202fto emails and a person familiar with the transaction. Reuters could not determine what SUSE\\'s original offer was.  Under the discounted deal, BNY Mellon was offered a one-year package that included\\u202f SUSE\\'s Platinum service and 10,000 \\'nodes\\', according to the documents and one of the people. Lawyers for Di Donato said \"the scope, the services and the size of the BNY Mellon deal changed from what was originally on offer and, therefore, the final price changed accordingly.\" Di Donato\\'s lawyer added \"it was not part of her responsibilities as the CEO of SUSE to track discounts on deals,\" and \"the discount was approved in collaboration with others\". A spokesperson for BNY Mellon declined to comment on the deal.  Lawyers for SUSE did not comment on the terms of the deal, saying it was confidential. Reuters could not determine if the deal was vetted by the deal desk nor how many deals have been going through the unit.\\u202f\\u202f  Without referring to SUSE specifically, Christian Strenger, director of the Corporate Governance Institute (CGI) at the Frankfurt School of Finance & Management, said that software-related services companies, in particular, have pressure to meet quarterly and year-end performance targets as they often rely on large individual sales.  Di Donato\\'s lawyers said that quarter-end pressures affected all companies: \"Our client has never forced a sales team to close a deal that was not good for the business.\" SUSE and Reuters parent company,  Thomson Reuters  (NYSE:TRI), are involved in litigation over the use of SUSE software products. SUSE claims that Thomson Reuters breached the terms that allegedly governed its use of SUSE software products.  ($1 = 0.9096 euros)',\n",
       " \"In recent trading sessions,  Oracle Corp . (NYSE:ORCL) has experienced a slight decline amid a landscape of fluctuating market movements. On Wednesday, the technology giant's shares dipped by a modest 0.03%, closing at $116.21. This marginal drop occurred against a backdrop of mixed market performance, with the Dow Jones Industrial Average inching up by 0.04%, while the S&P 500 saw a decline of 0.09%. This marks Oracle's second day in a row of stock price decreases.\\nDespite the recent dips, Oracle's current stock price stands robust when compared to its industry counterparts such as Microsoft (NASDAQ:MSFT), which closed at $378.85, and Alphabet (NASDAQ:GOOGL) Class C (NASDAQ:GOOG), which ended at $136.40. However, Oracle has yet to reclaim its June high of $127.54. Additionally, the company's trading volume has been notably lower than usual, trailing the 50-day average by more than 1 million shares.\\nInvestors may be keeping a watchful eye on Oracle's performance as it navigates through the dynamic market conditions. While the company's recent stock movements have been relatively small, they are occurring within a broader context of market uncertainty where even minor changes can signal investor sentiment and broader economic trends.InvestingPro InsightsIn light of Oracle Corp.'s recent market activity, certain metrics from InvestingPro provide additional context for investors. Oracle's market capitalization stands at a substantial $318.34 billion, and the company's price-to-earnings (P/E) ratio is 33.54, which adjusts to 31.25 for the last twelve months as of Q1 2024. This suggests a valuation that is high relative to near-term earnings growth, an InvestingPro Tip to consider for those evaluating the stock's current price.\\nAdditionally, Oracle's revenue growth of 15.41% over the last twelve months as of Q1 2024 reflects a solid performance in the technology sector. Despite the stock's recent dip, Oracle's dividend yield is 1.38%, with a notable dividend growth of 25.0% over the last twelve months, indicating a commitment to returning value to shareholders—a factor that has been consistent, as Oracle has raised its dividend for 10 consecutive years.\\nFor investors looking for a deeper dive into Oracle's financial health and future prospects, InvestingPro offers more insights. There are 13 additional InvestingPro Tips available, including analysis on the company's liquidity and valuation multiples. For those interested in leveraging these insights, the InvestingPro subscription is now on a special Cyber Monday sale with a discount of up to 55%. Plus, use the coupon code sfy23 to get an additional 10% off a 2-year InvestingPro+ subscription, providing a comprehensive tool for informed investment decisions.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " \"Quiver Quantitative - In the face of a robust stock market rally in 2023, a growing number of retail traders are cashing out, fearing an impending downturn. This sentiment arises from concerns over the sustainability of recent gains, particularly those driven by a small group of technology giants. David Noonan, a seasoned trader, exemplifies this cautious approach, having shifted to cash and short positions in Apple (NASDAQ:AAPL). This trend reflects a broader skepticism among investors about the market's reliance on key tech companies like Alphabet (NASDAQ:GOOGL), Amazon (NASDAQ:AMZN), Meta (NASDAQ:META), Microsoft (NASDAQ:MSFT), Nvidia (NASDAQ:NVDA), and Tesla (NASDAQ:TSLA), collectively known as the 'magnificent seven.'\\nRecent market data underscores this cautious approach, with individual investors selling nearly $16 billion in stocks in October, marking the highest sell-off in two years. Even exchange-traded funds (ETFs) focusing on the tech sector, such as ProShares UltraPro QQQ (TQQQ), have experienced significant outflows. While some investors, like Gerardo Giusti, still view tech giants as viable long-term investments, their short-term prospects appear less certain. Giusti's current strategy includes diversifying into gold (GLD (NYSE:GLD)), crypto (COIN), and cyclical sectors, while closely monitoring the Fed's interest rate decisions.\\nAs the Fed signals a cautious approach to future rate hikes, optimism for rate cuts in 2024 has fueled recent stock surges. However, the uncertainty surrounding the 2024 presidential election and potential economic slowdown keeps traders like Giusti on alert for market shifts. On the other hand, Ashton Jones, a financial analyst and part-time trader, anticipates a year-end rally but is preparing for a pullback in January. His plan to short the market reflects a common sentiment among traders: while the market's current trajectory defies gravity, a correction seems inevitable, driven by profit-taking and seasonal patterns.\\nIn summary, the current market scenario presents a complex picture for retail traders. While the stock rally has been strong, the heavy dependence on a few tech giants combined with potential economic headwinds and political uncertainties has prompted many to secure their gains and adopt a more defensive stance.\\nThis article was originally published on Quiver Quantitative\",\n",
       " 'Quiver Quantitative - Global bond markets are rallying, marking November as the best month since 2008, buoyed by growing optimism that the Federal Reserve might start easing interest rates in the first half of 2024. This surge in bond markets was catalyzed by recent U.S. economic data supporting a \\'Goldilocks scenario\\', where growth is just right – neither too hot nor too cold. Notably, two-year Treasury yields fell to 4.65%, while Fed swaps now anticipate a quarter-point rate cut by May. Meanwhile, the S&P 500 exhibits signs of reaching \"overbought\" levels, with mixed performances among key players such as  Nvidia Corp . (NASDAQ:NVDA), Tesla (NASDAQ:TSLA) Inc., and  Microsoft Corp  (NASDAQ:MSFT). Oil prices are also on the rise, anticipating the outcomes of the upcoming OPEC+ meeting.\\nThe U.S. economy showed remarkable growth in the third quarter, recording its fastest expansion in about two years. However, consumer spending saw a slower increase, attributed to reduced services spending. Contrasting with this robust GDP growth, the Federal Reserve\\'s preferred inflation metric, the personal consumption expenditures price index, was revised lower. The Beige Book report from the Fed indicated a slowdown in economic activity, as consumer discretionary spending wanes.\\nThe bond market\\'s November rally reflects speculation that the Fed\\'s aggressive rate hikes may be nearing an end. This scenario is supported by several Federal Reserve officials, including Cleveland President Loretta Mester and Atlanta\\'s Raphael Bostic, suggesting a pause or potential cut in rates. Despite these indicators, the U.S. economy remains robust, as highlighted by economist Neil Dutta, who sees a \"soft-landing nirvana\" for equity market investors and a potential for bull steepening in the yield curve for bond investors.\\nIn summary, the bond market\\'s strong performance, combined with economic data and Fed officials\\' comments, points to a potentially favorable period for investors. However, the juxtaposition of a robust GDP against a backdrop of slowing consumer spending and revised inflation metrics presents a complex picture. The anticipated Fed pivot and the upcoming decisions of OPEC+ are key factors that will continue to shape market dynamics in the near term.\\nThis article was originally published on Quiver Quantitative',\n",
       " 'Investing.com – U.S. stocks were mixed after the close on Wednesday, as gains in the Telecoms, Financials and Industrials sectors led shares higher while losses in the Oil & Gas, Utilities and Consumer Services sectors led shares lower.\\nAt the close in NYSE, the Dow Jones Industrial Average added 0.04% to hit a new 3-months high, while the S&P 500 index lost 0.09%, and the NASDAQ Composite index lost 0.16%.\\nThe best performers of the session on the Dow Jones Industrial Average were Salesforce Inc (NYSE:CRM), which rose 2.43% or 5.46 points to trade at 230.38 at the close. Meanwhile, Intel Corporation (NASDAQ:INTC) added 1.61% or 0.71 points to end at 44.94 and  Goldman Sachs Group  Inc (NYSE:GS) was up 1.58% or 5.28 points to 340.18 in late trade.\\nThe worst performers of the session were Walmart Inc (NYSE:WMT), which fell 1.59% or 2.53 points to trade at 156.11 at the close.  Chevron  Corp (NYSE:CVX) declined 1.09% or 1.59 points to end at 143.92 and Microsoft Corporation (NASDAQ:MSFT) was down 1.01% or 3.85 points to 378.85.\\nThe top performers on the S&P 500 were NetApp Inc (NASDAQ:NTAP) which rose 14.63% to 89.54, General Motors Company (NYSE:GM) which was up 9.35% to settle at 31.59 and  Charles Schwab  Corp (NYSE:SCHW) which gained 6.83% to close at 59.62.\\nThe worst performers were  Cigna Corp  (NYSE:CI) which was down 8.12% to 262.86 in late trade,  Humana Inc  (NYSE:HUM) which lost 5.52% to settle at 482.26 and  Las Vegas Sands  Corp (NYSE:LVS) which was down 4.89% to 45.33 at the close.\\nThe top performers on the NASDAQ Composite were Vivos Therapeutics\\xa0Inc (NASDAQ:VVOS) which rose 831.82% to 41.00, Altamira Therapeutics Ltd (NASDAQ:CYTO) which was up 180.22% to settle at 0.64 and  Yoshiharu Global  Co (NASDAQ:YOSH) which gained 92.69% to close at 9.23.\\nThe worst performers were  Biovie Inc  (NASDAQ:BIVI) which was down 60.72% to 1.96 in late trade,  Treasure Global  Inc (NASDAQ:TGL) which lost 49.37% to settle at 0.10 and Seelos Therapeutics Inc (NASDAQ:SEEL) which was down 47.48% to 1.25 at the close.\\nRising stocks outnumbered declining ones on the New York Stock Exchange by 1884 to 1027 and 65 ended unchanged; on the Nasdaq Stock Exchange, 2006 rose and 1469 declined, while 93 ended unchanged.\\nShares in NetApp Inc (NASDAQ:NTAP) rose to 52-week highs; gaining 14.63% or 11.43 to 89.54. Shares in Intel Corporation (NASDAQ:INTC) rose to 52-week highs; gaining 1.61% or 0.71 to 44.94. Shares in Biovie Inc (NASDAQ:BIVI) fell to 52-week lows; down 60.72% or 3.03 to 1.96. Shares in Treasure Global Inc (NASDAQ:TGL) fell to all time lows; falling 49.37% or 0.10 to 0.10. Shares in Seelos Therapeutics Inc (NASDAQ:SEEL) fell to all time lows; down 47.48% or 1.13 to 1.25. \\nThe CBOE Volatility Index, which measures the implied volatility of S&P 500 options, was up 2.29% to 12.98.\\nGold Futures for December delivery was up 0.26% or 5.35 to $2,045.35 a troy ounce. Elsewhere in commodities trading, Crude oil for delivery in January rose 1.71% or 1.31 to hit $77.72 a barrel, while the February Brent oil contract rose 1.46% or 1.19 to trade at $82.66 a barrel.\\nEUR/USD was unchanged 0.15% to 1.10, while USD/JPY fell 0.19% to 147.21.\\nThe US Dollar Index Futures was up 0.09% at 102.74.',\n",
       " \"CGI Group (NYSE:GIB), a prominent IT and business consulting firm, has recently enhanced its cybersecurity capabilities by joining the Microsoft (NASDAQ:MSFT) Intelligent Security Association (MISA). This collaboration integrates CGI's security solutions with Microsoft's advanced technologies, such as Defender for Cloud, to provide clients with enriched digital transformation services.\\nThe strategic move was announced on Monday by Raymond Daoud of CGI Group, who highlighted the importance of collaborative security efforts in delivering successful digital transformations. Maria Thomson, also from CGI, pointed out that the MISA membership will lead to improved threat visibility and protection, thanks to the integration of Microsoft's technologies.\\nCGI's decision to join MISA follows a significant five-year agreement with U.S. Strategic Command, focusing on Global Data Integration and enhancing operational confidence. The market responded positively to this news, with CGI's stock on the TSX modestly climbing by 0.2% to close at $138.01 on Monday.InvestingPro InsightsAs CGI Group cements its position in the cybersecurity landscape through strategic partnerships and contracts, its financial health remains a critical factor for investors. The company's commitment to high earnings quality is reflected in its free cash flow, which consistently exceeds net income. This is a sign of robust financial management and a key indicator of the company's ability to sustain and invest in its growth initiatives, such as the recent collaboration with Microsoft's Intelligent Security Association.\\nInvestingPro data highlights CGI Group's strong market presence with an adjusted market capitalization of $23.57 billion. The company's price-to-earnings (P/E) ratio stands at 20.08, with a slight adjustment to 19.1 when considering the last twelve months as of Q4 2023. This suggests that while the company is trading at a higher P/E ratio relative to near-term earnings growth, it is recognized for its stability and potential by the market.\\nWith a price to book ratio of 3.86 and a revenue growth of 11.11% over the last twelve months as of Q4 2023, CGI Group demonstrates a strong balance between value and growth, which should reassure investors looking for a stable investment with the potential for appreciation. Additionally, the company's ability to yield high returns on invested capital is an InvestingPro Tip that underscores its efficiency in utilizing investor funds to generate profit.\\nFor readers interested in a deeper dive into CGI Group's financial metrics and strategic positioning, InvestingPro offers a suite of additional tips. Currently, there are 15 more InvestingPro Tips available, which can be accessed with a subscription. Notably, InvestingPro subscription is now on a special Cyber Monday sale, offering up to a 55% discount, providing an opportune moment for investors to gain comprehensive insights into CGI Group and other potential investment opportunities.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " \"CGI Group (NYSE:GIB), a prominent IT and business consulting firm, has recently enhanced its cybersecurity capabilities by joining the Microsoft (NASDAQ:MSFT) Intelligent Security Association (MISA). This collaboration integrates CGI's security solutions with Microsoft's advanced technologies, such as Defender for Cloud, to provide clients with enriched digital transformation services.\\nThe strategic move was announced on Monday by Raymond Daoud of CGI Group, who highlighted the importance of collaborative security efforts in delivering successful digital transformations. Maria Thomson, also from CGI, pointed out that the MISA membership will lead to improved threat visibility and protection, thanks to the integration of Microsoft's technologies.\\nCGI's decision to join MISA follows a significant five-year agreement with U.S. Strategic Command, focusing on Global Data Integration and enhancing operational confidence. The market responded positively to this news, with CGI's stock on the TSX modestly climbing by 0.2% to close at $138.01 on Monday.InvestingPro InsightsAs CGI Group cements its position in the cybersecurity landscape through strategic partnerships and contracts, its financial health remains a critical factor for investors. The company's commitment to high earnings quality is reflected in its free cash flow, which consistently exceeds net income. This is a sign of robust financial management and a key indicator of the company's ability to sustain and invest in its growth initiatives, such as the recent collaboration with Microsoft's Intelligent Security Association.\\nInvestingPro data highlights CGI Group's strong market presence with an adjusted market capitalization of $23.57 billion. The company's price-to-earnings (P/E) ratio stands at 20.08, with a slight adjustment to 19.1 when considering the last twelve months as of Q4 2023. This suggests that while the company is trading at a higher P/E ratio relative to near-term earnings growth, it is recognized for its stability and potential by the market.\\nWith a price to book ratio of 3.86 and a revenue growth of 11.11% over the last twelve months as of Q4 2023, CGI Group demonstrates a strong balance between value and growth, which should reassure investors looking for a stable investment with the potential for appreciation. Additionally, the company's ability to yield high returns on invested capital is an InvestingPro Tip that underscores its efficiency in utilizing investor funds to generate profit.\\nFor readers interested in a deeper dive into CGI Group's financial metrics and strategic positioning, InvestingPro offers a suite of additional tips. Currently, there are 15 more InvestingPro Tips available, which can be accessed with a subscription. Notably, InvestingPro subscription is now on a special Cyber Monday sale, offering up to a 55% discount, providing an opportune moment for investors to gain comprehensive insights into CGI Group and other potential investment opportunities.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " \"By Stephen Nellis (Reuters) -Amazon.com on Tuesday announced a new artificial intelligence chip for its cloud computing service as competition with Microsoft (NASDAQ:MSFT) to dominate the market for artificial intelligence heats up. At a conference in Las Vegas, Amazon (NASDAQ:AMZN) Web Services (AWS) Chief Executive Adam Selipsky announced Trainium2, the second generation of chip designed for training AI systems. Selipsky said the new version is four times as fast as its predecessor while being twice as energy efficient. The AWS move comes weeks after Microsoft announced its own AI chip called Maia. The Trainium2 chip will also compete against AI chips from Alphabet (NASDAQ:GOOGL)'s Google, which has offered its Tensor Processing Unit (TPU) to its cloud computing customers since 2018. Selipsky said that AWS will start offering the new training chips next year. The proliferation of custom chips comes amid a scramble to find the computing power to develop technologies such as large language models that form the basis of services similar to ChatGPT. The cloud computing firms are offering their chips as a complement to Nvidia (NASDAQ:NVDA), the market leader in AI chips whose products have been in short supply for the past year. AWS also on Tuesday said that it will offer Nvidia's newest chips on its cloud service. \\nSelipsky on Tuesday also announced Graviton4, the cloud firm's fourth custom central processor chip, which it said is 30% faster than its predecessor. The news comes weeks after Microsoft announced its own custom chip called Cobalt designed to compete with Amazon's Graviton series. Both AWS and Microsoft are using technology from Arm Ltd in their chips, part of an ongoing trend away from chips made by Intel (NASDAQ:INTC) and Advanced Micro Devices (NASDAQ:AMD) in cloud computing. Oracle (NYSE:ORCL) is using chips from startup Ampere Computing for its cloud service.\",\n",
       " \"SAN MATEO - Freshworks Inc. (NASDAQ:FRSH) announced Tuesday the appointment of Mika Yamamoto as its Chief Customer and Marketing Officer.\\nYamamoto's extensive experience spans across various leadership roles, including her most recent position as Executive Vice President at F5, where she spearheaded initiatives that cut across multiple domains and cultivated innovative team dynamics. Her career includes significant stints at Marketo, where she rose from Global President to Senior Vice President/General Manager after its acquisition by Adobe (NASDAQ:ADBE). She has also served as Chief Digital Marketing Officer and CMO of SMB at SAP, and held key positions at Amazon (NASDAQ:AMZN) Books, Microsoft (NASDAQ:MSFT) Windows, Gartner, and Accenture (NYSE:ACN).\\nDennis Woodside (OTC:WOPEY), President at Freshworks, commended Yamamoto's appointment, highlighting her ability to implement significant improvements in global teams and market strategies. CEO Girish Mathrubootham expressed confidence that Yamamoto's role will be pivotal in advancing Freshworks' mission to provide intuitive and AI-powered business software solutions.\\nThe company expects Yamamoto's leadership to play a critical role in the company's ongoing digital transformation and its quest to deliver exceptional value to its customers.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " 'Investing.com -- The Dow closed higher Tuesday, underpinned by a fall in Treasury yields as dovish remarks from some Federal Reserve officials boosted bets on rate cuts.\\nBy 16:00 ET (21:00 GMT), the Dow Jones Industrial Average\\xa0rose 83 points, or 0.2%, the S&P 500 was 0.1% higher and the NASDAQ Composite\\xa0rose 0.3%.\\nThe main Wall Street indices are on course to post strong gains in November - the DJIA set to finish the month 6.9% higher, the S&P 500 up 8.5% and the technology-heavy Nasdaq 10.8% higher.\\nTreasury yields slip as Fed speak boosts rate-cut hopes\\xa0\\xa0\\nFederal Reserve Board Governor Christopher Waller said he was\\xa0\"increasingly confident\" that policy is currently well positioned to slow the economy and get inflation back to 2% target.\\nAdding to clout to bets of a Fed rate cut early next year, Waller added that should data show disinflation continuing for several more months, the Fed \"could then start lowering the policy rate just because inflation\\'s lower.\"\\nTreasury yields fell on the remarks, with the\\xa0yield on United States 2-Year falling nearly 12 basis points to about 4.74%, while the yield on the United States 10-Year fell 6 basis points to 4.330%.\\nThe remarks come just ahead of the Fed’s preferred inflation gauge, the personal consumption expenditures price index, on Thursday, which is expected to have risen just 0.1% on the month in November, a drop from 0.4% in September.\\nRetailers in focus after Cyber Monday sales hit record\\nSentiment on retailers were boosted by ongoing signs that the consumer remains in good shape after  consumer confidence in November surprised to the upside, while Cyber Monday sales hit a record high.\\xa0\\xa0\\nConsumer spending on Cyber Monday, the biggest U.S. online shopping day,\\xa0is\\xa0expected to have surged to an all-time high of over $12 billion, according to preliminary estimates from Adobe (NASDAQ:ADBE) Digital Insights cited by Reuters.\\nWalmart (NYSE:WMT) and  Foot Locker  Inc (NYSE:FL) were among a slew of retailers in the ascendency, while payments platform Affirm (NASDAQ:AFRM) jumped nearly 12% as\\xa0record number of holiday shoppers likely used buy now, pay later services.\\xa0\\nMicron boosts outlook, Zscaler cuts losses\\nZscaler (NASDAQ:ZS) cut loses to close 1% higher after\\xa0the cybersecurity company lifted its full-year guidance after reporting quarterly results that topped Wall Street estimates.\\nThe firm also left its billings guidance unchanged, but Wedbush said the guidance represented \"a conservative and prudent bar in our view given the underlying strength that ZS is seeing in the field.\"\\xa0\\nMicron Technology Inc (NASDAQ:MU) raised its profit and revenue guidance for its fiscal first-quarter, but the stock fell nearly 2% as the chipmaker also lifted its forecast on annual expenses to about $990 million from $900 million previously.\\xa0\\nEnergy stocks as oil rides bets on deeper production cuts ahead of\\xa0OPEC+ meeting higher\\nEnergy stocks were one of the biggest gainers on the day as oil prices rose\\xa0on hopes that OPEC+ will agree to extend or even deepen its ongoing production cuts at a meeting later this week on Thursday.\\xa0\\nHess Corporation (NYSE:HES),  Chevron  Corp (NYSE:CVX) and EOG Resources Inc (NYSE:EOG) more among the biggest gainers.\\nAmazon takes fight to Microsoft with launch of new chip, AI bot\\nAmazon (NASDAQ:AMZN) cloud business, Amazon Web Services, unveiled a new chip for customers to develop artificial intelligence applications and tech giant also said it would offer Nvidia (NASDAQ:NVDA)\\'s latest chips. The move comes after Microsoft (NASDAQ:MSFT) recently announced its own AI chip and also said customers of Azure cloud platform would have access to Nvidia\\'s GPUs.\\xa0\\nAmazon also announced Amazon Q, a new chatGPT-style AI-powered chatbot, that will help customers solve\\xa0AWS-related queries.\\n(Peter Nurse and Oliver Gray contributed to this report.)',\n",
       " 'LONDON - The UK\\'s Competition and Markets Authority (CMA) is set to receive expanded authority under the Digital Markets Act, emphasizing economic growth and innovation. This move comes after the CMA faced criticism for its initial resistance to Microsoft (NASDAQ:MSFT)\\'s $69 billion acquisition of Activision Blizzard (NASDAQ:ATVI), which was eventually approved in October. Despite the controversy, which Microsoft President Brad Smith referred to as Britain\\'s \"darkest day\" in tech, the CMA has reaffirmed its commitment to maintaining market competitiveness and fairness.\\nThe UK government has issued a policy paper instructing the CMA to reduce regulatory burdens on businesses and focus on sectors that are critical for economic growth. This guidance follows scrutiny over the handling of the Microsoft-Activision deal, particularly concerns related to the cloud gaming market. As part of its annual plan, the CMA aims to generate economic benefits at least ten times greater than its operational costs.\\nOn Thursday, Kevin Hollinrake, a UK official, highlighted objectives that include support for investment and measures to alleviate the cost of living through advancements in the digital economy. In response to these directives, Sarah Cardell of the CMA stated on Friday that while the agency is refining its merger investigation process for greater efficiency, it will not waver in protecting UK consumers and businesses from deals that could harm them.\\nThe CMA\\'s renewed focus on fostering growth and innovation while safeguarding consumer interests reflects its evolving role post-Brexit. With increased responsibilities, the agency is adapting its approach to ensure that it continues to serve as a guardian against monopolistic practices without stifling economic development.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.',\n",
       " \"(Reuters) - Elon Musk-owned social media company X could lose as much as $75 million in advertising revenue by the end of the year as dozens of major brands pause their marketing campaigns, the New York Times reported on Friday. Musk backing an antisemitic post on the platform last week has led several companies including Walt  Disney  (NYSE:DIS) and Warner Bros Discovery (NASDAQ:WBD) to pause their advertisements on the site formerly called Twitter.  X has struck back and sued media watchdog group Media Matters, alleging the organization defamed the platform with a report that said ads for major brands including Apple (NASDAQ:AAPL) and Oracle (NYSE:ORCL) had appeared next to posts touting Adolf Hitler and the Nazi party.  Internal documents viewed by The New York Times this week list more than 200 ad units of companies from the likes of Airbnb, Amazon (NASDAQ:AMZN), Coca-Cola (NYSE:KO) and Microsoft (NASDAQ:MSFT), many of which have halted or are considering pausing their ads on the social network, the report said.  X said on Friday $11 million in revenue was at risk and the exact figure fluctuated as some advertisers returned to the platform and others increased spending, according to the report. The company did not immediately respond to a Reuters request for comment.  \\nAdvertisers have fled X since Musk bought it in October 2022 and reduced content moderation, resulting in a sharp rise in hate speech on the site, according to civil rights groups. The platform's U.S. ad revenue has declined at least 55% year-over-year each month since Musk's takeover, Reuters previously reported.\",\n",
       " \"Stocks such as Microsoft (NASDAQ:MSFT), Adobe (NASDAQ:ADBE), S&P Global (NYSE:SPGI), Blackstone (NYSE:BX), and  Prologis  (NYSE:PLD) have outperformed the MSCI USA index as the 10-year Treasury yield continues to drop. In the world of mergers and acquisitions, Vista Outdoor (NYSE:VSTO)'s market value saw a post-market increase following a merger proposal from Colt CZ Group, accompanied by a buyback strategy. In the sports betting sector, ahead of Thanksgiving, apps like Penn Entertainment's ESPN Bet topped download charts, with Flutter Entertainment's FanDuel and DraftKings (NASDAQ:DKNG) also seeing increased activity.\\nIn corporate movements, OpenAI announced a significant AI innovation named Q* alongside news of CEO Sam Altman stepping down. Meanwhile, Howard Hughes (NYSE:HHH) Holdings experienced a surge in its share price after Pershing Square Capital Management made a substantial $12.7 million purchase. Conversely, PayPal (NASDAQ:PYPL) Holdings' executive Jonathan Auerbach sold shares, continuing a trend of insider selling at the company over recent months.\\nIn resource sector news, Morien Resources Corp. has paused dividend payments due to regulatory issues impacting operations at its Donkin Mine, which in turn affects its revenue. On the regulatory front, tech giants Google (NASDAQ:GOOGL) and Meta (NASDAQ:META) are adjusting to new EU rules aimed at curbing online child exploitation while maintaining encryption standards. Amazon (NASDAQ:AMZN) is also navigating the regulatory landscape, set to receive unconditional EU approval for its acquisition of iRobot (NASDAQ:IRBT).\\nLastly, Safety Shot has revealed plans that will enable current shareholders to resell their stock.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " 'By Lewis Krauskopf, David Randall and Saqib Iqbal Ahmed NEW YORK (Reuters) - Signs the U.S. stock market rally is broadening from the so-called Magnificent Seven of mega-cap growth and technology companies is bolstering investor hopes for a rally through year-end.  Equities have risen sharply, with the S&P 500 up over 8% in November, on the cusp of a new high for 2023, fueled by falling Treasury yields and cooling inflation readings that could signal the end of Federal Reserve rate hikes. Yields fall when Treasury prices rise, and the lower returns on guaranteed fixed-income investments make stocks more appealing.  While some big investors are skeptical the rally amounts to more than just a year-end rebound, recent signs of market strength include gains in areas that have lagged this year.  In one encouraging sign, about 55% of the S&P 500 were trading above their 200-day moving averages as of Monday. That level breached 50% last week for the first time in nearly two months, according to LPL Financial (NASDAQ:LPLA).  \"Breadth is finally starting to broaden out to levels more commensurate with bull markets,\" said Adam Turnquist, chief technical strategist at LPL Financial. \"This has been one of the keys to calling this recovery sustainable.\" Among other signs, the equal-weight S&P 500 -- a proxy for the average stock in the index -- rose 3.24% last week. That was substantially more than the 2.24% rise for the market-cap weighted S&P 500, the biggest percentage point outperformance for the equal-weight index in nearly five months. Even so, the S&P 500 equal-weight index has gained just 3% in 2023 against an 18% rise for the overall S&P 500 -- on pace for the biggest such annual percentage-point gap in 25 years. Much of that underperformance is due to the outsized gain in the Magnificent Seven stocks, which collectively hold a 28% weight in the S&P 500 index: Apple (NASDAQ:AAPL), Microsoft (NASDAQ:MSFT), Alphabet (NASDAQ:GOOGL), Amazon (NASDAQ:AMZN), Nvdia, Meta (NASDAQ:META) Plaforms and Tesla (NASDAQ:TSLA). Overall, the group of stocks makes up nearly 50% of the weighting of the Nasdaq 100, which is up nearly 47% for the year to date. Struggling small-cap and bank stocks have perked up, especially after last week\\'s U.S. consumer price data for October was unchanged from the prior month. The small-cap Russell 2000 is up 5.5% since the CPI data with the S&P 500 banks index up 6.5%, versus a 3% rise for the S&P 500. Year-to-date, the Russell 2000 is up 2%, while the S&P 500 banks index has fallen over 6%. Mona Mahajan, senior investment strategist at Edward Jones, said an environment that could be conducive for a broadening of the rally \"is starting to take shape.\" “This environment where rates are cooling, inflation is moderating and the Fed is on the sidelines, that is typically a good backdrop for risk assets,” Mahajan said. “Typically when rates start to move lower, you get valuation expansion and the areas that we could see some more meaningful valuation expansion is outside of large-cap tech,” she said.  The equal-weight S&P 500 is trading at a 5% discount to its 10-year average forward price-to-earnings ratio, according to Edward Jones.  Still, there are reasons to think that the market rally is not on the verge of a sustained broadening.  Investors will get further readings of consumer confidence and inflation next week. Stronger than expected data could spur a selloff in Treasuries, sending yields higher. At the same time, the sharp rally in stocks for the week ended Nov. 17 was accompanied by high demand for upside call options, particularly in parts of the market that have underperformed this year, such as the small-caps focused iShares Russell 2000 ETF. Some of that has already started to unwind. \"We saw a huge pickup in expectations for IWM, but now those seem to have stabilized,\" said Steve Sosnick, chief strategist at Interactive Brokers (NASDAQ:IBKR). \\nThe recent surge, which has pushed the broad S&P 500 up approximately 10% over the last three weeks, may not last as investors prepare to close their books for the year, said Jason Draho, head of asset allocation Americas at UBS Global Wealth Management. \"A lot of good news is already priced in and investors may be reluctant to chase the rally,\" he said.',\n",
       " 'OpenAI and Microsoft (NASDAQ:MSFT) are facing a lawsuit alleging the improper use of nonfiction authors’ work to train AI models, including OpenAI’s ChatGPT service. Author and reporter Julian Sancton sued the companies on Nov. 21. \\nAccording to Sancton’s complaint, OpenAI allegedly utilized tens of thousands of nonfiction books without authorization to train its extensive language models in responding to human text prompts. The author and Hollywood Reporter editor is leading a proposed class-action lawsuit filed in a New York federal court.Screenshot of the lawsuit. Source: CourtListenerContinue Reading on Cointelegraph',\n",
       " 'Investing.com – U.S. stocks were mixed after the close on Friday, as gains in the Healthcare, Oil & Gas and Financials sectors led shares higher while losses in the Technology, Telecoms and Consumer Services sectors led shares lower.\\nAt the close in NYSE, the Dow Jones Industrial Average rose 0.33% to hit a new 3-months high, while the S&P 500 index added 0.06%, and the NASDAQ Composite index declined 0.11%.\\nThe best performers of the session on the Dow Jones Industrial Average were Johnson & Johnson (NYSE:JNJ), which rose 1.11% or 1.68 points to trade at 152.50 at the close. Meanwhile, Walt  Disney  Company (NYSE:DIS) added 1.04% or 0.99 points to end at 96.06 and Walmart Inc (NYSE:WMT) was up 0.90% or 1.39 points to 156.06 in late trade.\\nThe worst performers of the session were Apple Inc (NASDAQ:AAPL), which fell 0.70% or 1.34 points to trade at 189.97 at the close.  Nike  Inc (NYSE:NKE) declined 0.26% or 0.28 points to end at 107.64 and Microsoft Corporation (NASDAQ:MSFT) was down 0.11% or 0.42 points to 377.43.\\nThe top performers on the S&P 500 were CF Industries Holdings Inc (NYSE:CF) which rose 2.55% to 78.36,  Best Buy  Co Inc (NYSE:BBY) which was up 2.18% to settle at 69.51 and  WestRock Co  (NYSE:WRK) which gained 2.14% to close at 37.70.\\nThe worst performers were  First Solar Inc  (NASDAQ:FSLR) which was down 3.34% to 154.38 in late trade, NVIDIA Corporation (NASDAQ:NVDA) which lost 1.93% to settle at 477.76 and  Bunge  Limited (NYSE:BG) which was down 1.42% to 107.99 at the close.\\nThe top performers on the NASDAQ Composite were  BioNexus Gene Lab  Corp (NASDAQ:BGLC) which rose 143.90% to 0.90, FLJ Group Ltd (NASDAQ:FLJ) which was up 45.24% to settle at 0.16 and Captivision Inc (NASDAQ:CAPT) which gained 44.44% to close at 2.34.\\nThe worst performers were Seelos Therapeutics Inc (NASDAQ:SEEL) which was down 22.95% to 0.13 in late trade,  LQR House  Inc (NASDAQ:LQR) which lost 22.14% to settle at 0.03 and  Tarena Intl Adr  (NASDAQ:TEDU) which was down 21.43% to 1.32 at the close.\\nRising stocks outnumbered declining ones on the New York Stock Exchange by 2087 to 716 and 100 ended unchanged; on the Nasdaq Stock Exchange, 2453 rose and 957 declined, while 117 ended unchanged.\\nShares in LQR House Inc (NASDAQ:LQR) fell to all time lows; down 22.14% or 0.01 to 0.03. Shares in Tarena Intl Adr (NASDAQ:TEDU) fell to 52-week lows; down 21.43% or 0.36 to 1.32. \\nThe CBOE Volatility Index, which measures the implied volatility of S&P 500 options, was down 2.66% to 12.46 a new 3-years low.\\nGold Futures for December delivery was up 0.54% or 10.80 to $2,003.60 a troy ounce. Elsewhere in commodities trading, Crude oil for delivery in January fell 2.50% or 1.93 to hit $75.17 a barrel, while the January Brent oil contract fell 1.46% or 1.19 to trade at $80.23 a barrel.\\nEUR/USD was unchanged 0.36% to 1.09, while USD/JPY fell 0.06% to 149.46.\\nThe US Dollar Index Futures was down 0.52% at 103.29.',\n",
       " \"As  Microsoft Corp . (NASDAQ:MSFT) navigates a pivotal moment with OpenAI, the tech giant is considering strategic options to bolster its artificial intelligence endeavors following Sam Altman's departure from OpenAI to join Microsoft. This move comes as the company aims to enhance its AI capabilities and maintain the growth of its ChatGPT subscriber base.\\nOn Tuesday, speculation swirled regarding who might replace Altman at OpenAI, with Dario Amodei, CEO of Anthropic, being a rumored candidate. However, these rumors were quashed when Amodei dismissed the idea of either stepping in as CEO or merging with OpenAI. Amid this leadership transition, investor unrest has surfaced, potentially leading to legal challenges against OpenAI's board.\\nDespite these developments and a slight dip in Microsoft's stock value earlier today, the company's shares remained stable after achieving a record high on Monday. Satya Nadella, Microsoft's CEO, described Altman's move as a necessary step for governance reform at OpenAI and reassured that it does not jeopardize their long-term objectives. Nadella also suggested the possibility for OpenAI staff to join Microsoft's substantial AI division.\\nAnalysts from Morgan Stanley highlighted Microsoft's delicate position with OpenAI, emphasizing the need to resolve executive conflicts or possibly integrate OpenAI's Research Lab into Microsoft. The focus remains on advancing Artificial General Intelligence research and securing the expansion of the ChatGPT subscriber base. Microsoft's development efforts include Azure OpenAI Services and products utilizing GPT technology.\\nWall Street analysts have maintained a Strong Buy consensus on Microsoft stock after a significant rally, with projections indicating an average price target suggesting over 10% growth potential.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " 'Changpeng Zhao is expected to step down as CEO of Binance as part of a plea deal with the United States Department of Justice. Binance transferred $3.9 billion in USDT (USDT) between its wallets days before a report about its $4 billion settlement with the DOJ was published. Meanwhile, the U.S. Securities and Exchange Commission has sued Kraken, alleging it violated several securities laws, and Sam Altman and Greg Brockman will join Microsoft (NASDAQ:MSFT) after the former’s ousting from OpenAI.CZ to reportedly plead guiltyChangpeng Zhao, also known as CZ, is expected to plead guilty to violating U.S. Anti-Money Laundering requirements, according to various reports and court documents unsealed on Nov. 21.Binance’s $3.9 billion USDT move gains community attention amid DOJ settlement claimsSEC sues Kraken, alleging slate of securities law violationsContinue Reading on Cointelegraph',\n",
       " 'Just two days after Microsoft (NASDAQ:MSFT) CEO Satya Nadella hired former OpenAI CEO Sam Altman to lead a new advanced AI research team, OpenAI backtracked on its decision to let go t\\nOn Nov. 22, OpenAI announced a new agreement that would see the return of Altman as the company CEO. The company confirmed this decision through an official tweet.\\nContinue Reading on Cointelegraph',\n",
       " \"Quiver Quantitative - In a stunning reversal, Sam Altman reclaims his position as CEO of OpenAI, signaling a significant turn in the tumultuous events that have gripped the artificial intelligence community. His reinstatement, following intense negotiations and considerable investor pressure, underscores Microsoft (NASDAQ:MSFT) strategic victory and influence in the ongoing narrative of AI's commercialization and ethical development.\\nOpenAI, in a bold move, has restructured its board, introducing notable figures like Bret Taylor and Larry Summers, alongside Quora's Adam D’Angelo. Their appointment is seen as a strategic alignment with investor interests and Silicon Valley's elite, providing a stabilizing force after the whirlwind of leadership changes. Microsoft's stock, reacting to the news, soared to unprecedented levels, reflecting investor optimism in the wake of the organization's recalibration.\\nThe reinstallation of Altman as CEO comes with the condition of an internal investigation into the circumstances leading to his initial dismissal. This development is a clear compromise designed to placate concerns around governance and operational transparency. As the dust settles, the focus shifts to the composition of the new board, with expectations of Microsoft's representation, indicating a closer intertwining of OpenAI's pioneering technology with Microsoft's expansive AI strategy.\\nAltman's return may quell investor unrest and mitigate the risk of talent attrition, yet it raises critical questions about the future trajectory of OpenAI. With the company at the intersection of rapid AI development and the need for substantial capital, the saga illuminates the complex dance between innovation, responsibility, and investment in the high-stakes world of technology startups.\\nThis article was originally published on Quiver Quantitative\",\n",
       " 'By Anna Tong, Jeffrey Dastin and Krystal Hu (Reuters) -Ahead of OpenAI CEO Sam Altman’s four days in exile, several staff researchers wrote a letter to the board of directors warning of a powerful artificial intelligence discovery that they said could threaten humanity, two people familiar with the matter told Reuters. The previously unreported letter and AI algorithm were key developments before the board\\'s ouster of Altman, the poster child of generative AI, the two sources said. Prior to his triumphant return late Tuesday, more than 700 employees had threatened to quit and join backer Microsoft (NASDAQ:MSFT) in solidarity with their fired leader. The sources cited the letter as one factor among a longer list of grievances by the board leading to Altman\\'s firing, among which were concerns over commercializing advances before understanding the consequences. Reuters was unable to review a copy of the letter. The staff who wrote the letter did not respond to requests for comment.  After being contacted by Reuters, OpenAI, which declined to comment, acknowledged in an internal message to staffers a project called Q* and a letter to the board before the weekend\\'s events, one of the people said. An OpenAI spokesperson said that the message, sent by long-time executive Mira Murati, alerted staff to certain media stories without commenting on their accuracy. Some at OpenAI believe Q* (pronounced Q-Star) could be a breakthrough in the startup\\'s search for what\\'s known as artificial general intelligence (AGI), one of the people told Reuters. OpenAI defines AGI as autonomous systems that surpass humans in most economically valuable tasks. Given vast computing resources, the new model was able to solve certain mathematical problems, the person said on condition of anonymity because the individual was not authorized to speak on behalf of the company. Though only performing math on the level of grade-school students, acing such tests made researchers very optimistic about Q*’s future success, the source said. Reuters could not independently verify the capabilities of Q* claimed by the researchers.\\xa0 \\'VEIL OF IGNORANCE\\' Researchers consider math to be a frontier of generative AI development. Currently, generative AI is good at writing and language translation by statistically predicting the next word, and answers to the same question can vary widely. But conquering the ability to do math — where there is only one right answer — implies AI would have greater reasoning capabilities resembling human intelligence. This could be applied to novel scientific research, for instance, AI researchers believe. Unlike a calculator that can solve a limited number of operations, AGI can generalize, learn and comprehend. In their letter to the board, researchers flagged AI’s prowess and potential danger, the sources said without specifying the exact safety concerns noted in the letter. There has long been discussion among computer scientists about the danger posed by highly intelligent machines, for instance if they might decide that the destruction of humanity was in their interest. Researchers have also flagged work by an \"AI scientist\" team, the existence of which multiple sources confirmed. The group, formed by combining earlier \"Code Gen\" and \"Math Gen\" teams, was exploring how to optimize existing AI models to improve their reasoning and eventually perform scientific work, one of the people said. Altman led efforts to make ChatGPT one of the fastest growing software applications in history and drew investment - and computing resources - necessary from Microsoft to get closer to AGI. In addition to announcing a slew of new tools in a demonstration this month, Altman last week teased at a summit of world leaders in San Francisco that he believed major advances were in sight. \\n\"Four times now in the history of OpenAI, the most recent time was just in the last couple weeks, I\\'ve gotten to be in the room, when we sort of push the veil of ignorance back and the frontier of discovery forward, and getting to do that is the professional honor of a lifetime,\" he said at the Asia-Pacific Economic Cooperation summit. A day later, the board fired Altman.\\xa0 (Anna Tong and Jeffrey Dastin in San Francisco and Krystal Hu in New York; Editing by Kenneth Li and Lisa Shumaker)',\n",
       " \"Quiver Quantitative - Summary\\n-Sam Altman has been reinstated as CEO of OpenAI.\\n-The decision comes after intense pressure from investors, employees, and Microsoft (NASDAQ:MSFT).\\n-Altman was ousted by the company's board of directors last week.\\n-A new board of directors will be formed, including Bret Taylor, Larry Summers, and Adam D'Angelo.\\nIn a narrative worthy of a Silicon Valley thriller, Sam Altman has reclaimed his position as CEO of OpenAI, barely a week after his startling dismissal—a coup that underscores the volatile nature of tech startups and the intricate dance between innovation and corporate governance.\\nAltman's ousting became a rallying cry that united OpenAI employees, investors, and tech behemoth Microsoft (MSFT), culminating in a dramatic reversal that reinstates Altman at the helm. This boardroom skirmish was not just about leadership—it was a stark depiction of the high stakes in the AI industry, where the quest for cutting-edge technology can clash with the principles of ethical governance.\\nCrafting a New Vision: OpenAI's Board Reworked\\nThe restoration of Altman heralds the formation of a new, potentially formidable board. Bret Taylor, known for his strategic agility as Salesforce (NYSE:CRM)'s former co-CEO, takes the chair, while the inclusion of Larry Summers and Adam D’Angelo infuses a blend of economic acumen and tech expertise. Their task? To steer OpenAI through uncharted territories as it seeks to fulfill its colossal promise without compromising its foundational ethos.\\nMicrosoft's Gambit: Ensuring Altman's Leadership\\nMicrosoft's pivotal role in Altman's return speaks volumes about the strategic interdependence between the two companies. By throwing its considerable influence behind Altman, Microsoft has made a calculated move to safeguard its interests in OpenAI's groundbreaking AI endeavors. This alliance is a testament to Microsoft's foresight in recognizing that Altman's vision aligns with its own aspirations for the AI landscape.\\nLooking Ahead: OpenAI at a Crossroads\\nWhile Altman's reinstatement might be a cause for celebration, it leaves the tech community pondering the lessons learned from this high-profile power struggle. OpenAI, a paragon of AI innovation, now faces the task of reconciling its ambitious technological pursuits with the imperative of prudent, transparent governance.\\nAs the dust settles on this corporate saga, the eyes of the world remain fixed on OpenAI, a company at the crossroads of a technological revolution. The decisions made in the coming days will not only shape the future of OpenAI but could also set precedents for the broader realm of AI development and management.\\nThis article was originally published on Quiver Quantitative\",\n",
       " \"Global markets have seen a flurry of activity this week, with Asian stocks reaching a two-month peak and US indices climbing ahead of the Thanksgiving holiday. Investors are closely monitoring various economic indicators and corporate earnings reports as they speculate on future monetary policy shifts.\\nOn today, MSCI's Asia-Pacific index soared to a two-month high of 509.82, reflecting a robust monthly surge of 7%. Japan's Nikkei index outshone its regional counterparts with an impressive yearly gain of approximately 28%, despite experiencing a slight dip. Meanwhile, the US dollar's strength waned as market participants pondered over the Federal Reserve's potential pivot from raising interest rates to cutting them in the upcoming year.\\nWall Street also witnessed gains earlier in the week, with the Nasdaq benefiting from Microsoft (NASDAQ:MSFT)'s recruitment of Sam Altman, OpenAI's former CEO. This move contributed to a positive sentiment on Monday, lifting major indexes.\\nAs anticipation builds for Nvidia (NASDAQ:NVDA)'s financial results and the Federal Reserve's meeting minutes, investors remain hopeful yet vigilant. November has seen markets rebound due to diminishing concerns over US inflation.\\nIn other market news, on Wednesday, U.S indices edged higher before the Thanksgiving break. The S&P 500 rose by 0.4%, the Dow Jones Industrial Average by 0.5%, and the Nasdaq saw a modest increase, supported by tech giants Microsoft and Alphabet (NASDAQ:GOOGL). However, Broadcom (NASDAQ:AVGO)'s shares fell following its announcement of the VMWare deal.\\nThe energy sector faced headwinds as oil prices dropped to $76.43 per barrel after OPEC postponed discussions on production cuts. Nvidia’s stock declined despite reporting strong earnings, affected by export constraints to China.\\nRetailers Nordstrom (NYSE:JWN) and Guess (NYSE:GES) experienced share price drops after issuing profit warnings. Despite these setbacks, consumer confidence remained buoyant, and lower Thanksgiving meal costs suggested that inflation pressures might be easing.\\nCurrency markets saw the dollar weaken against the yen but strengthen against the euro as traders await October's pivotal inflation data for clues on the economic trajectory.\\nWith the US Thanksgiving holiday today, trading volumes are expected to be lower throughout the week due to a limited data calendar available to investors. Meanwhile, in Asia today, markets closed mixed with Hong Kong’s Hang Seng index gaining from regulatory support for real estate firms like Country Garden and Sino-Ocean Group Holding. Conversely, Australia’s index experienced a modest decline.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " None,\n",
       " \"The semiconductor leader Nvidia (NASDAQ:NVDA) has reported a remarkable performance in its third-quarter earnings, with revenues reaching $18.1 billion, a significant increase from the previous year, and profits soaring to $9.2 billion. This growth defies earlier forecasts by nearly $2 billion and marks a near-fourteenfold increase in profits year-over-year.\\nNvidia's financial success comes amid a challenging landscape marked by competitive pressures, U.S. export restrictions affecting key markets such as China, Saudi Arabia, and Vietnam, and the need for innovation in chip designs to remain compliant. Despite these hurdles, Nvidia's partnership with Microsoft (NASDAQ:MSFT) has continued to strengthen. The collaboration has boosted cloud offerings with Nvidia’s GPUs and co-developed supercomputing capabilities that now rank third globally.\\nThe company's robust performance is partly attributed to increased production of the high-demand H100 chip, which is expected to drive Q4 sales towards an estimated $20 billion. This projection suggests sustained growth and optimism for Nvidia's near-term future.\\nIn related industry news, the termination of Sam Altman from OpenAI and his subsequent hiring by Microsoft could signal potential shifts in the generative AI market. Insider Intelligence analysts hinted at possible volatility in this sector following these developments.\\nCountries like India, Sweden, and France are also contributing to the evolving landscape by establishing specialized A.I.-focused data centers independent of U.S. computing resources, highlighting a global trend towards diversification in technology infrastructure.\\nNvidia's success story unfolds as it navigates through regulatory challenges and capitalizes on strategic partnerships and product innovation, positioning itself at the forefront of the semiconductor industry.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " 'By Jody Godoy (Reuters) -Few people can force OpenAI to change governance at the crisis-stricken artificial-intelligence company, and the head of Microsoft (NASDAQ:MSFT), a major financial backer, is not one of them, according to legal experts.  The nonprofit board overseeing the maker of the popular ChatGPT chatbot sent shock waves through Silicon Valley on Friday by abruptly firing Chief Executive Sam Altman. Nearly all of the company\\'s 700 employees signed a letter threatening to resign if the board does not step down, and Microsoft CEO Satya Nadella has called for governance changes. Microsoft did not immediately return a request for comment made after business hours on Tuesday. The turmoil has left investors weighing their legal options and illustrated a divide over how the potentially disruptive technology can be developed safely. Because it is a nonprofit, the only people who could force the current board of OpenAI to step down or change are judges or state attorneys general, said Alexander Reid, an attorney at BakerHostetler who counsels nonprofit organizations. Attorneys general oversee and investigate nonprofits, and have wide latitude to seek reforms. \"Even if they don\\'t go to court, their mere presence typically gets results,\" he said. Attorneys general can enact everything from leadership changes to complete shutdown of an organization, usually after finding fraud or illegal conflicts of interest.  Hershey Co  (NYSE:HSY) is one example. The trust that controls the candymaker agreed in 2016 to replace certain board members after the Pennsylvania attorney general challenged the trust\\'s spending. Darryll Jones, a law professor at Florida A&M University, said the U.S. Internal Revenue Service is another source of accountability. \"There is a whole boatload of scholarship noting that nonprofit enforcement is severely lacking, but for the most part nonprofits are pretty good at self-policing if only to avoid scandal that would impact donations,\" he said. OpenAI\\'s for-profit arm was under the full control of a nonprofit, an arrangement meant to insulate decisions about a potentially powerful technology from being driven by corporate greed. Because of that, investors who have collectively plowed billions of dollars into the startup face hurdles to suing the board over Altman\\'s firing, though sources have told Reuters some are considering legal action. Under OpenAI\\'s bylaws, only directors can remove or elect new board members. The arrangement, known as a self-perpetuating board, is very common in the nonprofit world, said Reid. There are currently four people on the board: three independent directors, and OpenAI chief scientist Ilya Sutskever. The latter worked with the other board members to remove Altman and former President Greg Brockman, but has since said he \"deeply regret(s)\" the action. Outside of government enforcers, Sutskever may now be the only person in a position to formally challenge the board\\'s decision. Board members can sue other board members, either directly or on behalf of the organization, for failing to exercise their duties, said Reid. But typically such court battles are fought only when there is suspected malfeasance connected with spending or compensation, he said. In fights over organizational direction or control, the more common play is for the organization to split.  \"You just form another nonprofit that does it slightly differently,\" he said. OpenAI has already survived one such break. \\nThe cofounders of Anthropic, who were also executives at OpenAI until 2020, had broken from their employer over disagreements regarding how to ensure AI\\'s safe development and governance. Whether OpenAI survives the rift between its board and employees will likely be determined in the next few days.',\n",
       " 'By Aditya Soni (Reuters) - Sam Altman\\'s return as OpenAI\\'s chief executive will strengthen his grip on the startup and may leave the ChatGPT creator with fewer checks on his power as the company introduces technology that could upend industries, corporate governance experts and analysts said. OpenAI is bringing Altman back just days after his ouster as well as installing a revamped board that could bring sharper scrutiny to the startup at the heart of the AI boom, but strong support from investors including Microsoft (NASDAQ:MSFT) may give Altman more leeway to commercialize the technology.  \"Sam\\'s return may put an end to the turmoil on the surface, but there may continue to be deep governance issues,\" said Mak Yuen Teen, director of the centre for investor protection at the National University of Singapore Business School. \"Altman seems awfully powerful and it is unclear that any board would be able to oversee him. The danger is the board becomes a rubber stamp,\" he said. OpenAI\\'s new board will boast more experience at the top level and strong ties to both the U.S. government and Wall Street.  The board fired Altman last week with little explanation and attempted to move on by naming an interim CEO twice. However, pressure from Microsoft - and the 38-year-old\\'s strong loyalty among the 700-plus OpenAI employees that caused nearly all of them to threaten to leave the company - led to Altman\\'s reinstatement as of Wednesday.  \"Altman has been invigorated by the last few days,\" GlobalData analyst Beatriz Valle said. But that could come at a cost, she said, adding that he has \"too much power now.\"  Bret Taylor, former co-CEO of Salesforce (NYSE:CRM) who also played a key role in forcing through Elon Musk\\'s $44 billion purchase of Twitter as a director, will be chairing the board. Other members include former U.S. Treasury Secretary Larry Summers, a Harvard academic and longtime economic aide to Democratic presidents. \"The fact that Summers and Taylor will join OpenAI is quite extraordinary and marks a dramatic reversal of fortunes in the company,\" Valle said. Summers, who also sits on the board of Jack Dorsey\\'s fintech firm Block, has in recent months been vocal about the potential job losses and disruption that could be caused by AI. \"ChatGPT is coming for the cognitive class. It\\'s going to replace what doctors do,\" he said in a post on X in April.  OpenAI\\'s previous board consisted of entrepreneur Tasha McCauley, Helen Toner, director of strategy at Georgetown\\'s Center for Security and Emerging Technology, OpenAI chief scientist Ilya Sutskever, as well as Quora CEO Adam D\\'Angelo, who also sits on the new board. It was not immediately clear if any of the other directors would remain, including Sutskever, who joined in the effort to fire Altman then signed onto an employee letter demanding his return, expressing regret for her \"participation in the board\\'s actions.\"  OpenAI on X said it was \"collaborating to figure out the details\" of the new board. Microsoft declined to comment. Summers and OpenAI did not immediately respond to requests for comment. Sutskever, Altman and Taylor could not be immediately reached for comment. Some analysts say the management fiasco will ensure that OpenAI executives proceed cautiously, as the high-flying startup will now be subject to more scrutiny. Several noted that companies such as Facebook (NASDAQ:META) parent Meta have flourished with a powerful CEO despite concerns about corporate governance. \\n\"Sam definitely comes out stronger but also dirtied and will have more of a microscope from the AI and broader tech and business community,\" Gartner analyst Jason Wong said. \"He can no longer do no wrong.\"  (This story has been refiled to capitalize a letter in GlobalData in paragraph 7)',\n",
       " \"Quiver Quantitative - Broadcom (NASDAQ:AVGO) has successfully concluded its ambitious $69 billion acquisition of VMware (NYSE:VMW), a landmark move that significantly bolsters its position in the software business. This acquisition, among the largest globally since its announcement in May 2022, marks a pivotal moment in Broadcom CEO Hock Tan's strategic push into the software domain. However, the path to this momentous closure was anything but smooth, with the deal encountering rigorous regulatory examinations across various jurisdictions and experiencing multiple delays.\\nThe final nod from China, a critical market, came through on Tuesday, offering relief amid heightened U.S.-China tensions, particularly concerning stringent chip export controls. The recent easing of diplomatic strains, following a meeting between President Xi Jinping and President Joe Biden, played a crucial role in assuaging investor concerns and ensuring the deal's completion before the looming November 26 deadline. European and UK regulatory bodies, including the European Commission and the UK's Competition and Markets Authority (CMA), also greenlit the acquisition, with Broadcom agreeing to certain concessions to maintain market competition, particularly benefiting rival Marvell Technology (NASDAQ:MRVL).\\nThis Broadcom-VMware deal sets a significant precedent in the tech industry, especially in an era where big tech mergers are under the microscope of regulatory scrutiny. The successful closure of Microsoft (NASDAQ:MSFT) $69 billion acquisition of Activision Blizzard (NASDAQ:ATVI), despite facing intense review from the U.S. Federal Trade Commission under Chair Lina Khan, underscores a potentially more accommodating environment for such large-scale tech mergers. This could signal a shift in the landscape, encouraging other companies to pursue similar ambitious consolidations.\\nThe Broadcom-VMware deal's closure represents not just a triumph for the companies involved but also a pivotal moment for the tech industry at large. It demonstrates the possibility of navigating the complex web of global regulatory challenges, setting a precedent for future tech mergers and acquisitions.\\nThis article was originally published on Quiver Quantitative\",\n",
       " \"Autodesk, Inc. (NASDAQ:ADSK) reported robust financial performance in the third quarter of fiscal 2024, with total revenue growing 10% and 13% in constant currency. The company also unveiled a new transaction model, Flex (NASDAQ:FLEX), aimed at strengthening its direct relationship with customers and channel partners. Autodesk also reaffirmed its commitment to developing next-generation technologies, including AI and data services, to drive digital transformation for its customers. \\nKey takeaways from the call include:Autodesk's revenue growth was driven by its subscription business model and product diversification across different regions and industries.The company plans to transition its indirect business to the new Flex model globally by fiscal years 2025 and 2026.Autodesk provided guidance for fiscal year 2024, expecting revenue between $5.45 billion and $5.47 billion, and non-GAAP operating margins similar to fiscal year 2023 levels.The company raised its guidance for non-GAAP earnings per share and free cash flow, forecasting free cash flow between $1.2 billion and $1.26 billion.For fiscal 2025, Autodesk anticipates revenue growth of about 9% or more, with a transition to annual billings and a reduction in free cash flow compared to fiscal 2024.Autodesk highlighted partnerships with companies such as WSP, TCE, LFD, and various educational institutions, showcasing the growth and adoption of their solutions in different industries.During the earnings call, Autodesk's CEO, Andrew Anagnost, highlighted the significance of the company's new transaction model and its aim to move all customers to cloud-based life cycle solutions powered by AI. CFO Debbie Clifford noted that the transition to annual billings is expected to be a three-year journey, with a mechanical rebuild of free cash flow in fiscal years 2025 and 2026.\\nAnagnost also emphasized the company's commitment to ethical and high-trust use of customer data in its AI and automation approach. Despite challenges in some sectors, Autodesk's construction business has seen strong growth, particularly in manufacturing, industrial, data centers, healthcare, and infrastructure.\\nThe company also discussed its partnerships and the trust built with partners to grow their businesses over time. Autodesk anticipates revenue growth acceleration with the transition to the Flex model, informed by learnings from the Australia rollout. The company also mentioned pricing adjustments for Fusion 360 to increase the base offering value and drive extension adoption. Autodesk affirmed its long-term goals and commitment to making smart decisions for the business and shareholders.InvestingPro InsightsAutodesk's recent earnings call showcased a promising trajectory for the company, and real-time data from InvestingPro further enriches this outlook. With a market capitalization of $46.53 billion and a robust gross profit margin of 91.42% over the last twelve months as of Q2 2024, Autodesk's financial health appears strong. The company's revenue growth of 9.87% during the same period signals a steady expansion, aligning with the company's guidance for fiscal year 2024.\\nInvestingPro Tips highlight that Autodesk yields a high return on invested capital and operates with an impressive gross profit margin, which may be indicative of the company's effective use of resources and strong market position. Additionally, the company's shares are trading at a P/E ratio of 53.5, which suggests investor confidence in Autodesk's future earnings potential, particularly when considering the PEG ratio of 0.9, indicating that the stock may be undervalued relative to its earnings growth.\\nWith the special Black Friday sale, a subscription to InvestingPro is now even more enticing, offering up to a 55% discount. For those interested in a deeper dive, InvestingPro provides over 20 additional tips for Autodesk, offering a comprehensive analysis for investors considering this stock.Full transcript -  Autodesk Inc  (NASDAQ:ADSK) Q3 2024:Operator: Thank you for standing by, and welcome to Autodesk's Third Quarter Fiscal Year 2024 Earnings Conference Call. [Operator Instructions] I would now like to hand the call over to VP of Investor Relations, Simon Mays-Smith. Please go ahead.\\nSimon Mays-Smith: Thanks, operator, and good afternoon. Thank you for joining our conference call to discuss the third quarter results of Autodesk's fiscal 2024. On the line with me are Andrew Anagnost, our CEO; and Debbie Clifford, our CFO. Today's conference call is being broadcast live via webcast. In addition, a replay of the call will be available at autodesk.com/investor. Following this call, you can find the earnings press release, slide presentation and transcript of today's opening commentary on our Investor Relations website. During this call, we may make forward-looking statements about our outlook, future results and related assumptions, products and product capabilities, business models and strategies. These statements reflect our best judgment based on currently known factors. Actual events or results could differ materially. Please refer to our SEC filings, including our most recent Form 10-Q and the Form 8-K filed with today's press release for important risks and other factors that may cause our actual results to differ from those in our forward-looking statements. Forward-looking statements made during the call are being made as of today. If this call is replayed or reviewed after today, the information presented during the call may not contain current or accurate information. Autodesk disclaims any obligation to update or revise any forward-looking statements. We will quote several numeric or growth changes during this call as we discuss our financial performance. Unless otherwise noted, each such reference represents a year-on-year comparison. All non-GAAP numbers referenced in today's call are reconciled in our press release or Excel Financials and other supplemental materials are available on our Investor Relations website. And now, I will turn the call over to Andrew.\\nAndrew Anagnost: Thank you, Simon, and welcome, everyone, to the call. Resilience, discipline and opportunity again underpinned Autodesk strong financial and competitive performance despite continued macroeconomic policy and geopolitical headwinds. Renewal rates have remained strong and new business trends have been largely consistent for many quarters. Our subscription business model and our product and customer diversification enable that. It means that accelerating growth in Canada has balanced decelerating growth in the United Kingdom. The growing momentum in construction has balanced deteriorating momentum in media and entertainment. And that strength from our enterprise and smaller customers has balanced softness from medium-sized customers. Our leading indicators remain consistent with last quarter with growing usage, record bid activity on building connected and cautious optimism from channel partners. Disciplined and focused execution and strategic deployment of capital through the economic cycle, enables Autodesk to realize the significant benefits of its strategy while mitigating the risk of having to make expensive catch-up investments later. As Steve said at Investor Day, we introduced a new transaction model for Flex, which gives Autodesk a more direct relationship with its customers and more closely integrates with its channel partners. We began testing the new transaction model across our product suite in Australia a couple of weeks ago. Assuming the launch proceeds is expected in fiscal '25 and '26, we intend to transition our indirect business to the new transaction model in all our major markets globally. In the new transaction model, partners provide a quote to customers, but the actual transaction happens directly between Autodesk and the customer. The new transaction model is an important step on our path to integrate more closely with our customers' workflows enabled by, among other things, Autodesk platform services and our industry cloud's fusion, forma and flow. Autodesk, its customers and partners will be able to build more valuable, data-driven and connected products and services in our industry cloud and on our platform. The new transaction model is consequential. Many of you will have seen other companies adopting agency models and will already understand the math. In the near term, the new transaction model results in a shift from contra revenue to operating costs that provide a tailwind to revenue growth, while being broadly neutral to operating profit and free cash flow dollars and mechanically result in percent operating margins taking a step or two backwards. Over the long term, optimization enabled by this transition will provide a tailwind to revenue, operating income and free cash flow dollars even after the cost of setting up our building platform. Finally, there is opportunity from developing next-generation technologies and services that deliver end-to-end digital transformation of our design and make customers and enable a better world designed and made for all. I was at Autodesk University last week, alongside more than 10,000 attendees, where we announced Autodesk AI, technology we've been working on and investing in for years. We showed how our design to make platform will automate noncreative work, help customers analyze their data and surface insights and augment their work to make them more agile and creative, but there is no AI without actionable data. And that's why we're also investing in Autodesk platform services, which are accessible, extensible and open via our APIs. Autodesk Platform Services offers several critical capabilities, but data services are the most impactful. These provide the tools that make data actionable. And at the core of our data services is the Autodesk data model. Think of the Autodesk data model is the knowledge graph that gives customers access to the design, make and project data in granular bite-sized chunks. The data chunks are the building blocks of new automation, analysis and augmentation that will enable our customers and partners to build more valuable, data-driven and connected products and services. Autodesk remains relentlessly curious propensity and desire to evolve and innovate. Time and again, well-executed transformation from desktop to cloud from perpetual license and maintenance to subscription has added new growth factors, built a more diverse and resilient business, forged broader trusted and more durable partnerships with more customers and given Autodesk a longer run rate of growth and free cash flow generation. With our transformation from file to data and outcomes from upfront to annual billings and from indirect to direct go-to-market motion, we are building an even brighter future with focus, purpose and optimism. Our customers are also committed to transformation and Autodesk is deploying automation to increase their success in an environment with ongoing headwinds from material scarcity, labor shortages and supply chain disruption. That commitment was reflected in Autodesk's largest-ever EBA signed during the quarter and record contributions from our construction and water verticals to our overall EBA performance. I will now turn the call over to Debbie to take you through our quarterly financial performance and guidance for the year. I'll then come back to update you on our strategic growth initiatives.\\nDebbie Clifford: Thanks, Andrew. Overall, market conditions and the underlying momentum of the business remained similar to the last few quarters. Our financial performance in the third quarter was strong, particularly from our EBA cohorts, where incremental true-up and upfront revenue from a handful of large customers drove upside. As expected, the co-termed deal we called out in our Q1 results renewed in the third quarter with a significant uplift in deal size. Total revenue grew 10% and 13% in constant currency. By product in constant currency, AutoCAD and AutoCAD LT revenue grew 7%, AEC revenue grew 20%, manufacturing revenue grew 9% and in double-digits, excluding variances and upfront revenue, and M&E revenue was down 4% and up high single-digits percent, excluding variances in upfront revenue. By region in constant currency, revenue grew 19% in the Americas, 11% in EMEA and 3% in APAC, which still reflects the impact of last year's COVID lockdown in China. Direct revenue increased 19% and represented 38% of total revenue, up 3 percentage points from last year, benefiting from strong growth in both EBAs and the eStore. Net revenue retention rate remained within the 100% to 110% range at constant exchange rates. The transition from upfront to annual billings for multiyear contracts is proceeding broadly as expected. We had the second full quarter impact in our third fiscal quarter, which resulted in billings declining 11%. Total deferred revenue increased 6% to $4 billion. Total RPO of $5.2 billion and current RPO of $3.5 billion both grew 12%. Excluding the tailwind from our largest ever EBA, total RPO growth decelerated modestly in Q3 as expected when compared to Q2, mostly due to the lower mix of multiyear contracts in fiscal 2024 when compared to fiscal 2023. Turning to the P&L. Non-GAAP gross margin remained broadly level at 93%. GAAP and non-GAAP operating margin increased driven by revenue growth and continued cost discipline. I'd also note that costs associated with Autodesk University have shifted from the third quarter last year to the fourth quarter this year due to the timing of the event. Free cash flow was $13 million in the third quarter, primarily limited by the transition from upfront to annual billings for multiyear contracts and the payment of federal taxes we discussed earlier this year. Turning to capital allocation. We continue to actively manage capital within our framework. Our strategy is underpinned by disciplined and focused capital deployment through the economic cycle. We remain vigilant during this period of macroeconomic uncertainty. As you heard from Andrew, we continue to invest organically and through acquisitions in our capabilities and services and the cloud and platform services that underpin them. We purchased approximately 500,000 shares for $112 million, at an average price of approximately $206 per share. We will continue to offset dilution from our stock-based compensation program to opportunistically accelerate repurchases when it makes sense to do so. Now, let me finish with guidance. The overall headline is that our end markets and competitive performance are at the better end of the range of possible outcomes we modeled at the beginning of the year. This means the business is generally trending towards the higher end of our expectations. Incrementally, FX and co-terming have been slightly more of a headwind to billings than we expected. EBA expansions have been slightly more of a tailwind to revenue and interest income has been slightly more of a tailwind to earnings per share and free cash flow. Against this backdrop, we are keeping our billings guidance constant, while raising our revenue, earnings per share, and free cash flow guidance. I'd like to summarize the key factors we've highlighted so far this year. The comments I've made in previous quarters regarding the fiscal 2024 EBA cohort, foreign exchange movements, and the impact of the switch from upfront annual billings for most multiyear customers are still applicable. We again saw some evidence of multiyear customers switching to annual contracts during the third quarter, as you'd expect, given the removal of the upfront discount. We're keeping an eye on it as we enter our significant fourth quarter. All else equal, if customers switch to annual contracts, it would proportionately reduce the unbilled portion of our total remaining performance obligations and negatively impact total RPO growth rates. Deferred revenue, billings, current remaining performance obligations, revenue, margins, and free cash flow would remain broadly unchanged. Annual renewals create more opportunities for us to drive adoption and upsell and are without the price lock embedded in multiyear contracts. Putting that all together, we now expect fiscal 2024 revenue to be between $5.45 billion and $5.47 billion. We expect non-GAAP operating margins to be similar to fiscal 2023 levels with constant currency margin improvement, offset by FX headwinds. We expect free cash flow to be between $1.2 billion and $1.26 billion to reflect higher revenue guidance we're increasing the guidance range for non-GAAP earnings per share to be between $7.43 and $7.49. Our billings guidance remains unchanged, given incremental foreign exchange headwinds and the potential for further EBA co-terming in the fourth quarter. The slide deck on our website has more details on modeling assumptions for Q4 and full year fiscal 2024. We continue to manage our business using a Rule of 40 framework with a goal of reaching 45% or more over time. We think this balance between compounding growth and strong free cash flow margins captured in the Rule of 40 framework is the hallmark of the most valuable companies in the world, and we intend to remain one of them. As we've been saying all year, the path to 45% will not be linear. We've talked about all of the factors behind that over the last three quarters, and I think it's useful to put them all in one place here, particularly as we look into fiscal 2025 and 2026. First, the macroeconomic drag on new subscriber growth, a smaller EBA renewal cohort with less upfront revenue mix, and the absence of EBA true-up payments are headwinds to revenue growth in fiscal 2025. Slightly offsetting that, we expect our new transaction model, which Andrew discussed earlier, to be a tailwind to revenue growth in fiscal 2025 and beyond. Assuming no material change in the macroeconomic, geopolitical or policy environment, we'd expect fiscal 2025 revenue growth to be about 9% or more. In other words, at least around the same or more growth as we are now expecting in fiscal 2024. And second, the transition to annual billings means that about $200 million of free cash flow in Q1 fiscal 2024 that came from multiyear contracts built upfront will not recur in fiscal 2025. This will reduce reported free cash flow growth in fiscal 2025. And make underlying comparisons between the two years harder. If you adjust fiscal 2024 free cash flow down by $200 million to make it more comparable with fiscal 2025 and fiscal 2026 on an underlying basis, the stacking of multiyear contracts build annually will mechanically generate significant free cash flow growth in fiscal 2025 and fiscal 2026. The progression from the adjusted fiscal 2024 free cash flow base, will be a bit more linear, although fiscal 2026 free cash flow growth is expected to be faster than fiscal 2025 as our largest renewal cohort converts to annual billings in that year. As you build your fiscal 2025 quarterly and full year estimates, please pay attention to what we've said each quarter during fiscal 2024. As Andrew said, our new transaction model will likely provide a tailwind to revenue growth be broadly neutral to operating profit and free cash flow dollars and be a headwind to operating margin percent. The magnitude of each will be dependent on the speed of deployment. Excluding any impact from the new transaction model, we are planning for operating margin improvement in fiscal 2025. Overall, we expect first half, second half free cash flow linearity in fiscal 2025 to be more normal than in fiscal 2024. And we still anticipate fiscal 2024 will be the free cash flow trough during our transition from upfront to annual billings for multiyear contracts. Per usual, we'll give fiscal 2025 guidance when we report fiscal 2024 results, so I don't intend to parse these comments before them. As I said at our Investor Day last March, the new normal is that there is no normal. Macroeconomic uncertainty is being compounded by geopolitical, policy, health and climate uncertainty. I'm thinking here of generational movements in monetary policy, fiscal policy, inflation, exchange rates, politics, geopolitical tension, supply chains, extreme weather events and, of course, the pandemic. These increased the number of factors outside of our control and the range of possible outcomes, which makes the operating environment harder to navigate both for Autodesk and its customers. In this context, the mechanical rebuilding of our free cash flow as we transition to annual billings for multiyear contracts, gives Autodesk an enviable source of visibility and certainty. I hope this gives you a better understanding of why we've consistently said that the path to 45% will not be linear. But let me also reiterate this. We're managing the business to this metric and feel it strikes the right balance between driving top line growth and delivering disciplined profit and cash flow growth. We intend to make meaningful steps over time toward achieving our 45% or more goal, regardless of the macroeconomic backdrop. Andrew, back to you.\\nAndrew Anagnost: Thank you, Debbie Let me finish by updating you on our progress in the third quarter. We continue to see good momentum in AEC, particularly in transportation, water infrastructure and construction. Fueled by customers consolidating on our solutions to connect and optimize previously siloed workflows to the cloud. Market conditions remain similar to previous quarters. In Q3, WSP, one of the world's leading professional services firms closed its sixth EBA with Autodesk, a testament to our strong and enduring partnership. Leveraging the breadth of our portfolio, WSP has delivered the comprehensive range of services demanded by its clients, generate millions of dollars in pipeline across the AEC and manufacturing industries, secured bridge and groundwork contracts through automation capabilities, reduce costs through increased efficiency and most importantly, delivered impactful results for its customers. TCE, a global engineering and consulting firm, which supports all types of infrastructure is harnessing Autodesk solutions to bolster its sustainable development goals around clean water and sanitation. Industry innovation, infrastructure and responsible consumption and production, utilizing Autodesk's BIM Collaborate Pro, TCE plans to improve team collaboration through easier data exchange, fewer classes and more effective designer years. Autodesk solutions are empowering TCE to manage, coordinate and execute projects more efficiently, thus contributing to a better quality of life through improved infrastructure. We are seeing growing customer interest in our complete end-to-end construction solutions, which encompass design, preconstruction and field execution through handover and into operations. Encouragingly, Autodesk Construction Cloud MAUs were again up well over 100% in the quarter. In Q3, LFD, Inc., an ENR top 200 general contractor based in Ohio, selected Autodesk Construction Cloud over directly competitive offerings as its end-to-end construction platform. With our preconstruction and cost capabilities of standout differentiator, it ultimately chose Autodesk based on our level of partnership, our aligned vision and commitment to serve the evolving needs of the construction industry and the momentum our solution has demonstrated in the marketplace. Again, these stories have a common theme: managing people, processes and data across the project lifecycle to increase efficiency and sustainability while decreasing risk. Over time, we expect the majority of all projects to be managed this way, and we remain focused on enabling that transition through our industry clouds. Moving on to manufacturing. We made excellent progress on our strategic initiatives. Customers continue to invest in their digital transformations and to consolidate on our design and make platform to grow their business and make it more resilient. For example, a global industrial company based in the U.S. is partnering with Autodesk to innovate more rapidly in its business. The customer had already standardized on Autodesk's up chain to streamline its data and process manager within their molding technology solutions and modernize its CAM process by adopting Fusion to significantly reduce programming time and eliminate risks from legacy software. During Q3, it renewed its EBA with Autodesk and plans to broaden its use of up chain, vault infusion. It is exploring Fusion's ability to enhance process management and its digital threat initiatives, which focus on product life cycle management, closed-loop quality, sustainability design, service life cycle management and supplier insight. Fusion continues to provide an easy on-ramp into our cloud ecosystem for existing and new customers. For example, a leading manufacturer for the agriculture industry is migrating from network licenses to named users and complementing those subscriptions with Flex tokens to maximize value and access for occasional users. As it digitizes its factories and create digital twins for its global facilities, it will use Flex to explore Autodesk's most advanced technology for Fusion, for CAM tool path automation and generative design. Flex provides the customer with the flexibility to scale its usage based on its needs, making sure its users have access to the right products at the right time. Fusion continues to grow strongly, ending the quarter with 241,000 subscribers as more customers connect more workflows in the cloud to drive efficiency, sustainability and resilience. In automotive, we continue to grow our footprint beyond the design studio into manufacturing and connected factories. In Q3, a leading automotive manufacturer renewed and expanded its EBA by more than 50%. In addition to its existing usage of alias for concept design modeling and design evaluation, the customer is replacing an internal tool with Red for lighting simulation. In the future, it will implement flow production tracking to improve and accelerate project communication and collaboration across departments and expand its use of Autodesk's integrated factory modeling to optimize factory layouts and enhance operational performance. In education, we are preparing future engineers to drive innovation through next-generation design, analysis and manufacturing solutions. For example, our partnership with PanState is making a positive impact in design classes and car CMC activities across the PanState, Barron's, Burks, Terresburg and University Park campuses. PSU Harrisburg has recently adopted Fusion in its core design class and plans to integrate it into its mechanical engineering curriculum. Fusion's accessible platform allows students to seamlessly transition from car to CAE (NYSE:CAE) and CAM enabling them to make a different outside the classroom and in industrial applications. They have already collaborated with NASA on a generative design project for spaceflight applications, inspiring numerous projects at NASA. And finally, we continue with our customers to ensure they are using the latest and most secure versions of our software. A publicly traded construction company in Japan thought to streamline software management processes and minimize compliance risks by leveraging single sign-on FFO and directly sync features available in our premium plan. Through a collaborative analysis of the client software usage logs, we identify instances of noncompliant usage and recommended an appropriate number of subscriptions based on usage frequency and actual requirements. This proactive approach ensures that the client has the necessary access to meet their needs while maintaining compliance. We've been laying the foundation to build enterprise-level AI for years with connected data, teams and workflows in industry cloud, real-time and immersive experiences, shared extensible and trusted platform services and innovative business models and trusted partnerships. Autodesk remains relentlessly curious with the propensity and desire to evolve and innovate. We are building the future with focus, purpose and optimism. Operator, we would now like to open the call up for questions.\\nOperator: [Operator Instructions] Our first question comes from the line of Saket Kalia of Barclays.\\nSaket Kalia: Okay. Great. Hi guys, thanks for taking my questions here. How are you?\\nAndrew Anagnost: Great, Saket.\\nSaket Kalia: Andrew, maybe for you. Lots of talk about here, right, particularly with the new transaction model. Maybe the right first question here would be. Why is this new model, I guess, as consequential as you said in your prepared remarks. Any color there you can add?\\nAndrew Anagnost: Yes, absolutely. Let me start Saket by saying First, the business is super resilient where we're built through resilience, and this is really showing up in these results this round as well. And that's going to continue into the future. When we talk about this new transaction model, I think it's important to back up and talking about what we're trying to do with our customers and the journey we've been on. We are trying to do no less than move all of our customers to cloud-based life cycle solutions powered by AI that connect their design and make processes in a way that they've never had connected before. Now to do that, you absolutely cannot use 40-year-old systems and business models. So we've been on this relentless journey to modernize the company. We started moving from developing cloud-based products to subscription models, to annualized billings. I mean you've seen journey after journey here to modernize the company. This is the next step and one of the most important steps in modernizing the company so that we have the kind of relationship with our customers that actually matches the kind of technology we're delivering to them. So through this, we're not only going to have direct engagement with our customers through the products they use in the cloud, we're going to have direct engagements with them as a customer as an account. We're going to understand them at the account level and as an entity, not just as a collection of transactions passed through several tiers. And that's really important. Because that will not only give us more information about our customers, it will help us give more information to our partners about our customers and understand them significantly better. And it will wrap up the whole solution and business model and capabilities in one package. The other really consequential thing here is our partner network has to move transaction-focused partners to solution-focused partners. They are going to be incredibly important on the front lines in helping our customers deploy and integrate new design to make solutions in the cloud. And this is going to be part of that transition for them. So yes, it is very consequential. And it's part and parcel of a long stream of modernizations we've been working on for a while, and I do think it's very significant.\\nSaket Kalia: Got it. No, absolutely. It sounds very strategic. Debbie, maybe for my follow-up for you. I know you said you wouldn't parse out your comments on fiscal 2025. But -- could we maybe parse out those comments for fiscal '25 just a bit kind of given some of the moving parts sort to ask, but...\\nDebbie Clifford: There are indeed a lot of moving parts, Saket. So thanks for the question. I know that's the question that everyone wants to ask. I'm not going to parse all the details, but I'll just highlight some of the things that we called out in the opening commentary. Those things are the non-recurrence of EBA upfront and true-up revenue, FX and the macro drag on new subscriber growth, these are all things that are headwinds to revenue growth next year. We also talked about the impact depending on the timing of this move to a new transaction model. That's going to be a tailwind to revenue. It will be margin and cash flow dollar neutral and is a headwind to margin percent. We'll give you all the details on this in February for the usual. But remember, what we're really trying to do is set ourselves up for success over the long term.\\nSaket Kalia: Makes a lot of sense. Thanks, guys. I’ll get back in queue.\\nOperator: Thank you. Please standby for our next question. Our next question comes from the line of Jay Vleeschhouwer of Griffin Securities.\\nJay Vleeschhouwer: Thank you. Good evening. So on fiscal 2025, following up on Saket's question, at the analyst meeting earlier this year, Debbie, you showed a chart depicting sustainable double-digit growth for Autodesk of 10% to 15%. And based on a combination of various price and volume components, are you still adhering to that plethora of price and volume sources of growth? Or have you changed your thinking in terms of the magnitude or mix of those sources of growth over time?\\nDebbie Clifford: We are still targeting those sources of growth, Jay, as well as targeting the growth parameters of 10 to 15 points of revenue growth. Really, what we're dealing with is this uncertain environment. And based on what we know today and assuming that market conditions are similar to what we've seen over the last several quarters, we do see revenue growth next year of about 9% or more. And what's driving that is really all those puts and takes that I talked about in the opening commentary. It's really important to remember that what we're trying to do is set ourselves up for success over the long-term.\\nJay Vleeschhouwer: Okay. Andrew, for you, following up on AU last week. There were a number of quite interesting and useful sessions on our roadmaps and product plans, particularly on AUC and more broadly with regard to the data model. So, let me ask you an unavoidably complex question about that. So, when you -- so when you think about the role of what you call granular data, does that ultimately affect as you implement that the packaging or composition or consumption of the software? And you also gave quite detailed description of where you're going with ACC and Bill and AC generally. But there's no timeline in any of those presentations. So, how are you thinking about the GA of much of what you talked about last week at AU in terms of making commercial a very large set of new technology features, particularly for AC?\\nAndrew Anagnost: Yes. Okay. So, let me tackle that with the first thing around the granular data. So, ultimately, as you journey down this path, what does happen is the way the product operates all the products Forma in particular in terms of how it interacts with Revit and ultimately how those two blend together, they do become an environment that looks very much like the fusion environment. And that environment is very different, as you know, than what it currently exists in most of the wide -- the mainstream usage of our AC products. So, yes, granular data ultimately leads to a different way that people consume and use the products in a different paradigm for which they actually engage with the product every day. So. that's clear, okay? Timeline, I don't know exactly which presentation you're in. I suspect given your questions, you are in the more longer term timeline presentation. So, a lot of stuff you saw there was over a two to five-year time frame, but a lot of that is going to show up in the two to three-year timeframe associated with some of the things you heard. Now, I think it's kind of obvious to tell which ones we're towards the earlier part of that spectrum rather to the later part of that spectrum. Turning some of these solutions over into infrastructure solutions and combining them with some of our infrastructure stuff. Probably towards the later end, getting the data more granular, uniting detailed design and conceptual design in Forma, probably much more closer. That kind of expectation you can have with those road maps.\\nJay Vleeschhouwer: Great, great. Thank you both.\\nOperator: Thank you. Our next question comes from the line of Adam Borg of Stifel.\\nAdam Borg: Awesome. Thanks so much for taking the question. Maybe for Debbie, just on the multiyear to annual billings transition. Maybe just if you could just remind us kind of where we are overall in the process relative to expectations. And I do know that there are still some smaller cohorts that have yet to transition and just curious kind of where we are for those and if that's going to take place next year? Or is that still kind of in process?\\nDebbie Clifford: Sure. Thanks. The rollout is going well. We're a couple of quarters in the systems are working. Customers and partners are behaving pretty much as we expected. So overall, the performance is in line with our expectations. I think the key thing is, remember, we're kind of at the beginning of this journey. This is going to be a three-year journey. So we're going to have a mechanical rebuild of free cash flow as we get into next year, fiscal 2025 and also in fiscal 2026. So some of the comments that I made on the call are important, and that is helping you think about how to normalize our fiscal 2024 cash flow headed into fiscal 2025. So we're moving that $200 million at the beginning of fiscal 2024 as we head into fiscal 2025. And then broadly, the fact that we'll have bigger cohorts coming up for renewal in fiscal 2026, which drives faster growth in free cash flow in that period. So overall, things are going well, and we're at this interesting point where we expect to see mechanical rebuilding of free cash flow from here.\\nAdam Borg: Got it. And maybe just a quick question. Interesting AI announcements with Autodesk AI, back at AU last week. Any commentary on how to think about any price uplift from those solutions? Thanks again.\\nAndrew Anagnost: Yes. So I'll take that one, Adam. Look, some of these features are already and will be delivered through our existing products. However, there are new models will be exploring with some of these capabilities. Obviously, it's a little too early to talk about actual monetization. But I do think some of the things you're seeing with Microsoft (NASDAQ:MSFT) right now are quite interesting where highly evolved large models, which we have not yet deployed out in the market are offered up to individual customers as a here's your model. Now you train it, you custom train it and extend it with your data. Those kind of models are going to be very interesting in the future and really look like possibilities that we'll probably explore and look at. As we move forward. But for now, a lot of this functionality is going to end up integrated with the existing products as it has been for the last several years.\\nAdam Borg: Great. Thanks again.\\nOperator: Thank you. Our next question comes from the line of Joe Vruwink of Baird.\\nJoe Vruwink: Great. Hi, everyone. Maybe just a follow-up on that last question. Andrew, like you said, AI and automation is not new at Autodesk. But I did think the messaging was maybe a little more exact and pointed just as it pertains to the cloud data strategy and how that really is the gateway to future AI capabilities that Autodesk how customers need to be thinking about this. So my question is just curious to hear any feedback from customers on this approach. And maybe levels of resistance or buy in, you've started to hear just pertaining the customers kind of pooling their data and Autodesk ends up being the aggregator of industry information?\\nAndrew Anagnost: Yes. So look, we have a very strong point of view on ethical and high trust use of data, and we intend to continue to pursue that with our customers and take a broad and strong stance around look, it's your data. We're going to work with you to use it appropriately for things that make the whole ecosystem better. We're going to do it in a way that's trusted. And we're also going to work with you in a way that allows you to preserve the IP that you think is important to you that does not become part of the entire ecosystem. So this is a conversation I have with many, many customers most obviously recognize the trade-off between massive amounts of productivity in terms of automating model creation and some of the benefits there. So they want to participate in ways that actually make sense for them and that maintain the trust and integrity that we're looking to do. So look for us to handle this in exactly that matter as we move forward.\\nJoe Vruwink: Okay. Great. And then I'm going to take my best shot at FY 2025 question as well. But I think maybe 2 points of clarification or additional information. So Debbie, just on kind of the known headwind to free cash flow next year because of the long-term deferreds that happened to hit in this year. Can you reconcile that with the normal seasonality comment. Should we be removing that and then thinking about modeling normal seasonality, part A? Part B, you've talked about currency a lot is having impacts on some of these numbers. And I would imagine just given what's on the balance sheet, you probably have a good sense of what currency will be next year. How does currency factor into that 9% plus revenue growth rate you provided?\\nDebbie Clifford: Sure. Thanks, Joe. So on the first question, I think you're thinking about it in a reasonable way. So take out the $200 million and then it should have a more reasonable that will give you more reasonable modeling expectations as you think about modeling fiscal 2025 and beyond. And then on currency, is it's really been all over the place. I think everybody has been seeing that. What we see right now is that it would be a headwind for us as we head into next year. But given the volatility, I think it really could go either way. But based on what we're seeing right now, it is a headwind to revenue growth next year.\\nJoe Vruwink: Okay. Thank you very much.\\nOperator: Thank you. Our next question comes from the line of Tyler Radke of Citi. Please go ahead, Tyler.\\nTyler Radke: Okay. Great. Thanks for taking the question here. Andrew, you talked about some record contribution from the construction side of the business, I think, broadly, but also within the EBAs. Could you elaborate within Autodesk Construction Cloud. What are the strongest areas you're seeing customers about?\\nAndrew Anagnost: Yes. No. What's interesting is. We're not seeing the softness in construction that others may have highlighted. In fact, we have incredibly strong performance at the top end of our business. We saw strong growth internationally. And we're seeing growth in the U.S. And a lot of things are going on in the construction business right now. And whereas you see some sectors slowing down retail warehouse office things like that. You're seeing other things offsetting it. Again, the dynamicism of Autodesk business, manufacturing, industrial, lots of factory construction going on data centers, health care, , infrastructure, all of these things are picking up. So we've got a lot of dynamics that are playing well with regards to our construction business right now. When we look at the business, we look at the indicators that we have, bid board activity was at record highs again. So we saw a good strong bid activity there. While construction backlog may have declined a bit, it's still high, all right? And the number one thing that I heard from general contractors at Autodesk University was still can't hire enough. So they're still going to be working through that backlog at a relatively slow pace. Also, what's really interesting is we're seeing ourselves in more deals down market now more competitive deals, and frankly, we're winning some of them. And I think that's interesting. I think that probably results in slowing down deals for some of our competitors in various markets. But we're actually seeing a lot more interesting deal activity.\\nTyler Radke: That's helpful. And a follow-up for Debbie. I appreciate you getting a lot of questions on FY 2025. And I'm not going to ask you to dissect it further. But if I think about just trying to bridge the 9%, which seems like it does have some tailwinds from the transactional changes relative to that 10% to 15% framework that you gave, it doesn't seem like macro has worsened relative to a few quarters ago or a year ago when you gave that out based on your commentary. Just help us understand that bridge. Is it mostly conservatism or maybe currency or some of the other headwinds are larger than we're thinking about? Thank you.\\nDebbie Clifford: Yes. Sure. So remember, you got to be thinking about the non-recurrence of the EBA upfront and true-up revenue that we've been talking about all year. FX, as I just mentioned, could be a factor right now, we're assuming that it is a headwind to revenue growth. And then finally, we have been talking about the macro drag on new subscriber growth all year. And remember, given the ratable revenue recognition model that we have, what we're seeing with new subscriber growth this year has more of an implication for revenue growth next year than for revenue today. So those three factors are the biggest factors driving our estimate of 9% or more as we head into next year.\\nTyler Radke: Okay. Thank you.\\nOperator: Thank you. Our next question comes from the line of Jason Celino of KeyBanc Capital Markets.\\nJason Celino: Great. Thanks for taking my question. Maybe one on the EBAs. So in Q3, did you see any of these renewals maybe happen earlier than expected? Overall, it sounds like you're still seeing some timing true-ups, but also some pretty big expansions. Is the overarching takeaway that the cohort is expanding maybe better than what you had anticipated?\\nDebbie Clifford: Thanks, Jason. So the EBA cohort has been performing really well all year, which has been great. Remember, this is a cohort that last renewed in late 2020. That was at the height of the pandemic. And back then, they made more conservative assumptions about usage because of the uncertain environment at that time. Fast forward to today, these customers are continuing to manage through a high demand for projects. That's led to higher overall usage on their contracts. And as we've mentioned before, we do monitor the usage. So we've had insight into the potential EBA upside as the year has progressed. We've continued to update our outlook, which is each quarter of the renewals and the true-ups. And as we look at Q4. We've got our eye on the remainder of this large EBA cohort and the signs continue to be strong. We factored all of this into our latest estimates, and that's what drove the top line upgrade that we communicated today.\\nJason Celino: Okay. Great. And then I asked this question last quarter, but it sounds like a few of your competitors might be starting to see some of the water infrastructure funding start to flow plan intended this time. Are you seeing this, too? And then is this the strength that you're already seeing? Or could this be an additional opportunity for maybe next year?\\nAndrew Anagnost: Yes. Jason, one of the things that you may have heard is that some of that money from the infrastructure bill that was targeting modernization of departments of transportation was really, it's about $34 million that's significant in that not only it starts these DOTs on their process of modernization and evaluating the modernization, but it also was directed at several DOTs that we have relationships with and where we've actually displaced competitors and engaged with the infrastructure. So that's pretty exciting stuff. That shows money starting to flow to the projects. As I've always said, it takes time to release this money from the flood gates of Washington into the places floodgate of Washington, that's kind of an oxymoronic comment. But it takes time to get there. And that -- these -- this money is going to kind of again, build up momentum for the rest of the projects and help us move forward. So I would say it continues to be an emerging opportunity. Projects are getting started, but there's more hope in the future for even more projects.\\nJason Celino: Okay, Great. Thank you.\\nOperator: Thank you. Our next question comes from the line of Michael Funk of Bank of America.\\nMichael Funk: Yes. Thanks for the questions. Two, if I could. So first for you, Andrew. A number of changes with partner relationships from last year, you mentioned the new transaction model I think earlier you also changed the commission structure to more back end versus front-end loaded. So curious what kind of reaction you're hearing from your partners and how you expect these changes to impact that relationship?\\nAndrew Anagnost: Yes. So obviously, we invest a lot in bringing our partners along on this. It doesn't mean all partners are going to be happy with these changes. Okay? It's just that is not an outcome that we're looking for or is likely. But many partners are going to be happy with these changes because we've been very clear about what the path is to growth for them. Beyond that, we've taken a really kind of incremental approach to these things with our partners. We kind of led them along, showed them the way. You might recall, we started the new transaction model with Flex, close to about 1.5 years ago in Australia, then we rolled it out to the broad-based partner network and everybody got experience with it. Now we're testing in Australia. Again, we take our partners along on these journeys in very deliberate ways. And frankly, the credibility we have with our partners in terms of making their businesses more consequential and significant and, frankly, larger over time has really created an environment where there's a lot of trust. And there's a lot of discussion about what's the best way to do this. What this means for them and where it's going to take their business. And this will make the partners ultimately more consequential in some of the business discussions with their partners with their customers by bringing them closer to the design and make infrastructure work that needs to happen in the services and support that.\\nMichael Funk: Thank you for that, Andrew. And one for you, Debbie, will see a check on the accounting of the math. You mentioned EBA revenue, non-recurrence in fiscal 2025 a few times. So, two pieces. First, accounting. I thought that the true-ups with upsizing in EBA, I thought that was recognized ratably over the term of the contract. Just trying to think how that might carry over an impact 2025, I'm correcting that accounting? And then second, if you can just remind us on the actual benefit to fiscal year 2024 from those items so we can pull that out of the fiscal year 2025 number?\\nDebbie Clifford: Sure. So, from an accounting standpoint, the true-ups we recognize upfront. So, think of it as an enterprise customer signs up for X number of tokens. And when they exceed X number of tokens, we build them for the differential and we recognize that revenue upfront and it doesn't recur in future periods, unless over the next three-year contract term, they utilized more than the tokens allotted in their contract again. So, that's why it's something that is sort of a one-off -- a good one off, but a one-off nonetheless, that could only recur three years from now for these contracts if we found ourselves in the same situation. And then in terms of sizing the benefit to fiscal 2024, we haven't gotten into exact numbers, but you can think about the overall guidance upgrade that we talked about today is largely being driven by the strength that we're seeing from the enterprise business this year.\\nMichael Funk: Great. Thank you both.\\nOperator: Thank you. Our next question comes from the line of Matt Hedberg of RBC.\\nMatt Hedberg: Great. Thanks for taking my questions. I guess for either of you, maybe thinking longer term, I'm curious if you could help us with maybe the -- this move from contra revenue. What is the impact to sort of like pro forma revenue growth once the business has migrated more so to the Flex model?\\nDebbie Clifford: Matt, it's going to be -- revenue growth will accelerate. And the pace at which it accelerates is going to be determined based on how we go about the rollout. But as we mentioned, we're working on that now. We launched Flex last year. We just launched Australia. We're learning from Australia -- as we -- right now. And then as we look ahead to next year, we intend to go global with this, but we need to make sure that we are set up for success, which is why we're watching Australia closely. But when we execute finally, on all aspects of this transition, it will be an accelerator to revenue growth.\\nMatt Hedberg: Got it. Thank you. And then I know last quarter, you saw some pretty good non-compliant conversions I don't think you mentioned the other side - recall you're talking about that. Was there any this quarter that you called out?\\nDebbie Clifford: We continue to perform well with our non-compliant conversions. So, I think you've heard us talk about a couple of things. I think historically, we have talked about some of the larger deals that we've closed, and those types of deals are still happening. But as I recall, on last quarter's earnings call, Andrew was talking about some of the stuff that we've been doing in product that's driving more conversion. That's on a smaller scale in terms of deal sizes, but is driving significant volume for Autodesk. So, over time, you're going to see us continue to flex different means of driving more compliance from non-compliant users, and it continues to be a steady drumbeat contributor to our revenue growth over time.\\nMatt Hedberg: Got it. Thanks.\\nOperator: Thank you. Our next question comes from the line of Bhavin Shah of Deutsche Bank. Please go ahead, Bhavin\\nBhavin Shah: Great. Thanks for taking my question. Just kind of following up on that last one on the new transaction model. Like what kind of learnings are you looking to see from Australia before kind of rolling the app more broadly and kind of any disruption kind of that we should think about from a pointer perspective?\\nAndrew Anagnost: Well, one of the things that we're trying to make sure that we see is how do the partners line up their deals so that they're able to enter them into the system and make sure they get their pipeline lined up with the new way of doing things because they're going after directly enter them into the system. Some of these services used to be taken over by distributors for some of our partners. We're going to make sure that -- we want to make sure that large volumes work well with the systems. We're pretty confident at this point because of the Flex experience, but we know we want to stress test these things. We want to make sure that it works for all the product offerings that there's no issues or hiccups with particular things that when people try to true up renewal dates or line them up, there's not issues with those things. It's all the things that go into the mechanics of a partner entering the deal, all right, and having those things actually function. We just want to make sure it all works. Again, we have a lot of confidence because of the Flex work, but those are the things we're going to be testing for in the Australia pilot.\\nBhavin Shah: That's helpful there. And just kind of following up on Fusion 360. I know you guys are making some pricing adjustments going into next year, kind of raising the list price on Fusion 360, but kind of rationalizing and lowering the price on the extensions. What's the kind of the rationale behind this? Is this to drive kind of further extension adoption down the road? And kind of how does this inform your views on as you think about rolling this out to the form and the like?\\nAndrew Anagnost: Yes. So Bob, what we're doing there is the price increase in Fusion is directly connected to the value we're delivering Fusion we're making sure some of the customers who have been with us for a long time are treated appropriately and fairly. So we're paying attention to all those things to customer dynamics. But what we saw is that the value in base Fusion has just increased to high level that we should be looking at the price more carefully. The value is going to continue to increase. And what we saw is that some of the extensions would probably see better adoption in some of the base -- the value was shifted to the base offering and the price of the extensions were contracted a little bit. So it's all this kind of balancing the overall cost of ownership for particular types of customers. And it's an appropriate time to do it.\\nBhavin Shah: Very helpful. Thanks for taking the question.\\nOperator: Thank you. Our next question comes from the line of Steve Tusa of JPMorgan. Your question please, Steve,\\nSteve Tusa: Hi, guys. Thanks for taking my question. Just on the subscriber growth, are we talking like -- you mentioned the macro impact several times. What are we kind of talking about what kind of rate this year? Is that like in the low to mid-single-digit range? And then what would it take for that to go flat? What type of macro do you think it would take to go flat? And then secondarily, just on the free cash flow side, I think at the Investor Day, you had put a chart in there that insinuated that cash would still grow from 2023 through the number in 2026. Are we still on track for that kind of longer-term view just to level set us on the longer-term cash outlook?\\nAndrew Anagnost: Yes. So Steve, let me take the first question a little bit. I won't answer the specific question. What I want to say is our business is incredibly resilient. You have to really pay attention to that, we're built for resilience. And I want to highlight some of the differences in puts and takes here. For instance, you probably noticed that AEC grew 20% in the quarter. And that offset some of the headwinds from media and entertainment due to wider strikes and after strikes. Regionally, India and Canada offset the U.S. and the U.K. Market segment-wise, EBAs and small businesses offset the mid-market. You have to think of the business through this built for resilient framework. And so I want to shift your lens a little bit, and then I'll let Debbie comment on the second part of your question.\\nDebbie Clifford: Yes, I would say, look, outside of the new transaction model, nothing has changed, and we're on track to achieving our goals. But this is a pretty big decision for us to transform our go-to-market. But I think is really beneficial to the company. It's going to drive greater free cash flow and greater revenue growth over the long term. I'm not going to parse comments about fiscal 2026 in addition to fiscal 2025 on this call. What we're really trying to do is set ourselves up for success over the long term and make smart decisions for the business and for our shareholders.\\nSteve Tusa: Okay. Great. Thanks a lot.\\nOperator: Thank you. That is all the time we have for Q&A today. I would now like to turn the conference back to Simon Mays-Smith for closing remarks.\\nSimon Mays-Smith: Thanks, everyone, for joining us. Wishing those who celebrate a happy Thanksgiving and looking forward to catching up with you on the road over the coming weeks and at next quarter's earnings. Thanks so much, Latif. Handing back to you.\\nOperator: Thank you. This concludes today's conference call. Thank you for participating. You may now disconnect.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " None,\n",
       " None,\n",
       " \"Best Buy  Co., Inc. (NYSE:BBY) reported better-than-expected profitability for Q3 2024, despite revenue falling slightly short of expectations. The electronics retailer experienced a 6.9% comparable sales decline due to softening consumer demand throughout the quarter. However, the company's gross profit rate increased by 90 basis points year-over-year, and its SG&A expenses were reduced through cost control measures and labor expense adjustments. \\nKey takeaways from the earnings call:Best Buy's gross profit rate increased due to improvements in its membership program and product margins.The company lowered its SG&A expenses by tightly controlling costs and adjusting labor expense rates.The company's paid membership base grew, and customer satisfaction scores improved across various service offerings.Despite unpredictable consumer demand, Best Buy has raised the midpoint of its annual EPS guidance slightly above the original guidance.The company plans to refresh its store portfolio next year, focusing on improving merchandising presentation and efficiency, and investing in formats that drive returns.Best Buy plans to open smaller footprint stores to test physical points of presence in new markets.The company is enhancing its supply chain network and optimizing shipping locations to improve efficiency and effectiveness.The company has adjusted its Q4 revenue outlook due to unpredictable consumer demand. However, it remains confident about future opportunities and expects stabilization and potential growth in the consumer electronics industry next year. \\nBest Buy's Q3 enterprise revenue declined 6.9% on a comparable basis, with a non-GAAP operating income rate of 3.8%. Domestic revenue decreased 8.2%, driven by a comparable sales decline of 7.3%, while international revenue decreased 3.4% due to a comparable sales decline of 1.9%. \\nIn the earnings call, Best Buy outlined plans to remodel and open new stores, especially smaller footprint stores for testing purposes. The company also plans to close existing stores as leases come up for renewal, with an expected closure rate of 15 to 20 stores per year in the near term. \\nBest Buy's full year guidance includes expectations of declining comparable sales, a non-GAAP operating income rate of 4% to 4.1%, and non-GAAP diluted earnings per share of $6 to $6.30. The company expects the gross profit rate to improve by approximately 60 basis points in 2024, primarily due to membership offerings. \\nThe company also discussed several factors that could impact their credit card business and overall performance in the coming year, including net credit losses and the industry's performance. They highlighted the importance of sales performance and the level of pressure on fixed costs. \\nDuring the earnings call, Best Buy also discussed its promotional events and expectations for the holiday season. The company expects holiday promotions to be ongoing while maintaining profitability for investors. \\nThe call concluded with a thank-you message and well-wishes for the holiday season. The next earnings call is scheduled for February.Full transcript - Best Buy (BBY) Q3 2024:Operator: Ladies and gentlemen, thank you for standing by. Welcome to the Best Buy's Third Quarter Fiscal 2024 Earnings Conference Call. At this time, all participants are in a listen-only mode. Later, we will conduct a question-and-answer session. [Operator Instructions] As a reminder, this call is being recorded for playback and will be available for -- by approximately 1:00 P.M Eastern Time today. [Operator Instructions] I will now turn the conference call over to Mollie O'Brien, Vice-President of Investor Relations.\\nMollie O'Brien: Thank you, and good morning, everyone. Joining me on the call today are Corie Barry, our CEO, and Matt Bilunas, our CFO. During the call today, we will be discussing both GAAP and non-GAAP financial measures, a reconciliation of these non-GAAP financial measures to the most directly comparable GAAP financial measures and an explanation of why these financial measures are useful can be found in this morning's earnings release, which is available on our website, investors.bestbuy.com. Some of the statements we will make today are considered forward-looking within the meaning of the Private Securities Litigation Reform Act of 1995. These statements may address the financial condition, business initiatives, growth plans, investments and expected performance of the company and are subject to risks and uncertainties that could cause actual results to differ materially from such forward-looking statements. Please refer to the company's current earnings release and our most recent 10-K and subsequent 10-Qs for more information on these risks and uncertainties. The company undertakes no obligation to update or revise any forward-looking statements to reflect events or circumstances that may arise after the date of this call. I will now turn the call over to Corie.\\nCorie Barry: Good morning, everyone, and thank you for joining us. For the third quarter, we are reporting better-than-expected profitability on slightly softer-than-expected revenue. Specifically, we are reporting a comparable sales decline of 6.9%, which is slightly below our outlook for the quarter as consumer demand softened through the quarter. At the same time, we expanded our Q3 gross profit rate 90 basis points from last year due to profitability improvements in our membership program and better product margins. We also lowered our SG&A expense compared to last year as we tightly controlled expenses and adjusted our labor expense rate with sales fluctuations. During the quarter, we grew our paid membership base and drove meaningful improvements in customer satisfaction scores across many of our service offerings, including in-home delivery, in-store services and remote support. Our Q3 results demonstrate our ongoing strong operational execution as we navigate through the sales pressure our industry has been experiencing for the past several quarters. The sales pressure is due to many factors, including the pandemic pull-forward of tech purchases, the shift back into services outside the home like travel and entertainment and inflation. In the more recent macro-environment, consumer demand has been even more uneven and difficult to predict. Based on the sales trends in Q3 and so far in November, we believe it is prudent to lower our revenue outlook for Q4. But despite the lowered sales outlook, the midpoint of our annual EPS guidance is now slightly higher than the midpoint of our original guidance as we entered the year. I want to thank our associates for their resilience and relentless focus on our customers. I continue to be so very proud of the way our teams are managing the business today and preparing for our future. Now, I would like to provide more color on our Q3 performance and holiday plans before passing the call off to Matt for the financial details on the quarter and our outlook. We continue to strategically manage our promotional plan and we're price-competitive in an environment, where consumers are very deal-focused and making spend tradeoffs right for their budget. Consumers are looking for value, and from an industry themes' perspective, we are seeing some trade-down in the television category, but not as much trade-down in other categories. As a result, and as expected, the goal of industry promotions and discounts were above last year and pre-pandemic fiscal '20. Similar to the first half of the year, during Q3, our purchasing customers were relatively consistent in terms of demographics versus last year. As a reminder, we over-index with higher-income consumers, compared to the general population. And we saw the percent of revenue categorized as premium and the percent of purchases over $1,000 remain constant versus last year. We have largely maintained our year-to-date industry share in our Circana, formerly NPD-tracked categories. Against this backdrop, our focus on deepening relationships with customers remains crucial. Our membership program delivered another quarter of growth and improved profitability versus last year. The Q3 contribution to the enterprise operating income rate was larger-than-expected due to the combination of a lower cost to serve and higher paid in-home installation services. For the full year of fiscal '24, we now expect our three-tiered membership program to contribute approximately 35 basis points of Enterprise year-over-year operating income rate expansion. It is still early since we introduced material changes in June, but there are a number of insights I would like to share. One, we continue to increase our paid membership base and now have 6.6 million members. This compares to 5.8 million at the start of the year. During the third quarter, we signed up approximately 35% more new paid members compared to the third quarter of last year, driven by the addition of the new Tier and buoyed by back-to-school and October’s member month events. Two, our paid members continue to interact with the brand and shop more frequently compared to non-members, which is the goal of any membership program. Three, and though it is early and we have not yet lapped the new programs, retention rates are outperforming expectations. Four, My Best Buy Total, which is the evolution of our prior Total tech offer, continues to resonate more strongly in our physical store setting. As a reminder, this tier is $179.99 per year and includes Geek Squad 24/7 tech support via in-store, remote, phone or chat on all your electronics no matter where you purchase them. It also includes up to two years of product protection, including AppleCare Plus on most new Best Buy purchases and includes all the benefits of My Best Buy Plus. And five, our My Best Buy Plus tier is resonating more with the digital customers and appeals to a broader set of customer segments. This is the new tier for customers who want value and access. For $49.99 per year, customers get exclusive prices and access to highly anticipated product releases. They also get free two-day shipping and an extended 60-day return and exchange window on most products. We are still early in the process and are testing different promotional offers to determine what resonates most with consumers as well as continuously improving the digital experience to make it even easier to find deals and benefits. Of course, we also have a free membership tier that enables free shipping for everyone, a great differentiator, especially in the holiday season. During the quarter, we continued to evolve our omnichannel capabilities to support our strategy and make it easy and enjoyable for consumers to get the best tech and premier expert consultation and service when they want it through our online store and in-home experiences. Last month, we introduced Best Buy Drops, which is a new experience only available through the Best Buy app. It gives customers the opportunity to access product releases, limited edition items, launches and deals from a variety of categories. There are multiple drops nearly every week, and they're only available in limited quantities. We are encouraged by the early results as Best Buy Drops is driving both incremental customer app downloads and higher frequency of app visits. We have also seen growth in sales from customers who are getting help from our virtual sales associates. These interactions, which can be via phone, chat or our virtual store, drive much higher conversion rates and average order values than our general dot.com levels. This quarter, we had 140,000 customer interactions by a video chat with associates, specifically out of our virtual store locations. As a reminder, this is a physical store in one of our distribution centers with merchandising and products that are staffed with dedicated associates and no physical customers. We also teamed up with live shopping platform Talk Shop Live, to test a series of online shopping events this month, starring our virtual sales associates. These events feature products from some of our newer categories like beauty and wellness as well as new tech and unique products. Our physical store portfolio is one of our key assets, and the role of our stores is to provide customers with differentiated experiences, services and multichannel fulfillment. At the same time, we need some stores to be more cost and capital efficient to operate. As a reminder, while almost one-third of our domestic sales are online, 43% of those sales were picked up in one of our stores by customers in Q3. And most customers shop us in multiple channels. Consistent with our normal cadence, we have largely completed the changes to our store portfolio for the year, so we can focus on the holiday season with minimal disruption to our physical stores. As we think about next year with the current economic backdrop, we plan to spend more of our capital expenditures refreshing a greater number of our stores and less on large-scale remodels. As such, we have three priorities for our US store fleet in the near term. Number one, we are refreshing our stores with a particular focus on improving enlivening the merchandising presentation given the shift to digital shopping and corresponding lower need to hold as much inventory on the shopping floor. For example, this year, in all our stores, we installed new premium end caps in partnership with key vendors that improve the merchandising in the center of the store. This year, we installed up to 10 of these new end caps per store or roughly one-third of our end caps per store and plan to add more next year as we work to upgrade these crucial locations in our stores. In addition, this year, we rightsized our traditional gaming spaces in roughly half of our stores to allow for the expansion of growing categories like PC gaming and newer offerings such as Greenworks cordless power tools, wellness products like the Oura ring, Epson short throw projectors, e-bikes and scooters and Lovesac home furnishing products. While small, we are seeing promising results in some of these new categories with meaningful market share growth. And as always, we continue to work closely with our vendor partners to add experiences to our stores. For example, LEGO and Therabody invested in new shop-in-shops in all our 35,000 square foot experience stores. In addition, and as you would expect, many of our premium partners are continuously updating their in-store spaces to reflect their latest innovations. We will continue this work next year in all our stores, rightsizing a number of categories to ensure we are leveraging the space in the center of our stores in the most exciting, relevant and efficient way possible. Our second priority is to keep investing in formats we know drive returns. This year, we implemented 8 large format 35,000 square foot experienced store remodels for a total of 54 and will end the year with 23 outlet stores. At this point in time, we plan to implement a minimal number of remodels and outlets next year. And the third priority is to open a few smaller footprint stores to keep testing our hypothesis at physical points of presence matter, and we need less selling square footage and more fulfillment and inventory holding space. In addition, we plan to open a few smaller stores in outstate markets to test the impact of adding new locations and geographies where we have no prior physical presence and our omnichannel sales penetration is low. At the same time, we also continue to close existing traditional stores as a result of our rigorous review of stores as their leases come up for renewal. This year, we have closed 24 stores. Over the past five years, we have closed approximately 100 Best Buy stores, which is a 10% decline in store count during that time frame. And we expect to close roughly 15 to 20 stores per year in the near term. We have been enhancing our supply chain network to support these footprint changes and deliver speed, predictability and choice to our customers. For example, we have worked to optimize our ship-from-store hub footprint to maintain substantial coverage for faster offers and take shipping volume pressure off the majority of the stores to allow them to focus on in-store and pickup experiences. Additionally, we are optimizing our shipping locations to enhance our efficiency and effectiveness while still delivering with speed. And as a result, in Q3, we had the lowest ship from store volume as a percent of total since well before the pandemic, with approximately 62% of e-commerce small packages delivered to customers from automated distribution centers. We also continued to augment our own supply chain through other partners and launched Best Buy on DoorDash (NASDAQ:DASH) marketplace, offering our second scheduled parcel delivery option in addition to Instacart (NASDAQ:CART) Marketplace. As we have discussed previously, we have made strategic structural changes to our store operating model over the past few years to adjust to the shifts we have seen in customer shopping behavior and our corresponding operational needs. These changes provide more flexibility and have allowed us to flex labor hours with the fluctuations in customer sales, shopping preferences like curbside and traffic. As a result, we kept our labor rates steady as a percent of revenue, even as our sales have declined over the past several quarters. As you can imagine, there is a delicate balance to maintain while we adjust our store operating model as the expert service our associates provide customers is a core competitive advantage. We keep a very close watch on our customer satisfaction trends to make sure we are not negatively impacting the customer experience. Broadly, I am proud that the team is doing this work while driving higher purchasing customer NPS for associate availability, product availability and pricing. We are also committed, of course, to providing a great employee experience through training opportunities and benefits. As we mentioned last quarter, we have now led thousands of our sales associates through a certification process focused on our foundational retail excellence. We are also leveraging technology in our stores more than ever to continue to elevate our customer and employee experiences in more cost-effective ways. A great example is our app built for employees called Solution Sidekick, that provides a guided selling experience consistent across departments, channels and locations. Our employees have embraced solution Sidekick and we can see higher customer NPS when our employees are utilizing the app in their interactions with customers. We are gratified that our employee retention rates continue to outperform the retail industry, particularly in key leadership roles, the vast majority of which we hire internally. Our average tenure, excluding our seasonal workforce, for field employees is just under five years, and our general manager tenure is almost 16 years. This is crucial as we can directly tie tenured experience and training certifications to NPS improvement over time. We have also seen a strong pool of applicants for new associates to supplement our store teams this holiday season. As you all have likely noticed, the holiday shopping season has begun. Since we are preparing for a customer who is very deal focused, we expect shopping patterns will look even more similar to historical holiday periods than they did last year with customer shopping activity concentrated on Black Friday week, Cyber Monday and the last two weeks of December. From an inventory perspective, we expect to have strong product availability across categories this year. We will continue to manage inventory strategically to maximize our ability to flex with customer demand. We are excited about the promotions and deals we have planned for all customers and budgets, including special promotions and early access to deals for our My Best Buy Plus and My Best Buy Total members. We have curated gift list to help everyone find the perfect gift. We also introduced a new resource on bestbuy.com and the Best Buy app called Yes Best Buy Sells That, where customers can find the latest in tech and gifting, like pet tech, baby tech or electric vehicle chargers, all the way to unique products, some shoppers may not know we sell like skin treatments, toys for all ages and electric outdoor power equipment. For added ease of shopping and peace of mind, we've extended both our store hours and our product return policy for the holiday season. And this year, for the first time, we also extended that our shoppers can connect directly with one of our virtual sales experts to get help with their holiday shopping. We're also offering free next-day delivery on thousands of items in addition to convenience store and curbside pickup options. Most orders placed on bestbuy.com or through the Best Buy app are ready for store pickup within one hour. Same-day delivery is also available on most products for a small fee. From a merchandising perspective, we're excited for shoppers to see new innovation in a variety of categories, including AI-powered devices like Microsoft (NASDAQ:MSFT) CoPilot and Windows 11 computers, the latest in virtual and mixed reality with Meta (NASDAQ:META) Quest 3 or Ray-Ban Meta Smart Glasses, immersive audio with Bose Quiet Comfort ultra-headphones and more, and we can help our holiday shoppers take advantage of this new innovation through our trade-in program, which gives the customer value for their old technology. In addition to great deals for our flagship categories like computing, home theater and gaming, that feature our unique ability to showcase higher-end technologies at great value, we also have an expanded assortment of new and growing categories, including e-transportation, health and wellness and outdoor living. Our e-transportation assortment has more options for people of all ages and skill levels. We have twice as many outdoor cooking brands compared to last year and more than 5,000 health and wellness products, including a lineup of fitness, recovery, beauty, skin care, baby tech and more. As you can likely hear, we are very excited to provide customers an amazing experience this holiday season. Of course, the macro environment remains uncertain with some tailwinds and increasingly more headwinds, all contributing uneven impacts on consumers. The job market remains strong and upper income and older demographics, in particular, continue to benefit from excess savings. Overarchingly, the consumer is still spending. But as we have said before, they are making careful choices and trade-offs right for their households. Given the sustained inflationary pressure on the basics, like food, fuel and lodging and the ongoing preference towards services spending, like restaurants, concert tickets indications. Additional indicators have continued to soften, including declining consumer confidence increasing debt and waning savings, and we saw sales trends soften as we move through the quarter. This environment continues to make it challenging to predict shopping behavior even during the most exciting time of the year. While we are lowering our Q4 sales outlook, we have a wide range to allow for a number of scenarios and the mid- to high end of the range reflects sequential improvement. As we discussed on our last call, there are several factors supporting our belief that our Q4 year-over-year comparable sales can improve. We expect home theater year-over-year performance to improve as we expect to be better positioned with inventory across all price points and budgets than last year. We are starting to see signs of stabilization in our TV units as they grew in Q2 and Q3 and are expected to grow in Q4. We expect performance in our computing category to improve as we build on our position of strength in the premium assortment. Notebook units were flat compared to last year in Q2, down as expected in Q3 and expected to be up slightly in Q4 and we expect to see continued growth in the gaming category as inventory is more readily available and there are strong new software titles. In summary, while the macro and industry backdrop continues to drive volatility, we have a proven track record of navigating well through dynamic and challenging environments, and we will continue to adjust as the macro conditions evolve. And we remain incredibly confident about our future opportunities. After two years of declines, we believe the consumer electronics industry should see more stabilization next year and possibly growth in the back half of the year. While our existing product categories have slightly different timing nuances, we believe they are poised for growth in the coming years, benefiting from a materially larger installed base and the ongoing desire and need to replace technology as it ages. Much of this replacement is spurred by innovation, and in addition, we continue to see several macro trends that should drive opportunities in our business over time, including cloud, augmented reality, expansion of broadband access and, of course, generative AI, where we know our vendor partners are working behind the scenes to create consumer products that optimize this material technology advancement. Our purpose to enrich lives through technology is more relevant today than ever. We're the largest CE specialty retailer. We continue to hold one-third of the market share in both the US computing and television industries and we can commercialize new technology for customers like no one else. With that, I would like to turn the call over to Matt for more details on our third quarter results our fiscal '24 outlook.\\nMatt Bilunas: Good morning, everyone. Let me start by sharing details on our third quarter results. Enterprise revenue of $9.8 billion declined 6.9% on a comparable basis. Our non-GAAP operating income rate of 3.8% declined 10 basis points compared to last year. Non-GAAP SG&A dollars were $57 million lower than last year, and it increased approximately 100 basis points as a percentage of revenue. Partially offsetting the higher SG&A rate was a 90 basis point improvement in our gross profit rate. Compared to last year, our non-GAAP diluted earnings per share decreased 6.5% to $1.29. When viewing our performance compared to our expectations, we did not see the sequential improvement versus the second quarter that our third quarter outlook assumed. From an enterprise comparable sales phasing perspective, August decline of approximately 6% was our best performing month, with September down 7% and October down 8%. Although our sales were below plan, our non-GAAP operating income rate exceeded our outlook by approximately 40 basis points, which was driven by lower SG&A. The lower-than-expected SG&A was largely driven by tighter expense management in areas such as store payroll and advertising expense as we adjusted plans to account for sales trends. Our gross profit rate was essentially flat to our expectations. Lastly, approximately $20 million of vendor funding qualified to be recognized as an offset to SG&A while our outlook assumed it would have been a reduction of cost of sales. We anticipate similar recognition of this funding in Q4 in the range of $15 million to $20 million. Next, I will walk through the details of our third quarter results compared to last year. In our Domestic segment, revenue decreased 8.2% to $9 billion, driven by a comparable sales decline of 7.3%. From a category standpoint, the largest contributors to the comparable sales decline in the quarter were appliances, computing, home theater and mobile phones, which were partially offset by growth in gaming. From an organic perspective, the overall blended average selling price of our products was essentially flat to last year, which is a slight improvement relative to the past few quarters. In our International segment, revenue decreased 3.4% to $760 million. This decrease was driven by a comparable sales decline of 1.9% and a negative impact of foreign exchange rates. Our Domestic gross profit rate increased 100 basis points to 22.9%. The higher gross profit rate was driven by the following. First, improvement from our membership offerings, which included a higher gross profit rate in our services category. Second, our product margin rates improved versus last year, including a higher level of vendor-supported promotions and the benefit from optimization efforts across multiple areas. And third, lower supply chain costs. Before moving on, I would like to give some additional context on the profit sharing revenue from our credit card arrangement, which performed better than we expected in the third quarter. On a year-over-year basis, the profit share has been approximately flat from a dollar perspective over the course of the year, which has resulted in a slightly positive impact to our gross profit rate. In the fourth quarter, we expect the profit share to come in better than we had expected and once again be very similar to last year from a dollar perspective. As we look to next year, we expect the credit card profit share to be a pressure to our gross profit rate. At this point in time, we expect this pressure to be offset by continued financial improvement from our membership offerings. Moving to SG&A. Our Domestic non-GAAP SG&A declined $58 million with the primary drivers being lower store payroll costs and reduced advertising, which were partially offset by higher incentive compensation. Next, let me touch on our inventory balance. Similar to last year at this time, we continue to feel good about our overall inventory position as well as the health of our inventory. Our quarter-end inventory balance was approximately 4% higher than last year's comparable period. As we noted during last year's third quarter earnings call, approximately $600 million of inventory receipts came in a few days later than we had expected, moving from October into November. Adjusting for that timing shift, this year's ending inventory balance would have been approximately 4% lower than last year's targeted ending balance. Year-to-date, we've returned a total of $873 million to shareholders through dividends of $603 million and share repurchases of $270 million. We now expect share repurchases of approximately $350 million for the year. Let me next share more color on our outlook for the year, starting with our thoughts on the fourth quarter. From a top line perspective, we now expect our fourth quarter comparable sales to be down in the range of 3% to 7%. Our Enterprise comparable sales through the first three weeks of November are near the low end of the fourth quarter range. On the profitability side, we expect our fourth quarter non-GAAP operating income rate to be in the range of 4.7% to 5%, which compares to a rate of 4.8% last year. Our fourth quarter gross profit rate is expected to improve versus last year by approximately 30 basis points. Although favorable to last year, the year-over-year improvement is less than the 90 basis points of expansion we reported for the third quarter. From a sequential standpoint, there are three main items I would highlight that are expected to reduce the rate expansion in the fourth quarter relative to the third quarter. First, although it is still a benefit compared to last year, the changes to our membership offering are less impactful in the larger holiday quarter. Second, product margin rates are expected to be closer to flat to last year in the fourth quarter compared to a benefit in the third quarter. And third, we expect supply chain cost to be a slight pressure in the fourth quarter versus a benefit in the third quarter. From an SG&A standpoint, when comparing to last year, we expect our fourth quarter SG&A as a percentage of sales to be more favorable than our year-to-date trends, which is due in part to the impact of the extra week this year. The range of SG&A implied in the fourth quarter incorporates our normal course of actions to adjust variable expenses under the different revenue scenarios as well as adjustments to incentive compensation align with our expected financial outcomes. As a reminder, we expect the extra week to add approximately $700 million of revenue, which is excluded from our comparable sales and $100 million in SG&A, we still expect it to benefit our full year non-GAAP operating income rate by approximately 10 basis points. Let me provide more details on our full year guidance, which incorporates the color I just shared on the fourth quarter. We now expect the following. Enterprise revenue in the range of $43.1 billion to $43.7 billion, Enterprise comparable sales to decline 6% to 7.5%, Enterprise non-GAAP operating income rate in the range of 4% to 4.1%, non-GAAP diluted earnings per share of $6 to $6.30, non-GAAP effective income tax rate of approximately 24%, and lastly, our interest income is still expected to exceed interest expense this year. Our full year gross profit and SG&A working assumptions remain very similar to what we shared last quarter. And some of the key callouts are the following. We still expect our gross profit rate to improve by approximately 60 basis points compared to fiscal '23. A large driver of the gross profit rate improvement is expected to come from our membership offerings, which includes a higher gross profit rate in our services category. Our membership offerings are now expected to provide approximately 35 basis points of improvement. At the midpoint of our guidance, we expect SG&A as a percentage of sales to increase by approximately 95 basis points compared to last year. We expect higher incentive compensation as we lapped up very low levels last year. The high end of our guidance now assumes incentive compensation increases by approximately $140 million compared to fiscal '23. I will now turn the call over to the operator for questions.\\nOperator: Thank you. [Operator Instructions] Your first question comes from the line of Simeon Gutman of Morgan Stanley. Your line is open.\\nSimeon Gutman: Good morning, everyone. I wanted to ask a question, as we get into the fourth quarter, it looks like we'll have negative comps, and it will be the third year in a row. As you step back, I think there's logic to this massive pull forward and there is a larger installed base. There should be a replacement cycle. But I wanted to kind of question that. And if it throws any water on this, if there's something else happening here, maybe there is a lack of newness. You mentioned that you didn't lose much share, but thinking about market share as well, but thinking about the cycles and whether we're not -- whether we could be in just a negative industry cycle for a little bit longer.\\nCorie Barry: Thanks for the question, Simeon. I think we have a few things going on to your point. So if you think about the comments that I had in the macro section, you've got a variety of stacked issues happening. One is absolutely you had pull forward throughout the pandemic. Two is you also have this kind of sustained inflation. And again, it's sustained inflation on the basics that we've been talking about food, fuel and lodging. And so that's pulling people. We also talked about the fact that a lot of spend right now is geared towards the more service type of things like concerts, like trips. Everywhere you look, people are taking more and more vacations. And so you have all of this kind of shift of spend that's happening. I think secondarily, as it relates specifically to the holiday time frame, the other interesting thing is people have also been buying CE a little bit more steadily throughout the year. If you think about CE as more of a need-based item, not just that kind of giftable item, and so I think there's also just been a little bit of a shift in where people are spending. But I think broadly, what we're seeing reflected right now is the kind of culmination of not just pull forward, but all of these other factors that we're seeing from the consumer as they make those trade-offs. And we've been using the words uneven for probably six quarters now, and I think that is what you're seeing in the variety of results from consumers and where they're choosing to spend their dollars.\\nSimeon Gutman: Maybe the quick follow-up is the Q4, I guess, you're running at the low end. Does it get better because the comparison gets better? Or you're hopeful around how the big holiday sales end up playing out?\\nMatt Bilunas: Yeah. I think the comparison certainly gets better. As you think about last year, we were about 3% lower than FY '20 levels. This year, we're lower against FY '20, but the sequential is better in Q4. I think there's still a lot of optimism for holiday. I think there's a lot of great holiday promotions and events. And I think we're trying to temper any expectation on holiday just with the pragmatic view of where the consumer is at right now. And I think you commented on the share. I think we actually -- we say largely held share because it's really hard to actually get a good meaningful number on share, but we feel good about our share position as we go into the holiday period.\\nSimeon Gutman: Okay. Thanks. Happy Thanksgiving. Good luck.\\nOperator: Your next question comes from the line of Chris Horvers of JPMorgan. Your line is open.\\nChris Horvers: Thanks, and good morning. So, thanks for the commentary around the credit card headwind that you're thinking about next year being offset by the membership. Can you talk about implicitly what you're assuming as a headwind in that comment, there's a lot of speculation. There's a lot of numbers getting thrown around in the market. And so I just want to try to understand what you're implicitly assuming? And then in addition, what are the other big puts and takes in gross margin as you think about 2024, as you think about initiative spending as well as your health efforts?\\nMatt Bilunas: Sure. Thanks for the question. For credit card next year, I mean, obviously, there's a number of scenarios we're trying to understand as you think about next year. So we're not really guiding next year now. But when we think about the credit card, that pressure that we expect to see, we believe, will largely be offset by benefits we might see from the membership program and services category expanding a bit more from a gross property perspective. The factors within the credit card, one of the biggest things we're trying to understand is just where do net credit losses go. They're -- at the moment, they're pretty close to where they were pre-pandemic. They were very low levels during the pandemic. And so we've been seeing them grow a little bit. And so the question would be how high did those grow if they do grow into next year and what sort of pressure. The other factor to consider is that, generally speaking, our receivable balance is higher than it used to be over the last several years. And so a higher receivable balance and interest income obviously can offset some of those pressures as well. So those are a couple of the bigger things we're trying to understand as we think about the credit card specifically. And as you mentioned, again, for next year, the other puts and takes, again, we're not guiding next year, but that credit card pressure and the membership benefit is one of the bigger factors we're trying to understand. Another one that I would call out would be -- we know that we're going to likely have to add STI expense back in as we're -- we've lowered the expense this year as we reset STI in the coming year to roughly $85 million that we would likely add back. Clearly, where the industry is a question as well to the extent that sales are flat or up, it helps relieve some of the pressure of some fixed costs. Clearly, the level of pressure matters quite a bit next year. If it's a small increase, small decline, it's less significant than is a bigger decline. So those are some of the bigger factors we're trying to think through as we go into next year.\\nChris Horvers: So that's a perfect segue. On the SG&A side, Corie, I know you talked about your NPS scores with purchasing customers and what you're seeing in the store, you've caught what feels like a lot of labor over the past few years or past couple of years. Are there any metrics that you're seeing, whether it's non-purchasing customers, like close rates versus people walking indoor that are concerning to you? And then as you think about '24, given that you've comped negatively for this sustained period, is there just less flexibility to manage the labor component?\\nCorie Barry: So I alluded to NPS being one of the factors that we -- you can imagine, there is a broad array of both operational and then more survey-based metrics that we're looking at, everything from how fast can we do an in-store pick, how good is the curbside experience. We specifically talked about and we can see meaningful improvements year-over-year in product availability, in associate availability, in a variety of products and pricing, and those have continuously improved even as this year has gone on. And actually, we can also see some level of improvement in some of those through non-purchasers as well. So we're watching both sides of this and that sequential improvement is happening across both purchasers and non-purchasers. And yes, we're watching close rate too, and the team is doing a really nice job measuring themselves and showing some progress against their close rate expectations as well. So we are -- literally, we have the almost Rubik's cube of operational and customer survey-based metrics so we can assess. I think what the team has done a really amazing job at is your point around flexibility. You talked about do you have less. Interestingly, now we have associates who can opt into and get certified in not just multiple areas of expertise within the store, but they can also get certified for operations roles and sales roles, and they can actually move between stores within their markets. And so we can flex not just against what's the consumer demand at the highest level, we can actually flex within a market depending on how and where people are choosing to shop. So I can even use an example like in the last week, we've seen a lot more people opting into in-store pickup and curbside and needing a bit more ship from store, and we can quickly then shift some of that labor into those areas, while still trying to strike the balance. We also talked about even moving some of the ship from store out of stores using those automated facilities so that when we do have labor in the stores, it's more customer-facing. It's facing more some of these key areas where we're trying to deliver these experiences. So not perfect and certainly not going to be perfect every single day at every single location, but we really are working hard, and I give the team a great deal of credit for every day monitoring both the experiences and the operational metrics that will tell us whether or not we're delivering.\\nChris Horvers: Got it. Have a great Thanksgiving.\\nCorie Barry: Thanks, you too.\\nOperator: Your next question comes from the line of Peter Keith of Piper Sandler. Your line is open.\\nPeter Keith: Hey, thanks. Good morning everyone. Happy holidays. Nice to see the membership program changes coming a bit more accretive than initially guided. Could you help us unpack that a little bit in terms of the drivers, if it's just removing the free installation or maybe that middle tier is trending a little more profitable than you thought. Maybe curious on what the uptick is from?\\nMatt Bilunas: Sure. The main drivers of the improvement from a rate perspective are -- there's basically four main areas. The first is the point change to the three My Best Buy program. The second would be just the growth in paid members over time and the recognition of those annual fees. The changes we made to the total tech program, moving it to Total, mainly came through with lowering the cost to fulfill as we removed the free installation that also was part of the 35 basis points, and the resumption of appliance at home theater installation, paid insulation is the other part of the number. The main drivers of it coming better than our expectations are around higher-than-expected paid installation volumes and then also lower-than-expected Best Buy claims and lower Apple (NASDAQ:AAPL) premiums than we had expected.\\nPeter Keith: Okay. That's helpful, Matt. And then -- and Corie, I guess everyone is very curious on product innovation and understanding we're kind of in this air pocket with very little innovation. But I'm curious are there any little green shoots that you're seeing in stores, maybe smaller products that we're not thinking about that give us some optimism that newness can drive sales?\\nCorie Barry: Yeah. I actually -- I mean, I might be biased, but I think there's a lot of green shoots that are out there. And you're right, back to the -- one of the first questions, definitely what has also caused the pullback in CE, and I didn't hit it, to begin with, I'll hit it now, is just a bit of a lack of innovation when everyone was trying so hard to produce as much as possible or pull back as hard as possible, we just haven't seen it. Now we are starting to see a little bit of that turn toward innovation. What we can see, even in TVs, we can see there's a lot more interest in those large screen sizes. We can see growth in the like 77-inch plus kind of categories where people want to get that newness. We actually have double the amount of SKUs in the 97-inch and above TV category, which I know sounds insane, but those are really interesting things to people from a true entertaining at home perspective. In majors, there's a brand-new washer/dryer combo unit from GE, so you can both do the washing and the drying in one unit, which is a really interesting innovation for people like me who might want to do two loads at once, full time and get through it all. In gaming, you can see there's really good availability of consoles, with some really interesting new titles that are driving some demand, some handheld gaming from ASUS and Lenovo. Those are great. And then there's kind of some smaller just interesting things. We talked about the Meta Quest 3, the Meta Ray-Ban sunglasses and not only can you capture pictures but has audio built in. And then I think there's lots of just really small fun giftable things, right? There's everything from the automated bird feeder to the automated litter box and everything in between. So what's cool and the reason a little tongue in cheek, we mentioned the Best Buy sells that is there are actually a ton of really interesting fun consumer technology devices. And to your point, they're kind of small, but they're starting to lead the way into what I think will be more meaningful cycles as we head into the back half of next year. As you think about, we mentioned generative AI and products and importantly, chips that geared toward running those kind of large language processing models. And you can imagine that will extend not just into computing, but into other areas. And we can't always talk about everything that we can see on the horizon, but we definitely can see some interesting products as our vendors, as you all know, are just as incented to stimulate demand as we are.\\nPeter Keith: Very good. Good luck with the holiday season.\\nCorie Barry: Thank you.\\nOperator: Your next question comes from the line of Mike Baker of D.A. Davidson. Your line is open.\\nMike Baker: Okay. Great. Thank you. And this was sort of touched on, but the promotional activity, I think you said it's up. Where is it versus plan? Do you expect it to get more promotional as we get through the holiday season? And you said this year, it will be more traditional, i.e., Black Friday, Cyber Monday, the last few weeks, et cetera. Can you remind us how the holiday played out last year?\\nMatt Bilunas: Sure. I think strategically -- I think we've done a really good job of managing our promotional plan overall. I think the promotions in terms of the discounts and mix of promotions are up versus last year, and in many cases, up compared to where they were pre-pandemic. Again, it hasn't necessarily manifested in our pressure on our product margin rates because we're still receiving a good amount of funding from our vendors to help stimulate the sales that you would expect us to want to do. I think as you look about the holiday season, I think we are expecting the holiday to be a very sales-driven event. Consumers are looking for deals, and they're looking for value. And because of that, we believe it will look probably more closely to like it was pre-pandemic, where people are gravitating towards the big sale events around Thanksgiving and Cyber Monday and a couple of weeks before Christmas. So a pretty similar cadence to what we saw in FY '20, although in FY '20, we did -- didn't have as much pull forward into October as we likely still have in this current year. So a more similar cadence to promotional events. I would expect holiday always to be promotional, and we are well positioned to be promotional and still maintain a great profitable story for our investors. So overall, I think we're in a great spot.\\nCorie Barry: And just to be explicit, what we actually had said, the promo environment was as expected. It was in line with our expectations in Q3. And you can imagine we're kind of taking -- what we're seeing and pushing that into Q4, but it hasn't been wildly outside our expectations.\\nMike Baker: Got it. Okay. Thank you. If I could ask one more, and maybe you can't answer this, but you did talk a lot about some crowding out in that kind of dynamic with the higher inflation. Well, now all of a sudden, the inflation concern is turning to deflation concern. Asking you to look into your crystal ball, how that could impact your sales results next year if the inflation goes away and we're more in a deflationary environment?\\nCorie Barry: Yeah. I mean we've been pretty consistent as we've talked about the effects of inflation. We've been pretty consistent in saying, where it's putting pressure on the consumer is because it's in those key basic areas of need, fuel, food, lodging, consumables like the stuff you just kind of need every single day. And that's what's been eating into a lot of that pent-up savings, especially for some of the lower income demographics. And so if you start to get into a world where you see more disinflation in some of those areas, then as you would expect, you start to free up some of that share of wallet for potentially getting back into goods or some of the kind of higher ticket purchases. And so we're watching that carefully. Right now, still very elevated versus especially pre-pandemic, slowing down, and to your point, people start to talk about it, which over time, I think, could present some opportunity for people to move back into the goods space, also, of course, depending on how elevated that spend remains around services and things like vacations and spending outside the home.\\nMike Baker: Yeah. Makes perfect sense. Okay. Thank you. Appreciate the color.\\nCorie Barry: Thanks.\\nOperator: Your next question comes from the line of Steven Zaccone of Citi. Your line is open.\\nSteven Zaccone: Great. Good morning. Thanks very much for taking my question. I wanted to ask a question on average selling prices. So it sounds like it was flat, slight improvement. What drove that improvement on a sequential basis by category? And then as you think about the fourth quarter, can you talk about your outlook for units versus ASPs?\\nMatt Bilunas: Yeah. I -- we'll get into the by category improvement to ASPs. Generally speaking, we are starting to, I would say, lap some of the ASP reduction. We've been seeing ASPs slowly get lower, also the last number of quarters. I think we're starting to lap some of that deflation in that average selling price, if you will. So I think it's probably as much as that as people are gravitating to in some cases, we mentioned that TV is an area of trade down that we are actually seeing. And so those do tend to lower your ASPs because it's a big ticket item. And as we start to lap that, I think you're starting to see some relief on the ASP sequentially. Again, I think in certain areas, so in terms of like Q4, clearly, we've been seeing unit pressure overall, but there are some areas where some of our bigger categories we are expecting the units to improve. We're expecting TV units to increase. We're expecting to see improvements in notebook units as well. So it's a little bit varied, but those are some of the bigger ones.\\nSteven Zaccone: Okay. Great. Thanks. And then Corie, I had a question. Just thinking about next year, I think you alluded to more stabilization and the potential for growth in the back half. I guess I was curious, how do you see the recovery playing out? We're waiting for the tech refresh cycle. But if the overall promotional environment stays challenging, how do you think about the recovery from market share position or maybe if the consumer is willing to trade down, how are you positioned to outpace the industry overall?\\nCorie Barry: So if I think about how the last year has played out, this industry has largely been in a very promotional stance for over the last year. We've been pretty consistent in saying promos are back to, if not greater, than FY '20 levels. So this is not a new phenomenon for us. So even as we head into next year, we're lapping that. And even in that environment where you've seen that level of promotionality, as Matt said, we've sustained our share position. So I think the team has done a beautiful job positioning us well in a very promotional environment. And I wouldn't be surprised to see that environment continue into the first part of next year. And again, we're lapping that kind of similar environment last year. So it's not a huge change in trajectory for us. I think what starts to make the back half, in our view, potentially more interesting next year is really a function of the innovation cycles. And we can start to see a line of sight toward even read a little bit about, especially on some of the computing and processing side, devices that might start to feed into that as we head into the back half of next year. And back to Peter's earlier question, we can start to see on the horizon, some of that newness and innovation really on the docket as you head into the back half and into holiday for next year as everyone again, it's pretty incented to want to bring some vitality back to the industry.\\nSteven Zaccone: Thanks very much for the detail. Have a nice Thanksgiving.\\nCorie Barry: Yeah. Thanks. You too.\\nOperator: Your next question comes from the line of Jonathan Matuszewski of Jefferies. Your line is open.\\nJonathan Matuszewski: Great. Thanks for taking my question. First one is on the competitive landscape. So you held market share in 3Q, and that's consistent with your comments in the first half. Obviously, you guys have superior customer service and assortment. So what's driving the success among competitors in the industry, who you're tracking who are taking share? Is it purely a function of price? And how is that informing your pricing strategy over the next couple of quarters?\\nCorie Barry: Again, I'm probably biased, but I don't think it's purely a function of price. I think we've been very clear, we have to be price competitive, and that is one of the base tentpoles of our strategy. And that said, we also, I think, have a team that has a proven track record of very adept promotional planning around key drive times, whether that's some of the secondary holidays or whether it's the main holiday that we're headed into. So I kind of think of price as the like primary tentpole. But in order to differentiate, I think what we're doubling down on is what we do, that is different than anyone else just given who we are. We are agnostic to the customer. So we don't care what the operating system is or who makes the hardware. We're there for the customer to help them to build on that. We have what we like to call human-enabled services. So we can help you in the store. We can consult for you in your home. We can repair. We can take back. We can trade in. You can buy open box. You can go to an outlet. Like, we just have the huge end-to-end variety of solutions all the way from inspire to support, so that's the kind of second differentiator for us. And then third, I think we're building on those things with a unique membership program with unique offers that reach out to our members with a membership program that's based on the things that we uniquely do well. And then fourth, I have to give major credit to our vendor partners as well, even though we're in a little bit of a slower innovation cycle, they remain closely committed to our success, which means we do have everything from the most new beautiful 98-inch TV that's out on the floor, all the way to those opening price point Chromebooks or opening price point televisions that might be right for you at a value play. And I think our ability to showcase those high and new experience as well as all the way through the rest of the assortment really is that last differentiating piece for us.\\nJonathan Matuszewski: That's great color. Thanks so much. And then a quick follow-up on Best Buy Health. You've had some exciting announcements on that side of the business in terms of partnerships in the industry. At the Investor Day, I think you called out expectations for that to grow at a CAGR of an impressive 40% over the next couple of years. Is that business at scale to switch from kind of dilution to accretion in terms of the overall enterprise next year? Any thoughts there would be helpful.\\nCorie Barry: Yeah. So we remain really excited about the Health business, and we were pretty clear that we had pulled the FY '25 targets on the whole or as the macro backdrop has changed. And so we are, of course, working behind the scenes to really fortify that business for the future. And I know someone had asked earlier as we think about the puts and takes for next year, we would continue to expect Health to become more accretive, and we laid that out as kind of our structural thesis at our Investor Day. And that part of the thesis remains true for us. And while it still is relatively small at this point, we are seeing some nice uptick, particularly in that kind of care-at-home side of things, where we've announced partnerships with Geisinger and with Atrium Health as we think about how we can use our unique Geek Squad assets as well as the unique product assortment that we have to help deliver care at home. So again, relatively small, but the team is doing a nice job continuing to ensure that, that part of the business is accretive and grows over time.\\nJonathan Matuszewski: Thanks so much.\\nOperator: Your next question comes from the line of Steven Forbes of Guggenheim. Your line is open.\\nSteven Forbes: Good morning, Corie, Matt.\\nCorie Barry: Good morning.\\nSteven Forbes: Matt, you briefly mentioned 15 to 20 basis points of vendor funding being recorded in expenses. Curious if you can maybe give us a little more color there? And then any sort of different way of thinking about how vendor funding maybe supports the margin outlook for 2024? Or are you changing the 2024 margin color of being able to hold margin in a flat sales environment, any update there?\\nMatt Bilunas: Sure. Yeah. So first of all, it was $15 million to $20 million of impact on net basis points, just to make sure I'm clear. And that would carry on as you get into next quarter Q4 and the first part of next year. And this is strictly a geography. There is no change to the overall financial statements, if you will, just moving as a cost -- offsetting a cost of sales to offsetting SG&A. Essentially, we get any number of types of vendor funding for a number of different things. And when we can actually be more specific with the funding, matching and offsetting the specific cost, we then record that as an offset to SG&A versus offsetting cost of sales. So that's specifically what's happened. And it's just -- it's part of the funding that we get not all of it, obviously. And so we would expect that to continue. To your second question, as you look at next year, at this point, we're not guiding next year, but we would expect product margins to be somewhat of a neutral impact to next year. Overall, we don't, at this point, see a lot of material changes either way. We have a very strong relationship with our vendors, and they are obviously as interested in us in stimulating sales and showcasing their products and innovations that they have. So at this point, we don't see any change to that as we look into next year.\\nCorie Barry: Matt hit on this, but I want to underscore, the way in which our vendors participate with us varies as you would expect, depending on what we're seeing in the macro. Sometimes that shows up as more promotional partnership. But a lot of times, that shows up in very different ways it can be in how we think about specialized labor, it can be in store experiences like we mentioned on the call, it can be in our Best Buy ads business or in supply chain fulfillment or in services. And I think what's important is our overall level of invested support has grown in the aggregate even as we compare it to pre-pandemic levels. And I think that is the part that for us as important is how can we be the very best partner to our vendors as we collectively want to bring, especially some of this newer innovation to market.\\nSteven Forbes: Thank you, Corie and Matt. Maybe just a quick follow-up for you, Corie. Any updated thoughts on maybe some of your newer growth initiatives such as device life cycle management, really just trying to think through whether the current sort of operating performance or challenges that are out there are impacting the investments you plan to put behind some of these initiatives? Or whether that's still sort of a growth sort of plan for next year?\\nCorie Barry: Yeah. As it relates -- you hit on specifically device life cycle management, I'm maybe going to take it up one level and that is, we've talked about Geek Squad as a service, because it can be everything from device cycle management, which is newer side of this, but also just providing service on behalf of vendors as you think about being an Apple authorized service provider or some of our Best Buy business offerings where we actually use our service profile to go out and do installations writ large. What's nice about an initiative like this is it doesn't require, especially in the earlier stages, much incremental investment. We already have Geek Squad City, which is a very large facility, well staffed with trained experts who we can leverage some of their capacities in order to deliver on something like device life cycle management. Now then we can make decisions as something like that ramps. We didn't mention it this quarter because in Q4, honestly, it's not the biggest front and center area of focus. But you can also imagine behind the scenes, if there are other ways for us to leverage our existing expertise and capacity. Those are very interesting strategic initiatives for us. And we remain excited about this one. We remain excited about the pipeline that we're seeing in this one. And obviously, I think you can expect that we will update you with more clarity as it develops. So with that, I think that -- thank you. I think that is our last question, and I want to thank you all for joining us today. Thank you for the nice wishes. I hope you all also have a wonderful holiday, and we look forward to updating you all on our results and progress during our next call in February. Thank you, and have a great day.\\nOperator: This concludes today's conference call. You may now disconnect.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " 'By Harshita Mary Varghese (Reuters) - Broadcom (NASDAQ:AVGO) on Wednesday closed its $69 billion acquisition of cloud-computing firm VMware (NYSE:VMW) after receiving regulatory approval in last major market China and ending a months-long saga. The deal, one of the biggest globally when announced in May 2022, was the latest in CEO Hock Tan\\'s efforts to boost the chipmaker\\'s software business.  However, the transaction faced tough regulatory scrutiny across the world and the companies had delayed the closing date three times. China\\'s regulatory approval came through on Tuesday after ongoing tensions with the U.S. around tougher chip export control measures had stoked fears among some investors on the company\\'s ability to close the deal before the Nov. 26 deadline.  \"The improved mood music after the meeting between China\\'s President Xi Jinping and U.S. President Joe Biden earlier this month helped to settle remaining nerves,\" Danni Hewson, head of financial analysis at AJ Bell, said on Tuesday, after the companies said they planned to close the transaction on Nov. 22. The European Commission had approved the acquisition after Broadcom offered remedies to help rival Marvell (NASDAQ:MRVL) Technology while the UK\\'s Competition and Markets Authority (CMA) gave its green light following an in-depth investigation. \\n\"Perhaps we will see some boards being willing to move forward now that we have seen the (Activision Blizzard (NASDAQ:ATVI)) and (VMware) get blessing, but don\\'t think we can count on it,\" said Cabot (NYSE:CBT) Henderson, market strategist at JonesTrading, on Tuesday.  Big Tech mergers such as Microsoft (NASDAQ:MSFT)\\'s now-closed $69 billion purchase of the \"Call of Duty\" publisher Activision have faced heightened regulatory pressure from the U.S. Federal Trade Commission under its Chair Lina Khan.',\n",
       " None,\n",
       " None,\n",
       " 'The U.S. securities regulator sued Kraken alleging it violated a slew of Meanwhile, The U.S. Department of Justice is negotiating with Binance to resolve its investigation of the crypto exchange. Sam Altman and Greg Brockman will join Microsoft (NASDAQ:MSFT) after their outing from OpenAI.SEC sues Kraken alleging slate of securities law violationsThe Securities and Exchange Commission has sued Kraken, saying it commingled customer funds and failed to register as a securities exchange, broker, dealer and clearing agency.DOJ is ready settle with Binance for $4 billion: Report\\nMicrosoft hires Sam Altman, Greg Brockman after departure from OpenAI\\nContinue Reading on Cointelegraph',\n",
       " 'By Blake Brittain (Reuters) -OpenAI and Microsoft (NASDAQ:MSFT) were sued on Tuesday over claims that they misused the work of nonfiction authors to train the artificial intelligence models that underlie services like OpenAI\\'s chatbot ChatGPT. OpenAI copied tens of thousands of nonfiction books without permission to teach its large language models to respond to human text prompts, said author and Hollywood Reporter editor Julian Sancton, who is leading the proposed class action filed in Manhattan federal court. The lawsuit is one of several that have been brought by groups of copyright owners, including authors John Grisham, George R.R. Martin and Jonathan Franzen, against OpenAI and other tech companies over the alleged misuse of their work to train AI systems. The companies have denied the allegations. Sancton\\'s complaint is the first author lawsuit against OpenAI to also name Microsoft as a defendant. The company has invested billions of dollars in the artificial intelligence startup and integrated OpenAI\\'s systems into its products. A spokesperson for OpenAI declined to comment on the Tuesday lawsuit, citing pending litigation. Representatives for Microsoft did not immediately respond to a request for comment. \"While OpenAI and Microsoft refuse to pay nonfiction authors, their AI platform is worth a fortune,\" Sancton\\'s attorney Justin Nelson said in a statement. \"The basis of OpenAI is nothing less than the rampant theft of copyrighted works.\" Sancton\\'s lawsuit said that OpenAI copied nonfiction books, including his \"Madhouse at the End of the Earth: The Belgica\\'s Journey into the Dark Antarctic Night\" to train its GPT large language models. \\nThe complaint also said that Microsoft has been \"deeply involved\" in training and developing the models and is also liable for copyright infringement. Sancton asked the court for an unspecified amount of monetary damages and a court order to block the alleged infringement.',\n",
       " \"U.S. - Microsoft Corporation (NASDAQ:MSFT) saw its shares climb after CEO Satya Nadella announced that tech entrepreneur Sam Altman will lead the company's artificial intelligence (AI) research initiatives. The news came amid broader market gains on Monday, with Wall Street buoyed by easing inflation concerns that could signal a slowdown in Federal Reserve rate hikes or even potential cuts.\\nInvestor confidence was further bolstered by the successful outcome of Monday's off-the-run 20-year Treasury auction, which indicated stronger market sentiment compared to disruptions experienced earlier in November after a cyberattack on the Industrial and Commercial Bank of China (ICBC). This positive shift aligns with previous Bank of America (BofA) survey predictions, suggesting that softer macroeconomic indicators may lead to lower bond yields.\\nIn commodities news, the Organization of the Petroleum Exporting Countries and its allies (OPEC+) reaffirmed their commitment to production cuts extending into 2024. Saudi Arabia has also decided to continue its reduced oil output until mid-2024, planning a gradual increase thereafter. These developments, coupled with anticipated economic stimulus measures from China and a reduced likelihood of aggressive U.S. monetary policy tightening, have injected momentum into commodity markets.InvestingPro InsightsMicrosoft Corporation (NASDAQ:MSFT) has been a consistent performer in the market, with a high return on invested capital and a strong track record of raising its dividend for 18 consecutive years (InvestingPro Tips). The company has also been a steady player in the software industry, offering its stockholders high returns on book equity.\\nInvestingPro's real-time data reveals that Microsoft has a market cap of 2810.0B USD and a P/E ratio of 36.5. Over the last twelve months as of Q1 2024, the company has achieved a revenue growth of 7.5% and an EBITDA growth of 9.8%. It's also worth noting that Microsoft's stock price has seen a strong return over the last month, three months, and year (InvestingPro Tips).\\nDid you find these insights helpful? InvestingPro offers a wealth of additional tips and real-time data metrics for subscribers. And right now, you can take advantage of our special Black Friday sale with a discount of up to 55%. In total, InvestingPro provides 22 additional tips for Microsoft alone, which can guide you in making informed investment decisions. Remember, knowledge is power in the world of investing.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " 'By Martin Coulter and Supantha Mukherjee LONDON (Reuters) - As the European Union edges closer to passing a wide-ranging set of laws governing artificial intelligence, lawmakers and experts say the surprise ousting of OpenAI CEO Sam Altman underscores the need for strict rules.  Altman, cofounder of the startup that last year kicked off the generative AI boom, was abruptly fired by OpenAI’s board last week, sending shockwaves through the tech world and prompting employees to make threats of a mass resignation at the company.  Across the Atlantic, the European Commission, the European Parliament and the EU Council have been hashing out the fine print of the AI Act, a sweeping set of laws that would require some companies to complete extensive risk assessments and make data available to regulators.  In recent weeks, talks have hit stumbling blocks over the extent to which companies should be allowed to self-regulate.  Brando Benifei, one of two European Parliament lawmakers leading negotiations on the laws, told Reuters: “The understandable drama around Altman being sacked from OpenAI and now joining Microsoft (NASDAQ:MSFT) shows us that we cannot rely on voluntary agreements brokered by visionary leaders.  “Regulation, especially when dealing with the most powerful AI models, needs to be sound, transparent and enforceable to protect our society.”  On Monday, Reuters reported that France, Germany and Italy had reached an agreement on how AI should be regulated, a move expected to accelerate negotiations at the European level. The three governments support \"mandatory self-regulation through codes of conduct\" for those using generative AI models, but some experts said this would not be enough.  Alexandra van Huffelen, Dutch minister for digitalisation, told Reuters the OpenAI saga underscored the need for strict rules. She said: “The lack of transparency and the dependence on a few influential companies in my opinion clearly underlines the necessity of regulation.” \\nMeanwhile, Gary Marcus, an AI expert at New York University, wrote on social media platform X: \"We can’t really trust the companies to self-regulate AI where even their own internal governance can be deeply conflicted.  \"Please don\\'t gut the EU AI Act; we need it now more than ever.\"',\n",
       " \"Wall Street enters Thanksgiving with a buoyant mood, as the Nasdaq 100 reaches a high not seen since early last year. This surge comes amid stable Federal Reserve interest rates, which have bolstered market confidence and supported equity growth across various sectors. Investors are embracing a period of relative calm as the central bank's cautious approach to rate hikes appears to align with inflation targets, according to the latest Fed minutes.\\nIn the UK, the stock market reacted positively to corporate news, with  Sage Group  (LON:SGE)'s shares jumping after the announcement of a stock buyback program, placing it among the top movers on the FTSE 100. In contrast,  Kingfisher  (LON:KGF) saw its shares take a hit following a profit warning, yet there remains potential for recovery as consumer spending is expected to increase in response to recent tax cuts.\\nU.S. Treasury yields have risen in reaction to consumer inflation expectations over the next year. Quincy Krosby of LPL Financial (NASDAQ:LPLA) has expressed concerns that these entrenched inflation perceptions could pose challenges to the Federal Reserve's policy efforts. The two-year Treasury yield has surpassed 4.9%, and the U.S. dollar experienced its most significant uptick in weeks.\\nIn tech news, Apple (NASDAQ:AAPL) is nearing a market value of $3 trillion, Amazon (NASDAQ:AMZN) benefits from a surge in pre-holiday activity, and Microsoft (NASDAQ:MSFT) sees gains following an announcement related to Sam Altman's OpenAI.\\nThe labor market is showing signs of cooling, with U.S. jobless claims decreasing. However, October's durable goods orders have taken a downturn due to reduced demand for business equipment and aircraft.\\nOil prices have dipped as OPEC+ postponed their meeting, leaving markets uncertain about potential supply adjustments.\\nOn the corporate side, Deere (NYSE:DE) & Co.'s profit projections did not meet analysts' expectations amid slowing demand. Autodesk (NASDAQ:ADSK) faced a downgrade from Piper Sandler, and disappointing earnings reports emerged for Guess?, Nordstrom (NYSE:JWN), and Urban Outfitters (NASDAQ:URBN). Virgin Galactic also grappled with a downgrade from Morgan Stanley amid forecasts of a hiatus in revenue flights between mid-2024 and mid-2026.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " \"NVIDIA's latest earnings call revealed record-breaking revenue of $18.1 billion, marking a 34% sequential increase and a 200% year-on-year increase. The company's Data Center segment was a primary driver of this growth, achieving record revenue of $14.5 billion. However, the company anticipates a significant decline in the fourth quarter due to new U.S. export control regulations affecting sales to China and other markets. \\nKey takeaways from the call include:NVIDIA's Data Center segment saw record revenue, driven by the NVIDIA HGX platform and InfiniBand networking. Major companies like Meta (NASDAQ:META) and Adobe (NASDAQ:ADBE) are integrating NVIDIA's technology into their platforms, contributing to the strong demand for AI supercomputers and data center infrastructure.NVIDIA is expanding its Data Center product portfolio to comply with new U.S. export control regulations. Despite expecting a decrease in sales to China and other markets in the fourth quarter, the company is focusing on national investment in sovereign AI infrastructure.The Gaming segment recorded revenue of $2.86 billion, a 15% sequential increase and an 80% year-on-year increase, driven by strong demand for NVIDIA's RTX ray tracing and AI technology.In the Pro Vis segment, revenue reached $416 million, a 10% sequential increase and a 108% year-on-year increase. NVIDIA RTX is the preferred platform for professional design, engineering, and simulation, with AI emerging as a demand driver.NVIDIA's CEO, Jensen Huang, discussed the expansion of AI factories and the transition into a new computing approach. He also expressed confidence in the continued growth of the Data Center business through 2025.During the call, NVIDIA highlighted its product innovations, including the H200 GPU, which offers faster and larger memory for generative AI and language models, and the GH200 Grace Hopper Superchip, a combination of an ARM-based CPU with a Hopper GPU. The Grace Hopper Superchip is being adopted by supercomputing customers, with the combined AI compute capacity of supercomputers built on Grace Hopper expected to exceed 200 exaflops next year. \\nIn addition to its product offerings, NVIDIA is also expanding its networking into the Ethernet space with its new Spectrum-X end-to-end Ethernet offering, which will be available in Q1 next year. This offering can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. \\nNVIDIA also discussed its business strategy, emphasizing the importance of reselling AI technology to customers and its growth in the enterprise market. The company introduced an AI foundry service to help enterprises build custom AI models and announced an aggressive product release schedule in the data center market. \\nLooking ahead, NVIDIA expects total revenue of $20 billion for Q4 2024, driven by strong demand in the Data Center sector. Despite regulatory challenges, the company is confident in its growth potential, citing expanded supply, new customers, and its entrance into the enterprise market.Full transcript -  Nvidia Corp  (NASDAQ:NVDA) Q3 2024:Operator: Good afternoon. My name is JL, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's Third Quarter Earnings Call. All lines have been placed on mute to prevent any background noise. After the speakers’ remarks, there will be a question-and-answer session. [Operator Instructions] Simona Jankowski, you may now begin your conference.\\nSimona Jankowski: Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2024. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter and fiscal 2024. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All statements are made as of today, November 21, 2023, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\\nColette Kress: Thanks, Simona. Q3 was another record quarter. Revenue of $18.1 billion was up 34% sequentially and up more than 200% year-on-year and well above our outlook for $16 billion. Starting with Data Center. The continued ramp of the NVIDIA HGX platform based on our Hopper Tensor Core GPU architecture, along with InfiniBand end-to-end networking, drove record revenue of $14.5 billion, up 41% sequentially and up 279% year-on-year. NVIDIA HGX with InfiniBand together are essentially the reference architecture for AI supercomputers and data center infrastructures. Some of the most exciting generative AI applications are built and run on NVIDIA, including Adobe Firefly, ChatGPT, Microsoft (NASDAQ:MSFT) 365 Copilot, CoAssist, now assist with ServiceNow (NYSE:NOW) and Zoom (NASDAQ:ZM) AI Companion. Our Data Center compute revenue quadrupled from last year and networking revenue nearly tripled. Investments in infrastructure for training and inferencing large language models, deep learning, recommender systems and generative AI applications is fueling strong broad-based demand for NVIDIA accelerated computing. Inferencing is now a major workload for NVIDIA AI computing. Consumer Internet companies and enterprises drove exceptional sequential growth in Q3, comprising approximately half of our Data Center revenue and outpacing total growth. Companies like Meta are in full production with deep learning, recommender systems and also investing in generative AI to help advertisers optimize images and text. Most major consumer Internet companies are racing to ramp up generative AI deployment. The enterprise wave of AI adoption is now beginning. Enterprise software companies such as Adobe, Databricks, Snowflake (NYSE:SNOW) and ServiceNow are adding AI copilots and the systems to their platforms. And broader enterprises are developing custom AI for vertical industry applications such as Tesla (NASDAQ:TSLA) in autonomous driving. Cloud service providers drove roughly the other half of our Data Center revenue in the quarter. Demand was strong from all hyperscale CSPs, as well as from a broadening set of GPU-specialized CSPs globally that are rapidly growing to address the new market opportunities in AI. NVIDIA H100 Tensor Core GPU instances are now generally available in virtually every cloud with instances in high demand. We have significantly increased supply every quarter this year to meet strong demand and expect to continue to do so next year. We will also have a broader and faster product launch cadence to meet the growing and diverse set of AI opportunities. Towards the end of the quarter, the U.S. government announced a new set of export control regulations for China and other markets, including Vietnam and certain countries in the Middle East. These regulations require licenses for the export of a number of our products, including our Hopper and Ampere 100 and 800 series and several others. Our sales to China and other affected destinations derived from products that are now subject to licensing requirements have consistently contributed approximately 20% to 25% of Data Center revenue over the past few quarters. We expect that our sales to these destinations will decline significantly in the fourth quarter. So we believe will be more than offset by strong growth in other regions. The U.S. government designed the regulation to allow the U.S. industry to provide data center compute products to markets worldwide, including China. Continuing to compete worldwide as the regulations encourage, promotes U.S. technology leadership, spurs economic growth and supports U.S. jobs. For the highest performance levels, the government requires licenses. For lower performance levels, the government requires a streamlined prior notification process. And for products even lower performance levels, the government does not require any notice at all. Following the government's clear guidelines, we are working to expand our Data Center product portfolio to offer compliance solutions for each regulatory category, including products for which the U.S. government does not wish to have advance notice before each shipment. We are working with some customers in China and the Middle East to pursue licenses from the U.S. government. It is too early to know whether these will be granted for any significant amount of revenue. Many countries are awakening to the need to invest in sovereign AI infrastructure to support economic growth and industrial innovation. With investments in domestic compute capacity, nations can use their own data to train LLMs and support their local generative AI ecosystems. For example, we are working with India's government and largest tech companies including  Infosys  (NS:INFY), Reliance and Tata to boost their sovereign AI infrastructure. And French private cloud provider, Scaleway, is building a regional AI cloud based on NVIDIA H100 InfiniBand and NVIDIA's AI Enterprise software to fuel advancement across France and Europe. National investment in compute capacity is a new economic imperative and serving the sovereign AI infrastructure market represents a multi-billion dollar opportunity over the next few years. From a product perspective, the vast majority of revenue in Q3 was driven by the NVIDIA HGX platform based on our Hopper GPU architecture with lower contribution from the prior generation Ampere GPU architecture. The new L40S GPU built for industry standard servers began to ship, supporting training and inference workloads across a variety of consumers. This was also the first revenue quarter of our GH200 Grace Hopper Superchip, which combines our ARM-based Grace CPU with a Hopper GPU. Grace and Grace Hopper are ramping into a new multi-billion dollar product line. Grace Hopper instances are now available at GPU specialized cloud providers, and coming soon to Oracle (NYSE:ORCL) Cloud. Grace Hopper is also getting significant traction with supercomputing customers. Initial shipments to Los Alamos National Lab and the Swiss National Supercomputing Center took place in the third quarter. The UK government announced it will build one of the world's fastest AI supercomputers called Isambard-AI with almost 5,500 Grace Hopper Superchips. German supercomputing center, Julich, also announced that it will build its next-generation AI supercomputer with close to 24,000 Grace Hopper Superchips and Quantum-2 InfiniBand, making it the world's most powerful AI supercomputer with over 90 exaflops of AI performance. All-in, we estimate that the combined AI compute capacity of all the supercomputers built on Grace Hopper across the U.S., Europe and Japan next year will exceed 200 exaflops with more wins to come. Inference is contributing significantly to our data center demand, as AI is now in full production for deep learning, recommenders, chatbots, copilots and text to image generation and this is just the beginning. NVIDIA AI offers the best inference performance and versatility, and thus the lower power and cost of ownership. We are also driving a fast cost reduction curve. With the release of TensorRT-LLM, we now achieved more than 2x the inference performance for half the cost of inferencing LLMs on NVIDIA GPUs. We also announced the latest member of the Hopper family, the H200, which will be the first GPU to offer HBM3e, faster, larger memory to further accelerate generative AI and LLMs. It moves inference speed up to another 2x compared to H100 GPUs for running LLMs like Norma2 (ph). Combined, TensorRT-LLM and H200, increased performance or reduced cost by 4x in just one year. With our customers changing their stack, this is a benefit of CUDA and our architecture compatibility. Compared to the A100, H200 delivers an 18x performance increase for inferencing models like GPT-3, allowing customers to move to larger models and with no increase in latency. Amazon (NASDAQ:AMZN) Web Services, Google (NASDAQ:GOOGL) Cloud, Microsoft Azure and Oracle Cloud will be among the first CSPs to offer H200-based instances starting next year. At last week's Microsoft Ignite, we deepened and expanded our collaboration with Microsoft across the entire stock. We introduced an AI foundry service for the development and tuning of custom generative AI enterprise applications running on Azure. Customers can bring their domain knowledge and proprietary data and we help them build their AI models using our AI expertise and software stock in our DGX cloud, all with enterprise grade security and support. SAP and  Amdocs  (NASDAQ:DOX) are the first customers of the NVIDIA AI foundry service on Microsoft Azure. In addition, Microsoft will launch new confidential computing instances based on the H100. The H100 remains the top performing and most versatile platform for AI training and by a wide margin, as shown in the latest MLPerf industry benchmark results. Our training cluster included more than 10,000 H100 GPUs or 3x more than in June, reflecting very efficient scaling. Efficient scaling is a key requirement in generative AI, because LLMs are growing by an order of magnitude every year. Microsoft Azure achieved similar results on a nearly identical cluster, demonstrating the efficiency of NVIDIA AI in public cloud deployments. Networking now exceeds a $10 billion annualized revenue run rate. Strong growth was driven by exceptional demand for InfiniBand, which grew fivefold year-on-year. InfiniBand is critical to gaining the scale and performance needed for training LLMs. Microsoft made this very point last week, highlighting that Azure uses over 29,000 miles of InfiniBand cabling, enough to circle the globe. We are expanding NVIDIA networking into the Ethernet space. Our new Spectrum-X end-to-end Ethernet offering with technologies, purpose built for AI, will be available in Q1 next year. With support from leading OEMs, including Dell (NYSE:DELL), HPE and Lenovo. Spectrum-X can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. Let me also provide an update on our software and services offerings, where we are starting to see excellent adoption. We are on track to exit the year at an annualized revenue run rate of $1 billion for our recurring software, support and services offerings. We see two primary opportunities for growth over the intermediate term with our DGX cloud service and with our NVIDIA AI Enterprise software, each reflects the growth of enterprise AI training and enterprise AI inference, respectively. Our latest DGX cloud customer announcement was this morning as part of an AI research collaboration with Gentech, the biotechnology pioneer also plans to use our BioNeMo LLM framework to help accelerate and optimize their AI drug discovery platform. We now have enterprise AI partnership with Adobe, Dropbox (NASDAQ:DBX), Getty, SAP, ServiceNow, Snowflake and others to come. Okay. Moving to Gaming. Gaming revenue of $2.86 billion was up 15% sequentially and up more than 80% year-on-year with strong demand in the important back-to-school shopping season with NVIDIA RTX ray tracing and AI technology now available at price points as low as $299. We entered the holidays with the best-ever line-up for gamers and creators. Gaming has doubled relative to pre-COVID levels even against the backdrop of lackluster PC market performance. This reflects the significant value we've brought to the gaming ecosystem with innovations like RTX and DLSS. The number of games and applications supporting these technologies has exploded in that period, driving upgrades and attracting new buyers. The RTX ecosystem continues to grow. There are now over 475 RTX-enabled games and applications. Generative AI is quickly emerging as the new pillar app for high performance PCs. NVIDIA RTX GPUs to find the most performance AI PCs and workstations. We just released TensorRT-LLM for Windows, which speeds on-device LLM inference up by 4x. With an installed base of over 100 million, NVIDIA RTX is the natural platform for AI application developers. Finally, our GeForce NOW cloud gaming service continues to build momentum. Its library of PC games surpassed 1,700 titles, including the launches of Alan Wake 2, Baldur's Gate 3, Cyberpunk 2077: Phantom Liberty and Starfield. Moving to the Pro Vis. Revenue of $416 million was up 10% sequentially and up 108% year-on year. NVIDIA RTX is the workstation platform of choice for professional design, engineering and simulation use cases and AI is emerging as a powerful demand driver. Early applications include inference for AI imaging in healthcare and edge AI in smart spaces and the public sector. We launched a new line of desktop workstations based on NVIDIA RTX Ada Lovelace generation GPUs and ConnectX, SmartNICs offering up to 2x the AI processing ray tracing and graphics performance of the previous generations. These powerful new workstations are optimized for AI workloads such as fine tune AI models, training smaller models and running inference locally. We continue to make progress on Omniverse, our software platform for designing, building and operating 3D virtual worlds. Mercedes-Benz (OTC:MBGAF) is using Omniverse powered digital twins to plan, design, build and operate its manufacturing and assembly facilities, helping it increase efficiency and reduce defects. Oxxon (ph) is also incorporating Omniverse into its manufacturing process, including end-to-end simulation for the entire robotics and automation pipeline, saving time and cost. We announced two new Omniverse Cloud services for automotive digitalization available on Microsoft Azure, a virtual factory simulation engine and autonomous vehicle simulation engine. Moving to Automotive. Revenue was $261 million, up 3% sequentially and up 4% year-on year, primarily driven by continued growth in self-driving platforms based on NVIDIA DRIVE Orin SOC and the ramp of AI cockpit solutions with global OEM customers. We extended our automotive partnership of Foxconn to include NVIDIA DRIVE for our next-generation automotive SOC. Foxconn has become the ODM for EVs. Our partnership provides Foxconn with a standard AV sensor and computing platform for their customers to easily build a state-of-an-art safe and secure software defined car. Now we're going to move to the rest of the P&L. GAAP gross margin expanded to 74% and non-GAAP gross margin to 75%, driven by higher Data Center sales and lower net inventory reserve, including a 1 percentage point benefit from the release of previously reserved inventory related to the Ampere GPU architecture products. Sequentially, GAAP operating expenses were up 12% and non-GAAP operating expenses were up 10%, primarily reflecting increased compensation and benefits. Let me turn to the fourth quarter of fiscal 2024. Total revenue is expected to be $20 billion, plus or minus 2%. We expect strong sequential growth to be driven by Data Center, with continued strong demand for both compute and networking. Gaming will likely decline sequentially as it is now more aligned with notebook seasonality. GAAP and non-GAAP gross margins are expected to be 74.5% and 75.5%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $3.17 billion and $2.2 billion, respectively. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $200 million, excluding gains and losses from non-affiliated investments. GAAP and non-GAAP tax rates are expected to be 15%, plus or minus 1% excluding any discrete items. Further financial information are included in the CFO commentary and other information available on our IR website. In closing, let me highlight some upcoming events for the financial community. We will attend the UBS Global Technology Conference in Scottsdale, Arizona, on November 28; the Wells Fargo TMT Summit in Rancho Palos Verdes, California on November 29; the Arete Virtual Tech Conference on December 7; and the J.P. Morgan Health Care Conference in San Francisco on January 8. Our earnings call to discuss the results of our fourth quarter and fiscal 2024 is scheduled for Wednesday, February 21. We will now open the call for questions. Operator, will you please poll for questions.\\nOperator: [Operator Instructions] Your first question comes from the line of Vivek Arya of Bank of America. Your line is open.\\nVivek Arya: Thanks for taking my question. Just, Colette, wanted to clarify what China contributions are you expecting in Q4. And then, Jensen, the main question is for you, where do you think we are in the adoption curve in terms of your shipments into the generative AI market? Because when I just look at the trajectory of your data center, is growth -- it will be close to nearly 30% of all the spending in data center next year. So what metrics are you keeping an eye on to inform you that you can continue to grow? Just where are we in the adoption curve of your products into the generative AI market? Thank you.\\nColette Kress: So, first let me start with your question, Vivek, on export controls and the impacts that we are seeing in our Q4 outlook and guidance that we provided. We had seen historically over the last several quarters that China and some of the other impacted destinations to be about 20% to 25% of our Data Center revenue. We are expecting in our guidance for that to decrease substantially as we move into Q4. The export controls will have a negative effect on our China business. And we do not have good visibility into the magnitude of that impact even over the long-term. We are though working to expand our Data Center product portfolio to possibly offer new regulation compliance solutions that do not require a license, these products, they may become available in the next coming months. However, we don't expect their contribution to be material or meaningful as a percentage of the revenue in Q4.\\nJensen Huang: Generative AI is the largest TAM expansion of software and hardware that we've seen in several decades. At the core of it, what's really exciting is that, what was largely a retrieval based computing approach, almost everything that you do is retrieved off of storage somewhere, has been augmented now, added with a generative method. And it's changed almost everything. You could see that text-to-text, text-to-image, text-to-video, text-to-3D, text-to-protein, text-to-chemicals, these were things that were processed and typed in by humans in the past. And these are now generative approaches. The way that we access data is changed. It used to be based on explicit queries. It is now based on natural language queries, intention queries, semantic queries. And so, we're excited about the work that we're doing with SAP and Dropbox and many others that you're going to hear about. And one of the areas that is really impactful is the software industry, which is about $1 trillion or so, has been building tools that are manually used over the last couple of decades. And now there's a whole new segment of software called copilots and assistants. Instead of manually used, these tools will have copilots to help you use it. And so, instead of licensing software, we will continue to do that, of course, but we will also hire copilots and assistants to help us use these -- use the software. We'll connect all of these copilots and assistants into teams of AIs, which is going to be the modern version of software, modern version of enterprise business software. And so the transformation of software and the way that software has done is driving the hardware underneath. And you can see that it's transforming hardware in two ways. One is something that's largely independent of generative AI. There's two trends: one is related to accelerated computing, general purpose computing is too wasteful of energy and cost. And now that we have much, much better approaches, call it, accelerated computing, you could save an order of magnitude of energy, you can save an order of magnitude of time or you can save an order of magnitudes of cost by using acceleration. And so, accelerated computing is transitioning, if you will, general purpose computing into this new approach. And that's been augmented by a new class of data centers. This is the traditional data centers that you were just talking about where we represent about a third of that. But there is a new class of data centers and this new class of data centers, unlike the data centers of the past, where you have a lot of applications running used by a great many people that are different tenants that are using the same infrastructure and that data center stores a lot of files. These new data centers are very few applications, if not one application, used by basically one tenant and it processes data, it trains models and then generates tokens and generates AI. And we call these new data centers AI factories. We're seeing AI factories being built out everywhere, and just by every country. And so if you look at the way where we are in the expansion, the transition into this new computing approach, the first wave you saw with large language model start-ups, generative AI start-ups and consumer Internet companies, and weren't in the process of ramping that. Meanwhile, while that's being ramped, you see that we're starting to partner with enterprise software companies who would like to build chatbots and copilots and assistants to augment the tools that they have on their platforms. You're seeing GPU specialized CSPs cropping up all over the world and they are dedicated to do really one thing, which is processing AI. You're seeing sovereign AI infrastructures, people -- countries that now recognize that they have to utilize their own data, keep their own data, keep their own culture, process that data and develop their own AI. You see that in India. Several -- about a year ago in Sweden, you are seeing in Japan. Last week, a big announcement in France. But the number of sovereign AI clouds that are being built is really quite significant. And my guess is that almost every major region will have and surely every major country will have their own AI clouds. And so I think you're seeing just new developments as the generative AI wave propagates through every industry, every company, every region. And so we're at the beginning of this inflection, this computing transition.\\nOperator: Your next question comes from the line of Aaron Rakers of Wells Fargo. Your line is open.\\nAaron Rakers: Yeah. Thanks for taking the question. I wanted to ask about kind of the networking side of the business. Given the growth rates that you've now cited, I think, it's 155% year-over-year and strong growth sequentially, it looks like that business is like almost approaching $2.5 billion to $3 billion quarterly level. I'm curious of how you see Ethernet involved evolving and maybe how you would characterize your differentiation of Spectrum-X relative to the traditional Ethernet stack as we start to think about that becoming part of the networking narrative above and maybe beyond just InfiniBand as we look into next year? Thank you.\\nJensen Huang: Yeah. Thanks for the question. Our networking business is already on a $10 billion plus run rate and it's going to get much larger. And as you mentioned, we added a new networking platform to our networking business recently. The vast majority of the dedicated large scale AI factories standardize on InfiniBand. And the reason for that is not only because of its data rate and not only just the latency, but the way that it moves traffic around the network is really important. The way that you process AI and a multi-tenant hyperscale Ethernet environment, the traffic pattern is just radically different. And with InfiniBand and with software defined networks, we could do congestion control, adaptive routing, performance isolation and noise isolation, not to mention, of course, the day rate and the low latency that -- and a very low overhead of InfiniBand that's natural part of InfiniBand. And so, InfiniBand is not so much just the network, it's also a computing fabric. We've put a lot of software-defined capabilities into the fabric including computation. We will do 40-point calculations and computation right on the switch, and right in the fabric itself. And so that's the reason why that difference in Ethernet versus InfiniBand or InfiniBand versus Ethernet for AI factories is so dramatic. And the difference is profound. And the reason for that is because you've just invested in a $2 billion infrastructure for AI factories. A 20%, 25%, 30% difference in overall effectiveness, especially as you scale up is measured in hundreds of millions of dollars of value. And if you will, renting that infrastructure over the course of four to five years, it really, really adds up. And so InfiniBand's value proposition is undeniable for AI factories. However, as we move AI into enterprise. This is enterprise computing what we'd like to enable every company to be able to build their own custom AIs. We're building customer AIs in our company based on our proprietary data, our proprietary type of skills. For example, recently we spoke about one of the models that we're creating, it's called ChipNeMo; we're building many others. There'll be tens, hundreds of custom AI models that we create inside our company. And our company is -- for all of our employee use, doesn't have to be as high performance as the AI factories we used to train the models. And so we would like the AI to be able to run in Ethernet environment. And so what we've done is we invented this new platform that extends Ethernet; doesn't replace Ethernet, it's 100% compliant with Ethernet. And it's optimized for East-West traffic, which is where the computing fabric is. It adds to Ethernet with an end-to-end solution with Bluefield, as well as our Spectrum switch that allows us to perform some of the capabilities that we have in InfiniBand, not all but some. And we achieved excellent results. And the way we go to market is we go to market with our large enterprise partners who already offer our computing solution. And so, HP (NYSE:HPQ), Dell and Lenovo has the NVIDIA AI stack, the NVIDIA AI Enterprise software stack and now they integrate with Bluefield, as well as bundle -- take a market there, Spectrum switch, and they'll be able to offer enterprise customers all over the world with their vast sales force and vast network of resellers a fully integrated, if you will, fully optimized, at least end-to-end AI solution. And so that's basically it, bringing AI to Ethernet for the world's enterprise.\\nOperator: Thank you. Your next question comes from the line of Joe Moore of Morgan Stanley. Your line is open.\\nJoseph Moore: Great. Thank you. I'm wondering if you could talk a little bit more about Grace Hopper and how you see the ability to leverage kind of the microprocessor, how you see that as a TAM expander. And what applications do you see using Grace Hopper versus more traditional H100 applications?\\nJensen Huang: Yeah. Thanks for the question. Grace Hopper is in production -- in high volume production now. We're expecting next year just with all of the design wins that we have in high performance computing and AI infrastructures, we are on a very, very fast ramp with our first data center CPU to a multi-billion dollar product line. This is going to be a very large product line for us. The capability of Grace Hopper is really quite spectacular. It has the ability to create computing nodes that simultaneously has very fast memory, as well as very large memory. In the areas of vector databases or semantic surge, what is called RAG, retrieval augmented generation. So that you could have a generative AI model be able to refer to proprietary data or a factual data before it generates a response, that data is quite large. And you can also have applications or generative models where the context length is very high. You basically store it in entire book into end-to-end system memory before you ask your questions. And so the context length can be quite large this way. The generative models has the ability to still be able to naturally interact with you on one hand. On the other hand, be able to refer to factual data, proprietary data or domain-specific data, you data and be contextually relevant and reduce hallucination. And so that particular use case for example is really quite fantastic for Grace Hopper. It also serves the customers that really care to have a different CPU than x86. Maybe it's a European supercomputing centers or European companies who would like to build up their own ARM ecosystem and like to build up a full stack or CSPs that have decided that they would like to pivot to ARM, because their own custom CPUs are based on ARM. There are variety of different reasons that drives the success of Grace Hopper, but we're off to a just an extraordinary start. This is a home run product.\\nOperator: Your next question comes from the line of Tim Arcuri of UBS. Your line is open.\\nTim Arcuri: Hi. Thanks. I wanted to ask a little bit about the visibility that you have on revenue. I know there's a few moving parts. I guess, on one hand, the purchase commitments went up a lot again. But on the other hand, China bans would arguably pull in when you can fill the demand beyond China. So I know we're not even into 2024 yet and it doesn't sound like, Jensen, you think that next year would be a peak in your Data Center revenue, but I just wanted to sort of explicitly ask you that. Do you think that Data Center can grow even in 2025? Thanks.\\nJensen Huang: Absolutely believe the Data Center can grow through 2025. And there are, of course, several reasons for that. We are expanding our supply quite significantly. We have already one of the broadest and largest and most capable supply chain in the world. Now, remember, people think that the GPU is a chip. But the HGX H100, the Hopper HGX has 35,000 parts, it weighs 70 pounds. Eight of the chips are Hopper. The other 35,000 are not. It is -- even its passive components are incredible. High voltage parts. High frequency parts. High current parts. It is a supercomputer, and therefore, the only way to test a supercomputer is with another supercomputer. Even the manufacturing of it is complicated, the testing of it is complicated, the shipping of it complicated and installation is complicated. And so, every aspect of our HGX supply chain is complicated. And the remarkable team that we have here has really scaled out the supply chain incredibly. Not to mention, all of our HGXs are connected with NVIDIA networking. And the networking, the transceivers, the mix, the cables, the switches, the amount of complexity there is just incredible. And so, I'm just -- first of all, I'm just super proud of the team for scaling up this incredible supply chain. We are absolutely world class. But meanwhile, we're adding new customers and new products. So we have new supply. We have new customers, as I was mentioning earlier. Different regions are standing up GPU specialist clouds, sovereign AI clouds coming out from all over the world, as people realize that they can't afford to export their country's knowledge, their country's culture for somebody else to then resell AI back to them, they have to -- they should, they have the skills and surely with us in combination, we can help them to do that build up their national AI. And so, the first thing that they have to do is, create their AI cloud, national AI cloud. You're also seeing us now growing into enterprise. The enterprise market has two paths. One path -- or if I could say three paths. The first path, of course, just off-the-shelf AI. And there are of course Chat GPT, a fabulous off-the-shelf AI, there'll be others. There's also a proprietary AI, because software companies like ServiceNow and SAP, there are many, many others that can't afford to have their company's intelligence be outsourced to somebody else. And they are about building tools and on top of their tools they should build custom and proprietary and domain-specific copilots and assistants that they can then rent to their customer base. This is -- they're sitting on a goldmine, almost every major tools company in the world is sitting on a goldmine, and they recognize that they have to go build their own custom AIs. We have a new service called an AI foundry, where we leverage NVS (ph) capabilities to be able to serve them in that. And then the next one is enterprises building their own custom AIs, their own custom chatbots, their own custom RAGs. And this capability is spreading all over the world. And the way that we're going to serve that marketplace is with the entire stacks of systems, which includes our compute, our networking and our switches, running our software stack called NVIDIA AI Enterprise, taking it through our market partners, HP, Dell, Lenovo, so on and so forth. And so we're just -- we're seeing the waves of generative AI starting from the start-ups and CSPs, moving to consumer Internet companies, moving to enterprise software platforms, moving to enterprise companies. And then ultimately, one of the areas that you guys have seen us spend a lot of energy on has to do with industrial generative AI. This is where NVIDIA AI and NVIDIA Omniverse comes together and that is a really, really exciting work. And so I think the -- we're at the beginning of a basically across-the-board industrial transition to generative AI to accelerated computing. This is going to affect every company, every industry, every country.\\nOperator: Your next question comes from the line of Toshiya Hari of Goldman Sachs. Your line is open.\\nToshiya Hari: Hi. Thank you. I wanted to clarify something with Colette real quick, and then I had a question for Jensen as well. Colette, you mentioned that you'll be introducing regulation-compliant products over the next couple of months. Yet, the contribution to Q4 revenue should be relatively limited. Is that a timing issue and could it be a source of reacceleration in growth for Data Center in April and beyond or are the price points such that the contribution to revenue going forward should be relatively limited? And then the question for Jensen, the AI foundry service announcement from last week. I just wanted to ask about that, and hopefully, have you expand on it. How is the monetization model going to work? Is it primarily services and software revenue? How should we think about the long term opportunity set? And is this going to be exclusive to Microsoft or do you have plans to expand to other partners as well? Thank you.\\nColette Kress: Thanks, Toshiya. On the question regarding potentially new products that we could provide to our China customers. It's a significant process to both design and develop these new products. As we discussed, we're going to make sure that we are in full discussions with the U.S. government of our intent to move products as well. Given our state about where we are in the quarter, we're already several weeks into the quarter. So it's just going to take some time for us to go through and discussing with our customers the needs and desires of these new products that we have. And moving forward, whether that's medium-term or long-term, it's just hard to say both the [Technical Difficulty] of what we can produce with the U.S. government and what the interest of our China customers in this. So we stay still focused on finding that right balance for our China customers, but it's hard to say at this time.\\nJensen Huang: Toshiya, thanks for the question. There is a glaring opportunity in the world for AI foundry, and it makes so much sense. First, every company has its core intelligence. It makes up our company. Our data, our domain expertise, in the case of many companies, we create tools, and most of the software companies in the world are tool platforms, and those tools are used by people today. And in the future, it's going to be used by people augmented with a whole bunch of AIs that we hire. And these platforms just got to go across the world and you'll see and we've only announced a few; SAP, ServiceNow, Dropbox, Getty, many others are coming. And the reason for that is because they have their own proprietary AI. They want their own proprietary AI. They can't afford to outsource their intelligence and handout their data, and handout their flywheel for other companies to build the AI for them. And so, they come to us. We have several things that are really essential in a foundry. Just as TSMC as a foundry, you have to have AI technology. And as you know, we have just an incredible depth of AI capability -- AI technology capability. And then second, you have to have the best practice known practice, the skills of processing data through the invention of AI models to create AIs that are guardrails, fine-tuned, so on and so forth, that are safe, so on and so forth. And the third thing is you need factories. And that's what DGX Cloud is. Our AI models are called AI Foundations. Our process, if you will, our CAD system for creating AIs are called NeMo and they run on NVIDIA's factories we call DGX Cloud. Our monetization model is that with each one of our partners they rent a sandbox on DGX Cloud, where we work together, they bring their data, they bring their domain expertise, we bring our researchers and engineers, we help them build their custom AI. We help them make that custom AI incredible. Then that custom AI becomes theirs. And they deploy it on the runtime that is enterprise grade, enterprise optimized or outperformance optimized, runs across everything NVIDIA. We have a giant installed base in the cloud, on-prem, anywhere. And it's secure, securely patched, constantly patched and optimized and supported. And we call that NVIDIA AI Enterprise. NVIDIA AI Enterprise is $4,500 per GP per year, that's our business model. Our business model is basically a license. Our customers then with that basic license can build their monetization model on top of. In a lot of ways we're wholesale, they become retail. They could have a per -- they could have subscription license base, they could per instance or they could do per usage, there is a lot of different ways that they could take a -- create their own business model, but ours is basically like a software license, like an operating system. And so our business model is help you create your custom models, you run those custom models on NVIDIA AI Enterprise. And it's off to a great start. NVIDIA AI Enterprise is going to be a very large business for us.\\nOperator: Your next question comes from the line of Stacy Rasgon of Bernstein Research. Your line is open.\\nStacy Rasgon: Hi, guys. Thanks for taking my questions. Colette, I wanted to know if it weren't for the China restrictions would the Q4 guide has been higher or are you supply-constrained in just reshipping stuff that would have gone to China elsewhere? And I guess along those lines you give us a feeling for where your lead times are right now in data center and just the China redirection such as-is, is it lowering those lead times, because you've got parts that are sort of immediately available to ship?\\nColette Kress: Yeah. Stacy, let me see if I can help you understand. Yes, there are still situations where we are working on both improving our supply each and every quarter. We've done a really solid job of ramping every quarter, which has defined our revenue. But with the absence of China for our outlook for Q4, sure, there could have been some things that we are not supply-constrained that we could have sold, but kind of we would no longer can. So could our guidance had been a little higher in our Q4? Yes. We are still working on improving our supply on plan, on continuing growing all throughout next year as well towards that.\\nOperator: Your next question comes from the line of Matt Ramsay of TD Cowen. Your line is open.\\nMatt Ramsay: Thank you very much. Congrats, everybody, on the results. Jensen, I had a two-part question for you, and it comes off of sort of one premise. And the premise is, I still get a lot of questions from investors thinking about AI training as being NVIDIA's dominant domain and somehow inference, even large model inference takes more and more of the TAM that the market will become more competitive. You'll be less differentiated et cetera., et cetera. So I guess the two parts of the question are: number one, maybe you could spend a little bit of time talking about the evolution of the inference workload as we move to LLMs and how your company is positioned for that rather than smaller model inference. And second, up until a month or two ago, I never really got any questions at all about the data processing piece of the AI workloads. So the pieces of manipulating the data before training, between training and inference, after inference and I think that's a large part of the workload now. Maybe you could talk about how CUDA is enabling acceleration of those pieces of the workload. Thanks.\\nJensen Huang: Sure. Inference is complicated. It's actually incredibly complicated. If you -- we this quarter announced one of the most exciting new engines, optimizing compilers called TensorRT-LLM. The reception has been incredible. You got to GitHub, it's been downloaded a ton, a whole lot of stars, integrated into stacks and frameworks all over the world, almost instantaneously. And there are several reasons for that, obviously. We could create TensorRT-LLM, because CUDA is programmable. If CUDA and our GPUs were not so programmable, it would really be hard for us to improve software stacks at the pace that we do. TensorRT-LLM, on the same GPU, without anybody touching anything, improves the performance by a factor of two. And then on top of that, of course, the pace of our innovation is so high. H200 increases it by another factor of two. And so, our inference performance, another way of saying inference cost, just reduced by a factor of four within about a year's time. And so, that's really, really hard to keep up with. The reason why everybody likes our inference engine is because our installed base. We've been dedicated to our installed base for 20 years, 20-plus years. We have an installed base that is not only largest in every single cloud, it's in every available from every enterprise system maker, it's used by companies of just about every industry. And every -- anytime you see a NVIDIA GPU, it runs our stack. It's architecturally compatible, something we've been dedicated to for a very long time. We're very disciplined about it. We make it our, if you will, architecture compatibility is job one. And that has conveyed to the world, the certainty of our platform stability. NVIDIA's platform stability certainty is the reason why everybody builds on us first and the reason why everybody optimizes on us first. All the engineering and all the work that you do, all the invention of technologies that you build on top of NVIDIA accrues to the -- and benefits everybody that uses our GPUs. And we have such a large installed base, large -- millions and millions of GPUs in cloud, 100 million GPUs from people’s PCs just about every workstation in the world, and they all architecturally compatible. And so, if you are an inference platform and you're deploying an inference application, you are basically an application provider. And as a software application provider, you're looking for large installed base. Data processing, before you could train a model, you have to curate the data, you have to dedupe the data, maybe you have to augment the data with synthetic data. So, process the data, clean the data, align the data, normalize the data, all of that data is measured not in bytes or megabytes, it's measured in terabytes and petabytes. And the amount of data processing that you do before data engineering, before that you do training is quite significant. It could represent 30%, 40%, 50% of the amount of work that you ultimately do. And what you -- and ultimately creating a data driven machine learning service. And so data processing is just a massive part. We accelerate Spark, we accelerate Python. One of the coolest things that we just did is called cuDF Pandas. Without one line of code, Pandas, which is the single most successful data science framework in the world. Pandas now is accelerated by NVIDIA CUDA. And just out-of-the box, without the line of code and so the acceleration is really quite terrific and people are just incredibly excited about it. And Pandas was designed for one purpose and one purpose only, really data processing, it's for data science. And so NVIDIA CUDA gives you all of that.\\nOperator: Your final question comes from the line of Harlan Sur of J.P. Morgan. Your line is open.\\nHarlan Sur: Good afternoon. Thanks for taking my question. If you look at the history of the tech industry like those companies that have been successful have always been focused on ecosystem; silicon, hardware, software, strong partnerships and just as importantly, right, an aggressive cadence of new products, more segmentation over time. The team recently announced a more aggressive new product cadence in data center from two years to now every year with higher levels of segmentation, training, optimization in printing CPU, GPU, DPU networking. How do we think about your R&D OpEx growth outlook to support a more aggressive and expanding forward roadmap, but more importantly, what is the team doing to manage and drive execution through all of this complexity?\\nJensen Huang: Gosh. Boy, that's just really excellent. You just wrote NVIDIA's business plan, and so you described our strategy. First of all, there is a fundamental reason why we accelerate our execution. And the reason for that is because it fundamentally drives down cost. When the combination of TensorRT-LLM and H200 reduce the cost for our customers for large model inference by a factor of four, and so that includes, of course, our speeds and feeds, but mostly it's because of our software, mostly the software benefits because of the architecture. And so we want to accelerate our roadmap for that reason. The second reason is to expand the reach of generative AI, the world's number of data center configurations -- this is kind of the amazing thing. NVIDIA is in every cloud, but not one cloud is the same. NVIDIA is working with every single cloud service provider and not one of the networking control plane, security posture is the same. Everybody's platform is different and yet we're integrated into all of their stacks, all of their data centers and we work incredibly well with all of them. And not to mention, we then take the whole thing and we create AI factories that are standalone. We take our platform, we can put them into supercomputers, we can put them into enterprise. Bringing AI to enterprise is something generative AI Enterprise something nobody's ever done before. And we're right now in the process of going to market with all of that. And so the complexity includes, of course, all the technologies and segments and the pace. It includes the fact that we are architecturally compatible across every single one of those. It includes all of the domain specific libraries that we create. The reason why every computer company, without thinking, can integrate NVIDIA into their roadmap and take it to market. And the reason for that is, because there is market demand for it. There is market demand in healthcare, there is market demand in manufacturing, there is market demand, of course, in AI, including financial services, in supercomputing and quantum computing. The list of markets and segments that we have domain specific libraries is incredibly broaden. And then finally, we have an end-to-end solution for data centers; InfiniBand networking, Ethernet networking, x86, ARM, just about every permutation combination of solutions -- technology solutions and software stacks provided. And that translates to having the largest number of ecosystem software developers; the largest ecosystem of system makers; the largest and broadest distribution partnership network; and ultimately, the greatest reach. And that takes -- surely that takes a lot of energy. But the thing that really holds it together, and this is a great decision that we made decades ago, which is everything is architecturally compatible. When we develop a domain specific language that runs on one GPU, it runs on every GPU. When we optimize TensorRT for the cloud, we optimized it for enterprise. When we do something that brings in a new feature, a new library, a new feature or a new developer, they instantly get the benefit of all of our reach. And so that discipline, that architecture compatible discipline that has lasted more than a couple of decades now, is one of the reasons why NVIDIA is still really, really efficient. I mean, we're 28,000 people large and serving just about every single company, every single industry, every single market around the world.\\nOperator: Thank you. I will now turn the call back over to Jensen Huang for closing remarks.\\nJensen Huang: Our strong growth reflects the broad industry platform transition from general purpose to accelerated computing and generative AI. Large language models start-ups consumer Internet companies and global cloud service providers are the first movers. The next waves are starting to build. Nations and regional CSPs are building AI clouds to serve local demand. Enterprise software companies like Adobe and Dropbox, SAP and ServiceNow are adding AI copilots and assistants to their platforms. Enterprises in the world's largest industries are creating custom AIs to automate and boost productivity. The generative AI era is in full steam and has created the need for a new type of data center, an AI factory; optimized for refining data and training, and inference, and generating AI. AI factory workloads are different and incremental to legacy data center workloads supporting IT tasks. AI factories run copilots and AI assistants, which are significant software TAM expansion and are driving significant new investment. Expanding the $1 trillion traditional data center infrastructure installed base, empowering the AI Industrial Revolution. NVIDIA H100 HGX with InfiniBand and the NVIDIA AI software stack define an AI factory today. As we expand our supply chain to meet the world's demand, we are also building new growth drivers for the next wave of AI. We highlighted three elements to our new growth strategy that are hitting their stride: CPU, networking, and software and services. Grace is NVIDIA's first data center CPU. Grace and Grace Hopper are in full production and ramping into a new multi-billion dollar product line next year. Irrespective of the CPU choice, we can help customers build an AI factory. NVIDIA networking now exceeds $10 billion annualized revenue run rate. InfiniBand grew five-fold year-over-year, and is positioned for excellent growth ahead as the networking of AI factories. Enterprises are also racing to adopt AI and Ethernet is the standard networking. This week we announced an Ethernet for AI platform for enterprises. NVIDIA Spectrum-X is an end-to-end solution of Bluefield SuperNIC, Spectrum-4 Ethernet switch and software that boosts Ethernet performance by up to 1.6x for AI workloads. Dell, HPE and Lenovo have joined us to bring a full generative AI solution of NVIDIA AI computing, networking and software to the world's enterprises. NVIDIA software and services is on track to exit the year at an annualized run rate of $1 billion. Enterprise software platforms like ServiceNow and SAP need to build and operate proprietary AI. Enterprises need to build and deploy custom AI copilots. We have the AI technology, expertise and scale to help customers build custom models with their proprietary data on NVIDIA DGX Cloud and deploy the AI applications on enterprise grade NVIDIA AI Enterprise. NVIDIA is essentially an AI foundry. NVIDIA's GPUs, CPUs, networking, AI foundry services and NVIDIA AI Enterprise software are all growth engines in full throttle. Thanks for joining us today. We look forward to updating you on our progress next quarter.\\nOperator: This concludes today's conference call. You may now disconnect.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " \"NVIDIA's latest earnings call revealed record-breaking revenue of $18.1 billion, marking a 34% sequential increase and a 200% year-on-year increase. The company's Data Center segment was a primary driver of this growth, achieving record revenue of $14.5 billion. However, the company anticipates a significant decline in the fourth quarter due to new U.S. export control regulations affecting sales to China and other markets. \\nKey takeaways from the call include:NVIDIA's Data Center segment saw record revenue, driven by the NVIDIA HGX platform and InfiniBand networking. Major companies like Meta (NASDAQ:META) and Adobe (NASDAQ:ADBE) are integrating NVIDIA's technology into their platforms, contributing to the strong demand for AI supercomputers and data center infrastructure.NVIDIA is expanding its Data Center product portfolio to comply with new U.S. export control regulations. Despite expecting a decrease in sales to China and other markets in the fourth quarter, the company is focusing on national investment in sovereign AI infrastructure.The Gaming segment recorded revenue of $2.86 billion, a 15% sequential increase and an 80% year-on-year increase, driven by strong demand for NVIDIA's RTX ray tracing and AI technology.In the Pro Vis segment, revenue reached $416 million, a 10% sequential increase and a 108% year-on-year increase. NVIDIA RTX is the preferred platform for professional design, engineering, and simulation, with AI emerging as a demand driver.NVIDIA's CEO, Jensen Huang, discussed the expansion of AI factories and the transition into a new computing approach. He also expressed confidence in the continued growth of the Data Center business through 2025.During the call, NVIDIA highlighted its product innovations, including the H200 GPU, which offers faster and larger memory for generative AI and language models, and the GH200 Grace Hopper Superchip, a combination of an ARM-based CPU with a Hopper GPU. The Grace Hopper Superchip is being adopted by supercomputing customers, with the combined AI compute capacity of supercomputers built on Grace Hopper expected to exceed 200 exaflops next year. \\nIn addition to its product offerings, NVIDIA is also expanding its networking into the Ethernet space with its new Spectrum-X end-to-end Ethernet offering, which will be available in Q1 next year. This offering can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. \\nNVIDIA also discussed its business strategy, emphasizing the importance of reselling AI technology to customers and its growth in the enterprise market. The company introduced an AI foundry service to help enterprises build custom AI models and announced an aggressive product release schedule in the data center market. \\nLooking ahead, NVIDIA expects total revenue of $20 billion for Q4 2024, driven by strong demand in the Data Center sector. Despite regulatory challenges, the company is confident in its growth potential, citing expanded supply, new customers, and its entrance into the enterprise market.Full transcript -  Nvidia Corp  (NASDAQ:NVDA) Q3 2024:Operator: Good afternoon. My name is JL, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's Third Quarter Earnings Call. All lines have been placed on mute to prevent any background noise. After the speakers’ remarks, there will be a question-and-answer session. [Operator Instructions] Simona Jankowski, you may now begin your conference.\\nSimona Jankowski: Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2024. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter and fiscal 2024. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All statements are made as of today, November 21, 2023, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\\nColette Kress: Thanks, Simona. Q3 was another record quarter. Revenue of $18.1 billion was up 34% sequentially and up more than 200% year-on-year and well above our outlook for $16 billion. Starting with Data Center. The continued ramp of the NVIDIA HGX platform based on our Hopper Tensor Core GPU architecture, along with InfiniBand end-to-end networking, drove record revenue of $14.5 billion, up 41% sequentially and up 279% year-on-year. NVIDIA HGX with InfiniBand together are essentially the reference architecture for AI supercomputers and data center infrastructures. Some of the most exciting generative AI applications are built and run on NVIDIA, including Adobe Firefly, ChatGPT, Microsoft (NASDAQ:MSFT) 365 Copilot, CoAssist, now assist with ServiceNow (NYSE:NOW) and Zoom (NASDAQ:ZM) AI Companion. Our Data Center compute revenue quadrupled from last year and networking revenue nearly tripled. Investments in infrastructure for training and inferencing large language models, deep learning, recommender systems and generative AI applications is fueling strong broad-based demand for NVIDIA accelerated computing. Inferencing is now a major workload for NVIDIA AI computing. Consumer Internet companies and enterprises drove exceptional sequential growth in Q3, comprising approximately half of our Data Center revenue and outpacing total growth. Companies like Meta are in full production with deep learning, recommender systems and also investing in generative AI to help advertisers optimize images and text. Most major consumer Internet companies are racing to ramp up generative AI deployment. The enterprise wave of AI adoption is now beginning. Enterprise software companies such as Adobe, Databricks, Snowflake (NYSE:SNOW) and ServiceNow are adding AI copilots and the systems to their platforms. And broader enterprises are developing custom AI for vertical industry applications such as Tesla (NASDAQ:TSLA) in autonomous driving. Cloud service providers drove roughly the other half of our Data Center revenue in the quarter. Demand was strong from all hyperscale CSPs, as well as from a broadening set of GPU-specialized CSPs globally that are rapidly growing to address the new market opportunities in AI. NVIDIA H100 Tensor Core GPU instances are now generally available in virtually every cloud with instances in high demand. We have significantly increased supply every quarter this year to meet strong demand and expect to continue to do so next year. We will also have a broader and faster product launch cadence to meet the growing and diverse set of AI opportunities. Towards the end of the quarter, the U.S. government announced a new set of export control regulations for China and other markets, including Vietnam and certain countries in the Middle East. These regulations require licenses for the export of a number of our products, including our Hopper and Ampere 100 and 800 series and several others. Our sales to China and other affected destinations derived from products that are now subject to licensing requirements have consistently contributed approximately 20% to 25% of Data Center revenue over the past few quarters. We expect that our sales to these destinations will decline significantly in the fourth quarter. So we believe will be more than offset by strong growth in other regions. The U.S. government designed the regulation to allow the U.S. industry to provide data center compute products to markets worldwide, including China. Continuing to compete worldwide as the regulations encourage, promotes U.S. technology leadership, spurs economic growth and supports U.S. jobs. For the highest performance levels, the government requires licenses. For lower performance levels, the government requires a streamlined prior notification process. And for products even lower performance levels, the government does not require any notice at all. Following the government's clear guidelines, we are working to expand our Data Center product portfolio to offer compliance solutions for each regulatory category, including products for which the U.S. government does not wish to have advance notice before each shipment. We are working with some customers in China and the Middle East to pursue licenses from the U.S. government. It is too early to know whether these will be granted for any significant amount of revenue. Many countries are awakening to the need to invest in sovereign AI infrastructure to support economic growth and industrial innovation. With investments in domestic compute capacity, nations can use their own data to train LLMs and support their local generative AI ecosystems. For example, we are working with India's government and largest tech companies including  Infosys  (NS:INFY), Reliance and Tata to boost their sovereign AI infrastructure. And French private cloud provider, Scaleway, is building a regional AI cloud based on NVIDIA H100 InfiniBand and NVIDIA's AI Enterprise software to fuel advancement across France and Europe. National investment in compute capacity is a new economic imperative and serving the sovereign AI infrastructure market represents a multi-billion dollar opportunity over the next few years. From a product perspective, the vast majority of revenue in Q3 was driven by the NVIDIA HGX platform based on our Hopper GPU architecture with lower contribution from the prior generation Ampere GPU architecture. The new L40S GPU built for industry standard servers began to ship, supporting training and inference workloads across a variety of consumers. This was also the first revenue quarter of our GH200 Grace Hopper Superchip, which combines our ARM-based Grace CPU with a Hopper GPU. Grace and Grace Hopper are ramping into a new multi-billion dollar product line. Grace Hopper instances are now available at GPU specialized cloud providers, and coming soon to Oracle (NYSE:ORCL) Cloud. Grace Hopper is also getting significant traction with supercomputing customers. Initial shipments to Los Alamos National Lab and the Swiss National Supercomputing Center took place in the third quarter. The UK government announced it will build one of the world's fastest AI supercomputers called Isambard-AI with almost 5,500 Grace Hopper Superchips. German supercomputing center, Julich, also announced that it will build its next-generation AI supercomputer with close to 24,000 Grace Hopper Superchips and Quantum-2 InfiniBand, making it the world's most powerful AI supercomputer with over 90 exaflops of AI performance. All-in, we estimate that the combined AI compute capacity of all the supercomputers built on Grace Hopper across the U.S., Europe and Japan next year will exceed 200 exaflops with more wins to come. Inference is contributing significantly to our data center demand, as AI is now in full production for deep learning, recommenders, chatbots, copilots and text to image generation and this is just the beginning. NVIDIA AI offers the best inference performance and versatility, and thus the lower power and cost of ownership. We are also driving a fast cost reduction curve. With the release of TensorRT-LLM, we now achieved more than 2x the inference performance for half the cost of inferencing LLMs on NVIDIA GPUs. We also announced the latest member of the Hopper family, the H200, which will be the first GPU to offer HBM3e, faster, larger memory to further accelerate generative AI and LLMs. It moves inference speed up to another 2x compared to H100 GPUs for running LLMs like Norma2 (ph). Combined, TensorRT-LLM and H200, increased performance or reduced cost by 4x in just one year. With our customers changing their stack, this is a benefit of CUDA and our architecture compatibility. Compared to the A100, H200 delivers an 18x performance increase for inferencing models like GPT-3, allowing customers to move to larger models and with no increase in latency. Amazon (NASDAQ:AMZN) Web Services, Google (NASDAQ:GOOGL) Cloud, Microsoft Azure and Oracle Cloud will be among the first CSPs to offer H200-based instances starting next year. At last week's Microsoft Ignite, we deepened and expanded our collaboration with Microsoft across the entire stock. We introduced an AI foundry service for the development and tuning of custom generative AI enterprise applications running on Azure. Customers can bring their domain knowledge and proprietary data and we help them build their AI models using our AI expertise and software stock in our DGX cloud, all with enterprise grade security and support. SAP and  Amdocs  (NASDAQ:DOX) are the first customers of the NVIDIA AI foundry service on Microsoft Azure. In addition, Microsoft will launch new confidential computing instances based on the H100. The H100 remains the top performing and most versatile platform for AI training and by a wide margin, as shown in the latest MLPerf industry benchmark results. Our training cluster included more than 10,000 H100 GPUs or 3x more than in June, reflecting very efficient scaling. Efficient scaling is a key requirement in generative AI, because LLMs are growing by an order of magnitude every year. Microsoft Azure achieved similar results on a nearly identical cluster, demonstrating the efficiency of NVIDIA AI in public cloud deployments. Networking now exceeds a $10 billion annualized revenue run rate. Strong growth was driven by exceptional demand for InfiniBand, which grew fivefold year-on-year. InfiniBand is critical to gaining the scale and performance needed for training LLMs. Microsoft made this very point last week, highlighting that Azure uses over 29,000 miles of InfiniBand cabling, enough to circle the globe. We are expanding NVIDIA networking into the Ethernet space. Our new Spectrum-X end-to-end Ethernet offering with technologies, purpose built for AI, will be available in Q1 next year. With support from leading OEMs, including Dell (NYSE:DELL), HPE and Lenovo. Spectrum-X can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. Let me also provide an update on our software and services offerings, where we are starting to see excellent adoption. We are on track to exit the year at an annualized revenue run rate of $1 billion for our recurring software, support and services offerings. We see two primary opportunities for growth over the intermediate term with our DGX cloud service and with our NVIDIA AI Enterprise software, each reflects the growth of enterprise AI training and enterprise AI inference, respectively. Our latest DGX cloud customer announcement was this morning as part of an AI research collaboration with Gentech, the biotechnology pioneer also plans to use our BioNeMo LLM framework to help accelerate and optimize their AI drug discovery platform. We now have enterprise AI partnership with Adobe, Dropbox (NASDAQ:DBX), Getty, SAP, ServiceNow, Snowflake and others to come. Okay. Moving to Gaming. Gaming revenue of $2.86 billion was up 15% sequentially and up more than 80% year-on-year with strong demand in the important back-to-school shopping season with NVIDIA RTX ray tracing and AI technology now available at price points as low as $299. We entered the holidays with the best-ever line-up for gamers and creators. Gaming has doubled relative to pre-COVID levels even against the backdrop of lackluster PC market performance. This reflects the significant value we've brought to the gaming ecosystem with innovations like RTX and DLSS. The number of games and applications supporting these technologies has exploded in that period, driving upgrades and attracting new buyers. The RTX ecosystem continues to grow. There are now over 475 RTX-enabled games and applications. Generative AI is quickly emerging as the new pillar app for high performance PCs. NVIDIA RTX GPUs to find the most performance AI PCs and workstations. We just released TensorRT-LLM for Windows, which speeds on-device LLM inference up by 4x. With an installed base of over 100 million, NVIDIA RTX is the natural platform for AI application developers. Finally, our GeForce NOW cloud gaming service continues to build momentum. Its library of PC games surpassed 1,700 titles, including the launches of Alan Wake 2, Baldur's Gate 3, Cyberpunk 2077: Phantom Liberty and Starfield. Moving to the Pro Vis. Revenue of $416 million was up 10% sequentially and up 108% year-on year. NVIDIA RTX is the workstation platform of choice for professional design, engineering and simulation use cases and AI is emerging as a powerful demand driver. Early applications include inference for AI imaging in healthcare and edge AI in smart spaces and the public sector. We launched a new line of desktop workstations based on NVIDIA RTX Ada Lovelace generation GPUs and ConnectX, SmartNICs offering up to 2x the AI processing ray tracing and graphics performance of the previous generations. These powerful new workstations are optimized for AI workloads such as fine tune AI models, training smaller models and running inference locally. We continue to make progress on Omniverse, our software platform for designing, building and operating 3D virtual worlds. Mercedes-Benz (OTC:MBGAF) is using Omniverse powered digital twins to plan, design, build and operate its manufacturing and assembly facilities, helping it increase efficiency and reduce defects. Oxxon (ph) is also incorporating Omniverse into its manufacturing process, including end-to-end simulation for the entire robotics and automation pipeline, saving time and cost. We announced two new Omniverse Cloud services for automotive digitalization available on Microsoft Azure, a virtual factory simulation engine and autonomous vehicle simulation engine. Moving to Automotive. Revenue was $261 million, up 3% sequentially and up 4% year-on year, primarily driven by continued growth in self-driving platforms based on NVIDIA DRIVE Orin SOC and the ramp of AI cockpit solutions with global OEM customers. We extended our automotive partnership of Foxconn to include NVIDIA DRIVE for our next-generation automotive SOC. Foxconn has become the ODM for EVs. Our partnership provides Foxconn with a standard AV sensor and computing platform for their customers to easily build a state-of-an-art safe and secure software defined car. Now we're going to move to the rest of the P&L. GAAP gross margin expanded to 74% and non-GAAP gross margin to 75%, driven by higher Data Center sales and lower net inventory reserve, including a 1 percentage point benefit from the release of previously reserved inventory related to the Ampere GPU architecture products. Sequentially, GAAP operating expenses were up 12% and non-GAAP operating expenses were up 10%, primarily reflecting increased compensation and benefits. Let me turn to the fourth quarter of fiscal 2024. Total revenue is expected to be $20 billion, plus or minus 2%. We expect strong sequential growth to be driven by Data Center, with continued strong demand for both compute and networking. Gaming will likely decline sequentially as it is now more aligned with notebook seasonality. GAAP and non-GAAP gross margins are expected to be 74.5% and 75.5%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $3.17 billion and $2.2 billion, respectively. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $200 million, excluding gains and losses from non-affiliated investments. GAAP and non-GAAP tax rates are expected to be 15%, plus or minus 1% excluding any discrete items. Further financial information are included in the CFO commentary and other information available on our IR website. In closing, let me highlight some upcoming events for the financial community. We will attend the UBS Global Technology Conference in Scottsdale, Arizona, on November 28; the Wells Fargo TMT Summit in Rancho Palos Verdes, California on November 29; the Arete Virtual Tech Conference on December 7; and the J.P. Morgan Health Care Conference in San Francisco on January 8. Our earnings call to discuss the results of our fourth quarter and fiscal 2024 is scheduled for Wednesday, February 21. We will now open the call for questions. Operator, will you please poll for questions.\\nOperator: [Operator Instructions] Your first question comes from the line of Vivek Arya of Bank of America. Your line is open.\\nVivek Arya: Thanks for taking my question. Just, Colette, wanted to clarify what China contributions are you expecting in Q4. And then, Jensen, the main question is for you, where do you think we are in the adoption curve in terms of your shipments into the generative AI market? Because when I just look at the trajectory of your data center, is growth -- it will be close to nearly 30% of all the spending in data center next year. So what metrics are you keeping an eye on to inform you that you can continue to grow? Just where are we in the adoption curve of your products into the generative AI market? Thank you.\\nColette Kress: So, first let me start with your question, Vivek, on export controls and the impacts that we are seeing in our Q4 outlook and guidance that we provided. We had seen historically over the last several quarters that China and some of the other impacted destinations to be about 20% to 25% of our Data Center revenue. We are expecting in our guidance for that to decrease substantially as we move into Q4. The export controls will have a negative effect on our China business. And we do not have good visibility into the magnitude of that impact even over the long-term. We are though working to expand our Data Center product portfolio to possibly offer new regulation compliance solutions that do not require a license, these products, they may become available in the next coming months. However, we don't expect their contribution to be material or meaningful as a percentage of the revenue in Q4.\\nJensen Huang: Generative AI is the largest TAM expansion of software and hardware that we've seen in several decades. At the core of it, what's really exciting is that, what was largely a retrieval based computing approach, almost everything that you do is retrieved off of storage somewhere, has been augmented now, added with a generative method. And it's changed almost everything. You could see that text-to-text, text-to-image, text-to-video, text-to-3D, text-to-protein, text-to-chemicals, these were things that were processed and typed in by humans in the past. And these are now generative approaches. The way that we access data is changed. It used to be based on explicit queries. It is now based on natural language queries, intention queries, semantic queries. And so, we're excited about the work that we're doing with SAP and Dropbox and many others that you're going to hear about. And one of the areas that is really impactful is the software industry, which is about $1 trillion or so, has been building tools that are manually used over the last couple of decades. And now there's a whole new segment of software called copilots and assistants. Instead of manually used, these tools will have copilots to help you use it. And so, instead of licensing software, we will continue to do that, of course, but we will also hire copilots and assistants to help us use these -- use the software. We'll connect all of these copilots and assistants into teams of AIs, which is going to be the modern version of software, modern version of enterprise business software. And so the transformation of software and the way that software has done is driving the hardware underneath. And you can see that it's transforming hardware in two ways. One is something that's largely independent of generative AI. There's two trends: one is related to accelerated computing, general purpose computing is too wasteful of energy and cost. And now that we have much, much better approaches, call it, accelerated computing, you could save an order of magnitude of energy, you can save an order of magnitude of time or you can save an order of magnitudes of cost by using acceleration. And so, accelerated computing is transitioning, if you will, general purpose computing into this new approach. And that's been augmented by a new class of data centers. This is the traditional data centers that you were just talking about where we represent about a third of that. But there is a new class of data centers and this new class of data centers, unlike the data centers of the past, where you have a lot of applications running used by a great many people that are different tenants that are using the same infrastructure and that data center stores a lot of files. These new data centers are very few applications, if not one application, used by basically one tenant and it processes data, it trains models and then generates tokens and generates AI. And we call these new data centers AI factories. We're seeing AI factories being built out everywhere, and just by every country. And so if you look at the way where we are in the expansion, the transition into this new computing approach, the first wave you saw with large language model start-ups, generative AI start-ups and consumer Internet companies, and weren't in the process of ramping that. Meanwhile, while that's being ramped, you see that we're starting to partner with enterprise software companies who would like to build chatbots and copilots and assistants to augment the tools that they have on their platforms. You're seeing GPU specialized CSPs cropping up all over the world and they are dedicated to do really one thing, which is processing AI. You're seeing sovereign AI infrastructures, people -- countries that now recognize that they have to utilize their own data, keep their own data, keep their own culture, process that data and develop their own AI. You see that in India. Several -- about a year ago in Sweden, you are seeing in Japan. Last week, a big announcement in France. But the number of sovereign AI clouds that are being built is really quite significant. And my guess is that almost every major region will have and surely every major country will have their own AI clouds. And so I think you're seeing just new developments as the generative AI wave propagates through every industry, every company, every region. And so we're at the beginning of this inflection, this computing transition.\\nOperator: Your next question comes from the line of Aaron Rakers of Wells Fargo. Your line is open.\\nAaron Rakers: Yeah. Thanks for taking the question. I wanted to ask about kind of the networking side of the business. Given the growth rates that you've now cited, I think, it's 155% year-over-year and strong growth sequentially, it looks like that business is like almost approaching $2.5 billion to $3 billion quarterly level. I'm curious of how you see Ethernet involved evolving and maybe how you would characterize your differentiation of Spectrum-X relative to the traditional Ethernet stack as we start to think about that becoming part of the networking narrative above and maybe beyond just InfiniBand as we look into next year? Thank you.\\nJensen Huang: Yeah. Thanks for the question. Our networking business is already on a $10 billion plus run rate and it's going to get much larger. And as you mentioned, we added a new networking platform to our networking business recently. The vast majority of the dedicated large scale AI factories standardize on InfiniBand. And the reason for that is not only because of its data rate and not only just the latency, but the way that it moves traffic around the network is really important. The way that you process AI and a multi-tenant hyperscale Ethernet environment, the traffic pattern is just radically different. And with InfiniBand and with software defined networks, we could do congestion control, adaptive routing, performance isolation and noise isolation, not to mention, of course, the day rate and the low latency that -- and a very low overhead of InfiniBand that's natural part of InfiniBand. And so, InfiniBand is not so much just the network, it's also a computing fabric. We've put a lot of software-defined capabilities into the fabric including computation. We will do 40-point calculations and computation right on the switch, and right in the fabric itself. And so that's the reason why that difference in Ethernet versus InfiniBand or InfiniBand versus Ethernet for AI factories is so dramatic. And the difference is profound. And the reason for that is because you've just invested in a $2 billion infrastructure for AI factories. A 20%, 25%, 30% difference in overall effectiveness, especially as you scale up is measured in hundreds of millions of dollars of value. And if you will, renting that infrastructure over the course of four to five years, it really, really adds up. And so InfiniBand's value proposition is undeniable for AI factories. However, as we move AI into enterprise. This is enterprise computing what we'd like to enable every company to be able to build their own custom AIs. We're building customer AIs in our company based on our proprietary data, our proprietary type of skills. For example, recently we spoke about one of the models that we're creating, it's called ChipNeMo; we're building many others. There'll be tens, hundreds of custom AI models that we create inside our company. And our company is -- for all of our employee use, doesn't have to be as high performance as the AI factories we used to train the models. And so we would like the AI to be able to run in Ethernet environment. And so what we've done is we invented this new platform that extends Ethernet; doesn't replace Ethernet, it's 100% compliant with Ethernet. And it's optimized for East-West traffic, which is where the computing fabric is. It adds to Ethernet with an end-to-end solution with Bluefield, as well as our Spectrum switch that allows us to perform some of the capabilities that we have in InfiniBand, not all but some. And we achieved excellent results. And the way we go to market is we go to market with our large enterprise partners who already offer our computing solution. And so, HP (NYSE:HPQ), Dell and Lenovo has the NVIDIA AI stack, the NVIDIA AI Enterprise software stack and now they integrate with Bluefield, as well as bundle -- take a market there, Spectrum switch, and they'll be able to offer enterprise customers all over the world with their vast sales force and vast network of resellers a fully integrated, if you will, fully optimized, at least end-to-end AI solution. And so that's basically it, bringing AI to Ethernet for the world's enterprise.\\nOperator: Thank you. Your next question comes from the line of Joe Moore of Morgan Stanley. Your line is open.\\nJoseph Moore: Great. Thank you. I'm wondering if you could talk a little bit more about Grace Hopper and how you see the ability to leverage kind of the microprocessor, how you see that as a TAM expander. And what applications do you see using Grace Hopper versus more traditional H100 applications?\\nJensen Huang: Yeah. Thanks for the question. Grace Hopper is in production -- in high volume production now. We're expecting next year just with all of the design wins that we have in high performance computing and AI infrastructures, we are on a very, very fast ramp with our first data center CPU to a multi-billion dollar product line. This is going to be a very large product line for us. The capability of Grace Hopper is really quite spectacular. It has the ability to create computing nodes that simultaneously has very fast memory, as well as very large memory. In the areas of vector databases or semantic surge, what is called RAG, retrieval augmented generation. So that you could have a generative AI model be able to refer to proprietary data or a factual data before it generates a response, that data is quite large. And you can also have applications or generative models where the context length is very high. You basically store it in entire book into end-to-end system memory before you ask your questions. And so the context length can be quite large this way. The generative models has the ability to still be able to naturally interact with you on one hand. On the other hand, be able to refer to factual data, proprietary data or domain-specific data, you data and be contextually relevant and reduce hallucination. And so that particular use case for example is really quite fantastic for Grace Hopper. It also serves the customers that really care to have a different CPU than x86. Maybe it's a European supercomputing centers or European companies who would like to build up their own ARM ecosystem and like to build up a full stack or CSPs that have decided that they would like to pivot to ARM, because their own custom CPUs are based on ARM. There are variety of different reasons that drives the success of Grace Hopper, but we're off to a just an extraordinary start. This is a home run product.\\nOperator: Your next question comes from the line of Tim Arcuri of UBS. Your line is open.\\nTim Arcuri: Hi. Thanks. I wanted to ask a little bit about the visibility that you have on revenue. I know there's a few moving parts. I guess, on one hand, the purchase commitments went up a lot again. But on the other hand, China bans would arguably pull in when you can fill the demand beyond China. So I know we're not even into 2024 yet and it doesn't sound like, Jensen, you think that next year would be a peak in your Data Center revenue, but I just wanted to sort of explicitly ask you that. Do you think that Data Center can grow even in 2025? Thanks.\\nJensen Huang: Absolutely believe the Data Center can grow through 2025. And there are, of course, several reasons for that. We are expanding our supply quite significantly. We have already one of the broadest and largest and most capable supply chain in the world. Now, remember, people think that the GPU is a chip. But the HGX H100, the Hopper HGX has 35,000 parts, it weighs 70 pounds. Eight of the chips are Hopper. The other 35,000 are not. It is -- even its passive components are incredible. High voltage parts. High frequency parts. High current parts. It is a supercomputer, and therefore, the only way to test a supercomputer is with another supercomputer. Even the manufacturing of it is complicated, the testing of it is complicated, the shipping of it complicated and installation is complicated. And so, every aspect of our HGX supply chain is complicated. And the remarkable team that we have here has really scaled out the supply chain incredibly. Not to mention, all of our HGXs are connected with NVIDIA networking. And the networking, the transceivers, the mix, the cables, the switches, the amount of complexity there is just incredible. And so, I'm just -- first of all, I'm just super proud of the team for scaling up this incredible supply chain. We are absolutely world class. But meanwhile, we're adding new customers and new products. So we have new supply. We have new customers, as I was mentioning earlier. Different regions are standing up GPU specialist clouds, sovereign AI clouds coming out from all over the world, as people realize that they can't afford to export their country's knowledge, their country's culture for somebody else to then resell AI back to them, they have to -- they should, they have the skills and surely with us in combination, we can help them to do that build up their national AI. And so, the first thing that they have to do is, create their AI cloud, national AI cloud. You're also seeing us now growing into enterprise. The enterprise market has two paths. One path -- or if I could say three paths. The first path, of course, just off-the-shelf AI. And there are of course Chat GPT, a fabulous off-the-shelf AI, there'll be others. There's also a proprietary AI, because software companies like ServiceNow and SAP, there are many, many others that can't afford to have their company's intelligence be outsourced to somebody else. And they are about building tools and on top of their tools they should build custom and proprietary and domain-specific copilots and assistants that they can then rent to their customer base. This is -- they're sitting on a goldmine, almost every major tools company in the world is sitting on a goldmine, and they recognize that they have to go build their own custom AIs. We have a new service called an AI foundry, where we leverage NVS (ph) capabilities to be able to serve them in that. And then the next one is enterprises building their own custom AIs, their own custom chatbots, their own custom RAGs. And this capability is spreading all over the world. And the way that we're going to serve that marketplace is with the entire stacks of systems, which includes our compute, our networking and our switches, running our software stack called NVIDIA AI Enterprise, taking it through our market partners, HP, Dell, Lenovo, so on and so forth. And so we're just -- we're seeing the waves of generative AI starting from the start-ups and CSPs, moving to consumer Internet companies, moving to enterprise software platforms, moving to enterprise companies. And then ultimately, one of the areas that you guys have seen us spend a lot of energy on has to do with industrial generative AI. This is where NVIDIA AI and NVIDIA Omniverse comes together and that is a really, really exciting work. And so I think the -- we're at the beginning of a basically across-the-board industrial transition to generative AI to accelerated computing. This is going to affect every company, every industry, every country.\\nOperator: Your next question comes from the line of Toshiya Hari of Goldman Sachs. Your line is open.\\nToshiya Hari: Hi. Thank you. I wanted to clarify something with Colette real quick, and then I had a question for Jensen as well. Colette, you mentioned that you'll be introducing regulation-compliant products over the next couple of months. Yet, the contribution to Q4 revenue should be relatively limited. Is that a timing issue and could it be a source of reacceleration in growth for Data Center in April and beyond or are the price points such that the contribution to revenue going forward should be relatively limited? And then the question for Jensen, the AI foundry service announcement from last week. I just wanted to ask about that, and hopefully, have you expand on it. How is the monetization model going to work? Is it primarily services and software revenue? How should we think about the long term opportunity set? And is this going to be exclusive to Microsoft or do you have plans to expand to other partners as well? Thank you.\\nColette Kress: Thanks, Toshiya. On the question regarding potentially new products that we could provide to our China customers. It's a significant process to both design and develop these new products. As we discussed, we're going to make sure that we are in full discussions with the U.S. government of our intent to move products as well. Given our state about where we are in the quarter, we're already several weeks into the quarter. So it's just going to take some time for us to go through and discussing with our customers the needs and desires of these new products that we have. And moving forward, whether that's medium-term or long-term, it's just hard to say both the [Technical Difficulty] of what we can produce with the U.S. government and what the interest of our China customers in this. So we stay still focused on finding that right balance for our China customers, but it's hard to say at this time.\\nJensen Huang: Toshiya, thanks for the question. There is a glaring opportunity in the world for AI foundry, and it makes so much sense. First, every company has its core intelligence. It makes up our company. Our data, our domain expertise, in the case of many companies, we create tools, and most of the software companies in the world are tool platforms, and those tools are used by people today. And in the future, it's going to be used by people augmented with a whole bunch of AIs that we hire. And these platforms just got to go across the world and you'll see and we've only announced a few; SAP, ServiceNow, Dropbox, Getty, many others are coming. And the reason for that is because they have their own proprietary AI. They want their own proprietary AI. They can't afford to outsource their intelligence and handout their data, and handout their flywheel for other companies to build the AI for them. And so, they come to us. We have several things that are really essential in a foundry. Just as TSMC as a foundry, you have to have AI technology. And as you know, we have just an incredible depth of AI capability -- AI technology capability. And then second, you have to have the best practice known practice, the skills of processing data through the invention of AI models to create AIs that are guardrails, fine-tuned, so on and so forth, that are safe, so on and so forth. And the third thing is you need factories. And that's what DGX Cloud is. Our AI models are called AI Foundations. Our process, if you will, our CAD system for creating AIs are called NeMo and they run on NVIDIA's factories we call DGX Cloud. Our monetization model is that with each one of our partners they rent a sandbox on DGX Cloud, where we work together, they bring their data, they bring their domain expertise, we bring our researchers and engineers, we help them build their custom AI. We help them make that custom AI incredible. Then that custom AI becomes theirs. And they deploy it on the runtime that is enterprise grade, enterprise optimized or outperformance optimized, runs across everything NVIDIA. We have a giant installed base in the cloud, on-prem, anywhere. And it's secure, securely patched, constantly patched and optimized and supported. And we call that NVIDIA AI Enterprise. NVIDIA AI Enterprise is $4,500 per GP per year, that's our business model. Our business model is basically a license. Our customers then with that basic license can build their monetization model on top of. In a lot of ways we're wholesale, they become retail. They could have a per -- they could have subscription license base, they could per instance or they could do per usage, there is a lot of different ways that they could take a -- create their own business model, but ours is basically like a software license, like an operating system. And so our business model is help you create your custom models, you run those custom models on NVIDIA AI Enterprise. And it's off to a great start. NVIDIA AI Enterprise is going to be a very large business for us.\\nOperator: Your next question comes from the line of Stacy Rasgon of Bernstein Research. Your line is open.\\nStacy Rasgon: Hi, guys. Thanks for taking my questions. Colette, I wanted to know if it weren't for the China restrictions would the Q4 guide has been higher or are you supply-constrained in just reshipping stuff that would have gone to China elsewhere? And I guess along those lines you give us a feeling for where your lead times are right now in data center and just the China redirection such as-is, is it lowering those lead times, because you've got parts that are sort of immediately available to ship?\\nColette Kress: Yeah. Stacy, let me see if I can help you understand. Yes, there are still situations where we are working on both improving our supply each and every quarter. We've done a really solid job of ramping every quarter, which has defined our revenue. But with the absence of China for our outlook for Q4, sure, there could have been some things that we are not supply-constrained that we could have sold, but kind of we would no longer can. So could our guidance had been a little higher in our Q4? Yes. We are still working on improving our supply on plan, on continuing growing all throughout next year as well towards that.\\nOperator: Your next question comes from the line of Matt Ramsay of TD Cowen. Your line is open.\\nMatt Ramsay: Thank you very much. Congrats, everybody, on the results. Jensen, I had a two-part question for you, and it comes off of sort of one premise. And the premise is, I still get a lot of questions from investors thinking about AI training as being NVIDIA's dominant domain and somehow inference, even large model inference takes more and more of the TAM that the market will become more competitive. You'll be less differentiated et cetera., et cetera. So I guess the two parts of the question are: number one, maybe you could spend a little bit of time talking about the evolution of the inference workload as we move to LLMs and how your company is positioned for that rather than smaller model inference. And second, up until a month or two ago, I never really got any questions at all about the data processing piece of the AI workloads. So the pieces of manipulating the data before training, between training and inference, after inference and I think that's a large part of the workload now. Maybe you could talk about how CUDA is enabling acceleration of those pieces of the workload. Thanks.\\nJensen Huang: Sure. Inference is complicated. It's actually incredibly complicated. If you -- we this quarter announced one of the most exciting new engines, optimizing compilers called TensorRT-LLM. The reception has been incredible. You got to GitHub, it's been downloaded a ton, a whole lot of stars, integrated into stacks and frameworks all over the world, almost instantaneously. And there are several reasons for that, obviously. We could create TensorRT-LLM, because CUDA is programmable. If CUDA and our GPUs were not so programmable, it would really be hard for us to improve software stacks at the pace that we do. TensorRT-LLM, on the same GPU, without anybody touching anything, improves the performance by a factor of two. And then on top of that, of course, the pace of our innovation is so high. H200 increases it by another factor of two. And so, our inference performance, another way of saying inference cost, just reduced by a factor of four within about a year's time. And so, that's really, really hard to keep up with. The reason why everybody likes our inference engine is because our installed base. We've been dedicated to our installed base for 20 years, 20-plus years. We have an installed base that is not only largest in every single cloud, it's in every available from every enterprise system maker, it's used by companies of just about every industry. And every -- anytime you see a NVIDIA GPU, it runs our stack. It's architecturally compatible, something we've been dedicated to for a very long time. We're very disciplined about it. We make it our, if you will, architecture compatibility is job one. And that has conveyed to the world, the certainty of our platform stability. NVIDIA's platform stability certainty is the reason why everybody builds on us first and the reason why everybody optimizes on us first. All the engineering and all the work that you do, all the invention of technologies that you build on top of NVIDIA accrues to the -- and benefits everybody that uses our GPUs. And we have such a large installed base, large -- millions and millions of GPUs in cloud, 100 million GPUs from people’s PCs just about every workstation in the world, and they all architecturally compatible. And so, if you are an inference platform and you're deploying an inference application, you are basically an application provider. And as a software application provider, you're looking for large installed base. Data processing, before you could train a model, you have to curate the data, you have to dedupe the data, maybe you have to augment the data with synthetic data. So, process the data, clean the data, align the data, normalize the data, all of that data is measured not in bytes or megabytes, it's measured in terabytes and petabytes. And the amount of data processing that you do before data engineering, before that you do training is quite significant. It could represent 30%, 40%, 50% of the amount of work that you ultimately do. And what you -- and ultimately creating a data driven machine learning service. And so data processing is just a massive part. We accelerate Spark, we accelerate Python. One of the coolest things that we just did is called cuDF Pandas. Without one line of code, Pandas, which is the single most successful data science framework in the world. Pandas now is accelerated by NVIDIA CUDA. And just out-of-the box, without the line of code and so the acceleration is really quite terrific and people are just incredibly excited about it. And Pandas was designed for one purpose and one purpose only, really data processing, it's for data science. And so NVIDIA CUDA gives you all of that.\\nOperator: Your final question comes from the line of Harlan Sur of J.P. Morgan. Your line is open.\\nHarlan Sur: Good afternoon. Thanks for taking my question. If you look at the history of the tech industry like those companies that have been successful have always been focused on ecosystem; silicon, hardware, software, strong partnerships and just as importantly, right, an aggressive cadence of new products, more segmentation over time. The team recently announced a more aggressive new product cadence in data center from two years to now every year with higher levels of segmentation, training, optimization in printing CPU, GPU, DPU networking. How do we think about your R&D OpEx growth outlook to support a more aggressive and expanding forward roadmap, but more importantly, what is the team doing to manage and drive execution through all of this complexity?\\nJensen Huang: Gosh. Boy, that's just really excellent. You just wrote NVIDIA's business plan, and so you described our strategy. First of all, there is a fundamental reason why we accelerate our execution. And the reason for that is because it fundamentally drives down cost. When the combination of TensorRT-LLM and H200 reduce the cost for our customers for large model inference by a factor of four, and so that includes, of course, our speeds and feeds, but mostly it's because of our software, mostly the software benefits because of the architecture. And so we want to accelerate our roadmap for that reason. The second reason is to expand the reach of generative AI, the world's number of data center configurations -- this is kind of the amazing thing. NVIDIA is in every cloud, but not one cloud is the same. NVIDIA is working with every single cloud service provider and not one of the networking control plane, security posture is the same. Everybody's platform is different and yet we're integrated into all of their stacks, all of their data centers and we work incredibly well with all of them. And not to mention, we then take the whole thing and we create AI factories that are standalone. We take our platform, we can put them into supercomputers, we can put them into enterprise. Bringing AI to enterprise is something generative AI Enterprise something nobody's ever done before. And we're right now in the process of going to market with all of that. And so the complexity includes, of course, all the technologies and segments and the pace. It includes the fact that we are architecturally compatible across every single one of those. It includes all of the domain specific libraries that we create. The reason why every computer company, without thinking, can integrate NVIDIA into their roadmap and take it to market. And the reason for that is, because there is market demand for it. There is market demand in healthcare, there is market demand in manufacturing, there is market demand, of course, in AI, including financial services, in supercomputing and quantum computing. The list of markets and segments that we have domain specific libraries is incredibly broaden. And then finally, we have an end-to-end solution for data centers; InfiniBand networking, Ethernet networking, x86, ARM, just about every permutation combination of solutions -- technology solutions and software stacks provided. And that translates to having the largest number of ecosystem software developers; the largest ecosystem of system makers; the largest and broadest distribution partnership network; and ultimately, the greatest reach. And that takes -- surely that takes a lot of energy. But the thing that really holds it together, and this is a great decision that we made decades ago, which is everything is architecturally compatible. When we develop a domain specific language that runs on one GPU, it runs on every GPU. When we optimize TensorRT for the cloud, we optimized it for enterprise. When we do something that brings in a new feature, a new library, a new feature or a new developer, they instantly get the benefit of all of our reach. And so that discipline, that architecture compatible discipline that has lasted more than a couple of decades now, is one of the reasons why NVIDIA is still really, really efficient. I mean, we're 28,000 people large and serving just about every single company, every single industry, every single market around the world.\\nOperator: Thank you. I will now turn the call back over to Jensen Huang for closing remarks.\\nJensen Huang: Our strong growth reflects the broad industry platform transition from general purpose to accelerated computing and generative AI. Large language models start-ups consumer Internet companies and global cloud service providers are the first movers. The next waves are starting to build. Nations and regional CSPs are building AI clouds to serve local demand. Enterprise software companies like Adobe and Dropbox, SAP and ServiceNow are adding AI copilots and assistants to their platforms. Enterprises in the world's largest industries are creating custom AIs to automate and boost productivity. The generative AI era is in full steam and has created the need for a new type of data center, an AI factory; optimized for refining data and training, and inference, and generating AI. AI factory workloads are different and incremental to legacy data center workloads supporting IT tasks. AI factories run copilots and AI assistants, which are significant software TAM expansion and are driving significant new investment. Expanding the $1 trillion traditional data center infrastructure installed base, empowering the AI Industrial Revolution. NVIDIA H100 HGX with InfiniBand and the NVIDIA AI software stack define an AI factory today. As we expand our supply chain to meet the world's demand, we are also building new growth drivers for the next wave of AI. We highlighted three elements to our new growth strategy that are hitting their stride: CPU, networking, and software and services. Grace is NVIDIA's first data center CPU. Grace and Grace Hopper are in full production and ramping into a new multi-billion dollar product line next year. Irrespective of the CPU choice, we can help customers build an AI factory. NVIDIA networking now exceeds $10 billion annualized revenue run rate. InfiniBand grew five-fold year-over-year, and is positioned for excellent growth ahead as the networking of AI factories. Enterprises are also racing to adopt AI and Ethernet is the standard networking. This week we announced an Ethernet for AI platform for enterprises. NVIDIA Spectrum-X is an end-to-end solution of Bluefield SuperNIC, Spectrum-4 Ethernet switch and software that boosts Ethernet performance by up to 1.6x for AI workloads. Dell, HPE and Lenovo have joined us to bring a full generative AI solution of NVIDIA AI computing, networking and software to the world's enterprises. NVIDIA software and services is on track to exit the year at an annualized run rate of $1 billion. Enterprise software platforms like ServiceNow and SAP need to build and operate proprietary AI. Enterprises need to build and deploy custom AI copilots. We have the AI technology, expertise and scale to help customers build custom models with their proprietary data on NVIDIA DGX Cloud and deploy the AI applications on enterprise grade NVIDIA AI Enterprise. NVIDIA is essentially an AI foundry. NVIDIA's GPUs, CPUs, networking, AI foundry services and NVIDIA AI Enterprise software are all growth engines in full throttle. Thanks for joining us today. We look forward to updating you on our progress next quarter.\\nOperator: This concludes today's conference call. You may now disconnect.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " \"NVIDIA's latest earnings call revealed record-breaking revenue of $18.1 billion, marking a 34% sequential increase and a 200% year-on-year increase. The company's Data Center segment was a primary driver of this growth, achieving record revenue of $14.5 billion. However, the company anticipates a significant decline in the fourth quarter due to new U.S. export control regulations affecting sales to China and other markets. \\nKey takeaways from the call include:NVIDIA's Data Center segment saw record revenue, driven by the NVIDIA HGX platform and InfiniBand networking. Major companies like Meta (NASDAQ:META) and Adobe (NASDAQ:ADBE) are integrating NVIDIA's technology into their platforms, contributing to the strong demand for AI supercomputers and data center infrastructure.NVIDIA is expanding its Data Center product portfolio to comply with new U.S. export control regulations. Despite expecting a decrease in sales to China and other markets in the fourth quarter, the company is focusing on national investment in sovereign AI infrastructure.The Gaming segment recorded revenue of $2.86 billion, a 15% sequential increase and an 80% year-on-year increase, driven by strong demand for NVIDIA's RTX ray tracing and AI technology.In the Pro Vis segment, revenue reached $416 million, a 10% sequential increase and a 108% year-on-year increase. NVIDIA RTX is the preferred platform for professional design, engineering, and simulation, with AI emerging as a demand driver.NVIDIA's CEO, Jensen Huang, discussed the expansion of AI factories and the transition into a new computing approach. He also expressed confidence in the continued growth of the Data Center business through 2025.During the call, NVIDIA highlighted its product innovations, including the H200 GPU, which offers faster and larger memory for generative AI and language models, and the GH200 Grace Hopper Superchip, a combination of an ARM-based CPU with a Hopper GPU. The Grace Hopper Superchip is being adopted by supercomputing customers, with the combined AI compute capacity of supercomputers built on Grace Hopper expected to exceed 200 exaflops next year. \\nIn addition to its product offerings, NVIDIA is also expanding its networking into the Ethernet space with its new Spectrum-X end-to-end Ethernet offering, which will be available in Q1 next year. This offering can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. \\nNVIDIA also discussed its business strategy, emphasizing the importance of reselling AI technology to customers and its growth in the enterprise market. The company introduced an AI foundry service to help enterprises build custom AI models and announced an aggressive product release schedule in the data center market. \\nLooking ahead, NVIDIA expects total revenue of $20 billion for Q4 2024, driven by strong demand in the Data Center sector. Despite regulatory challenges, the company is confident in its growth potential, citing expanded supply, new customers, and its entrance into the enterprise market.Full transcript -  Nvidia Corp  (NASDAQ:NVDA) Q3 2024:Operator: Good afternoon. My name is JL, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's Third Quarter Earnings Call. All lines have been placed on mute to prevent any background noise. After the speakers’ remarks, there will be a question-and-answer session. [Operator Instructions] Simona Jankowski, you may now begin your conference.\\nSimona Jankowski: Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2024. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter and fiscal 2024. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All statements are made as of today, November 21, 2023, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\\nColette Kress: Thanks, Simona. Q3 was another record quarter. Revenue of $18.1 billion was up 34% sequentially and up more than 200% year-on-year and well above our outlook for $16 billion. Starting with Data Center. The continued ramp of the NVIDIA HGX platform based on our Hopper Tensor Core GPU architecture, along with InfiniBand end-to-end networking, drove record revenue of $14.5 billion, up 41% sequentially and up 279% year-on-year. NVIDIA HGX with InfiniBand together are essentially the reference architecture for AI supercomputers and data center infrastructures. Some of the most exciting generative AI applications are built and run on NVIDIA, including Adobe Firefly, ChatGPT, Microsoft (NASDAQ:MSFT) 365 Copilot, CoAssist, now assist with ServiceNow (NYSE:NOW) and Zoom (NASDAQ:ZM) AI Companion. Our Data Center compute revenue quadrupled from last year and networking revenue nearly tripled. Investments in infrastructure for training and inferencing large language models, deep learning, recommender systems and generative AI applications is fueling strong broad-based demand for NVIDIA accelerated computing. Inferencing is now a major workload for NVIDIA AI computing. Consumer Internet companies and enterprises drove exceptional sequential growth in Q3, comprising approximately half of our Data Center revenue and outpacing total growth. Companies like Meta are in full production with deep learning, recommender systems and also investing in generative AI to help advertisers optimize images and text. Most major consumer Internet companies are racing to ramp up generative AI deployment. The enterprise wave of AI adoption is now beginning. Enterprise software companies such as Adobe, Databricks, Snowflake (NYSE:SNOW) and ServiceNow are adding AI copilots and the systems to their platforms. And broader enterprises are developing custom AI for vertical industry applications such as Tesla (NASDAQ:TSLA) in autonomous driving. Cloud service providers drove roughly the other half of our Data Center revenue in the quarter. Demand was strong from all hyperscale CSPs, as well as from a broadening set of GPU-specialized CSPs globally that are rapidly growing to address the new market opportunities in AI. NVIDIA H100 Tensor Core GPU instances are now generally available in virtually every cloud with instances in high demand. We have significantly increased supply every quarter this year to meet strong demand and expect to continue to do so next year. We will also have a broader and faster product launch cadence to meet the growing and diverse set of AI opportunities. Towards the end of the quarter, the U.S. government announced a new set of export control regulations for China and other markets, including Vietnam and certain countries in the Middle East. These regulations require licenses for the export of a number of our products, including our Hopper and Ampere 100 and 800 series and several others. Our sales to China and other affected destinations derived from products that are now subject to licensing requirements have consistently contributed approximately 20% to 25% of Data Center revenue over the past few quarters. We expect that our sales to these destinations will decline significantly in the fourth quarter. So we believe will be more than offset by strong growth in other regions. The U.S. government designed the regulation to allow the U.S. industry to provide data center compute products to markets worldwide, including China. Continuing to compete worldwide as the regulations encourage, promotes U.S. technology leadership, spurs economic growth and supports U.S. jobs. For the highest performance levels, the government requires licenses. For lower performance levels, the government requires a streamlined prior notification process. And for products even lower performance levels, the government does not require any notice at all. Following the government's clear guidelines, we are working to expand our Data Center product portfolio to offer compliance solutions for each regulatory category, including products for which the U.S. government does not wish to have advance notice before each shipment. We are working with some customers in China and the Middle East to pursue licenses from the U.S. government. It is too early to know whether these will be granted for any significant amount of revenue. Many countries are awakening to the need to invest in sovereign AI infrastructure to support economic growth and industrial innovation. With investments in domestic compute capacity, nations can use their own data to train LLMs and support their local generative AI ecosystems. For example, we are working with India's government and largest tech companies including  Infosys  (NS:INFY), Reliance and Tata to boost their sovereign AI infrastructure. And French private cloud provider, Scaleway, is building a regional AI cloud based on NVIDIA H100 InfiniBand and NVIDIA's AI Enterprise software to fuel advancement across France and Europe. National investment in compute capacity is a new economic imperative and serving the sovereign AI infrastructure market represents a multi-billion dollar opportunity over the next few years. From a product perspective, the vast majority of revenue in Q3 was driven by the NVIDIA HGX platform based on our Hopper GPU architecture with lower contribution from the prior generation Ampere GPU architecture. The new L40S GPU built for industry standard servers began to ship, supporting training and inference workloads across a variety of consumers. This was also the first revenue quarter of our GH200 Grace Hopper Superchip, which combines our ARM-based Grace CPU with a Hopper GPU. Grace and Grace Hopper are ramping into a new multi-billion dollar product line. Grace Hopper instances are now available at GPU specialized cloud providers, and coming soon to Oracle (NYSE:ORCL) Cloud. Grace Hopper is also getting significant traction with supercomputing customers. Initial shipments to Los Alamos National Lab and the Swiss National Supercomputing Center took place in the third quarter. The UK government announced it will build one of the world's fastest AI supercomputers called Isambard-AI with almost 5,500 Grace Hopper Superchips. German supercomputing center, Julich, also announced that it will build its next-generation AI supercomputer with close to 24,000 Grace Hopper Superchips and Quantum-2 InfiniBand, making it the world's most powerful AI supercomputer with over 90 exaflops of AI performance. All-in, we estimate that the combined AI compute capacity of all the supercomputers built on Grace Hopper across the U.S., Europe and Japan next year will exceed 200 exaflops with more wins to come. Inference is contributing significantly to our data center demand, as AI is now in full production for deep learning, recommenders, chatbots, copilots and text to image generation and this is just the beginning. NVIDIA AI offers the best inference performance and versatility, and thus the lower power and cost of ownership. We are also driving a fast cost reduction curve. With the release of TensorRT-LLM, we now achieved more than 2x the inference performance for half the cost of inferencing LLMs on NVIDIA GPUs. We also announced the latest member of the Hopper family, the H200, which will be the first GPU to offer HBM3e, faster, larger memory to further accelerate generative AI and LLMs. It moves inference speed up to another 2x compared to H100 GPUs for running LLMs like Norma2 (ph). Combined, TensorRT-LLM and H200, increased performance or reduced cost by 4x in just one year. With our customers changing their stack, this is a benefit of CUDA and our architecture compatibility. Compared to the A100, H200 delivers an 18x performance increase for inferencing models like GPT-3, allowing customers to move to larger models and with no increase in latency. Amazon (NASDAQ:AMZN) Web Services, Google (NASDAQ:GOOGL) Cloud, Microsoft Azure and Oracle Cloud will be among the first CSPs to offer H200-based instances starting next year. At last week's Microsoft Ignite, we deepened and expanded our collaboration with Microsoft across the entire stock. We introduced an AI foundry service for the development and tuning of custom generative AI enterprise applications running on Azure. Customers can bring their domain knowledge and proprietary data and we help them build their AI models using our AI expertise and software stock in our DGX cloud, all with enterprise grade security and support. SAP and  Amdocs  (NASDAQ:DOX) are the first customers of the NVIDIA AI foundry service on Microsoft Azure. In addition, Microsoft will launch new confidential computing instances based on the H100. The H100 remains the top performing and most versatile platform for AI training and by a wide margin, as shown in the latest MLPerf industry benchmark results. Our training cluster included more than 10,000 H100 GPUs or 3x more than in June, reflecting very efficient scaling. Efficient scaling is a key requirement in generative AI, because LLMs are growing by an order of magnitude every year. Microsoft Azure achieved similar results on a nearly identical cluster, demonstrating the efficiency of NVIDIA AI in public cloud deployments. Networking now exceeds a $10 billion annualized revenue run rate. Strong growth was driven by exceptional demand for InfiniBand, which grew fivefold year-on-year. InfiniBand is critical to gaining the scale and performance needed for training LLMs. Microsoft made this very point last week, highlighting that Azure uses over 29,000 miles of InfiniBand cabling, enough to circle the globe. We are expanding NVIDIA networking into the Ethernet space. Our new Spectrum-X end-to-end Ethernet offering with technologies, purpose built for AI, will be available in Q1 next year. With support from leading OEMs, including Dell (NYSE:DELL), HPE and Lenovo. Spectrum-X can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. Let me also provide an update on our software and services offerings, where we are starting to see excellent adoption. We are on track to exit the year at an annualized revenue run rate of $1 billion for our recurring software, support and services offerings. We see two primary opportunities for growth over the intermediate term with our DGX cloud service and with our NVIDIA AI Enterprise software, each reflects the growth of enterprise AI training and enterprise AI inference, respectively. Our latest DGX cloud customer announcement was this morning as part of an AI research collaboration with Gentech, the biotechnology pioneer also plans to use our BioNeMo LLM framework to help accelerate and optimize their AI drug discovery platform. We now have enterprise AI partnership with Adobe, Dropbox (NASDAQ:DBX), Getty, SAP, ServiceNow, Snowflake and others to come. Okay. Moving to Gaming. Gaming revenue of $2.86 billion was up 15% sequentially and up more than 80% year-on-year with strong demand in the important back-to-school shopping season with NVIDIA RTX ray tracing and AI technology now available at price points as low as $299. We entered the holidays with the best-ever line-up for gamers and creators. Gaming has doubled relative to pre-COVID levels even against the backdrop of lackluster PC market performance. This reflects the significant value we've brought to the gaming ecosystem with innovations like RTX and DLSS. The number of games and applications supporting these technologies has exploded in that period, driving upgrades and attracting new buyers. The RTX ecosystem continues to grow. There are now over 475 RTX-enabled games and applications. Generative AI is quickly emerging as the new pillar app for high performance PCs. NVIDIA RTX GPUs to find the most performance AI PCs and workstations. We just released TensorRT-LLM for Windows, which speeds on-device LLM inference up by 4x. With an installed base of over 100 million, NVIDIA RTX is the natural platform for AI application developers. Finally, our GeForce NOW cloud gaming service continues to build momentum. Its library of PC games surpassed 1,700 titles, including the launches of Alan Wake 2, Baldur's Gate 3, Cyberpunk 2077: Phantom Liberty and Starfield. Moving to the Pro Vis. Revenue of $416 million was up 10% sequentially and up 108% year-on year. NVIDIA RTX is the workstation platform of choice for professional design, engineering and simulation use cases and AI is emerging as a powerful demand driver. Early applications include inference for AI imaging in healthcare and edge AI in smart spaces and the public sector. We launched a new line of desktop workstations based on NVIDIA RTX Ada Lovelace generation GPUs and ConnectX, SmartNICs offering up to 2x the AI processing ray tracing and graphics performance of the previous generations. These powerful new workstations are optimized for AI workloads such as fine tune AI models, training smaller models and running inference locally. We continue to make progress on Omniverse, our software platform for designing, building and operating 3D virtual worlds. Mercedes-Benz (OTC:MBGAF) is using Omniverse powered digital twins to plan, design, build and operate its manufacturing and assembly facilities, helping it increase efficiency and reduce defects. Oxxon (ph) is also incorporating Omniverse into its manufacturing process, including end-to-end simulation for the entire robotics and automation pipeline, saving time and cost. We announced two new Omniverse Cloud services for automotive digitalization available on Microsoft Azure, a virtual factory simulation engine and autonomous vehicle simulation engine. Moving to Automotive. Revenue was $261 million, up 3% sequentially and up 4% year-on year, primarily driven by continued growth in self-driving platforms based on NVIDIA DRIVE Orin SOC and the ramp of AI cockpit solutions with global OEM customers. We extended our automotive partnership of Foxconn to include NVIDIA DRIVE for our next-generation automotive SOC. Foxconn has become the ODM for EVs. Our partnership provides Foxconn with a standard AV sensor and computing platform for their customers to easily build a state-of-an-art safe and secure software defined car. Now we're going to move to the rest of the P&L. GAAP gross margin expanded to 74% and non-GAAP gross margin to 75%, driven by higher Data Center sales and lower net inventory reserve, including a 1 percentage point benefit from the release of previously reserved inventory related to the Ampere GPU architecture products. Sequentially, GAAP operating expenses were up 12% and non-GAAP operating expenses were up 10%, primarily reflecting increased compensation and benefits. Let me turn to the fourth quarter of fiscal 2024. Total revenue is expected to be $20 billion, plus or minus 2%. We expect strong sequential growth to be driven by Data Center, with continued strong demand for both compute and networking. Gaming will likely decline sequentially as it is now more aligned with notebook seasonality. GAAP and non-GAAP gross margins are expected to be 74.5% and 75.5%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $3.17 billion and $2.2 billion, respectively. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $200 million, excluding gains and losses from non-affiliated investments. GAAP and non-GAAP tax rates are expected to be 15%, plus or minus 1% excluding any discrete items. Further financial information are included in the CFO commentary and other information available on our IR website. In closing, let me highlight some upcoming events for the financial community. We will attend the UBS Global Technology Conference in Scottsdale, Arizona, on November 28; the Wells Fargo TMT Summit in Rancho Palos Verdes, California on November 29; the Arete Virtual Tech Conference on December 7; and the J.P. Morgan Health Care Conference in San Francisco on January 8. Our earnings call to discuss the results of our fourth quarter and fiscal 2024 is scheduled for Wednesday, February 21. We will now open the call for questions. Operator, will you please poll for questions.\\nOperator: [Operator Instructions] Your first question comes from the line of Vivek Arya of Bank of America. Your line is open.\\nVivek Arya: Thanks for taking my question. Just, Colette, wanted to clarify what China contributions are you expecting in Q4. And then, Jensen, the main question is for you, where do you think we are in the adoption curve in terms of your shipments into the generative AI market? Because when I just look at the trajectory of your data center, is growth -- it will be close to nearly 30% of all the spending in data center next year. So what metrics are you keeping an eye on to inform you that you can continue to grow? Just where are we in the adoption curve of your products into the generative AI market? Thank you.\\nColette Kress: So, first let me start with your question, Vivek, on export controls and the impacts that we are seeing in our Q4 outlook and guidance that we provided. We had seen historically over the last several quarters that China and some of the other impacted destinations to be about 20% to 25% of our Data Center revenue. We are expecting in our guidance for that to decrease substantially as we move into Q4. The export controls will have a negative effect on our China business. And we do not have good visibility into the magnitude of that impact even over the long-term. We are though working to expand our Data Center product portfolio to possibly offer new regulation compliance solutions that do not require a license, these products, they may become available in the next coming months. However, we don't expect their contribution to be material or meaningful as a percentage of the revenue in Q4.\\nJensen Huang: Generative AI is the largest TAM expansion of software and hardware that we've seen in several decades. At the core of it, what's really exciting is that, what was largely a retrieval based computing approach, almost everything that you do is retrieved off of storage somewhere, has been augmented now, added with a generative method. And it's changed almost everything. You could see that text-to-text, text-to-image, text-to-video, text-to-3D, text-to-protein, text-to-chemicals, these were things that were processed and typed in by humans in the past. And these are now generative approaches. The way that we access data is changed. It used to be based on explicit queries. It is now based on natural language queries, intention queries, semantic queries. And so, we're excited about the work that we're doing with SAP and Dropbox and many others that you're going to hear about. And one of the areas that is really impactful is the software industry, which is about $1 trillion or so, has been building tools that are manually used over the last couple of decades. And now there's a whole new segment of software called copilots and assistants. Instead of manually used, these tools will have copilots to help you use it. And so, instead of licensing software, we will continue to do that, of course, but we will also hire copilots and assistants to help us use these -- use the software. We'll connect all of these copilots and assistants into teams of AIs, which is going to be the modern version of software, modern version of enterprise business software. And so the transformation of software and the way that software has done is driving the hardware underneath. And you can see that it's transforming hardware in two ways. One is something that's largely independent of generative AI. There's two trends: one is related to accelerated computing, general purpose computing is too wasteful of energy and cost. And now that we have much, much better approaches, call it, accelerated computing, you could save an order of magnitude of energy, you can save an order of magnitude of time or you can save an order of magnitudes of cost by using acceleration. And so, accelerated computing is transitioning, if you will, general purpose computing into this new approach. And that's been augmented by a new class of data centers. This is the traditional data centers that you were just talking about where we represent about a third of that. But there is a new class of data centers and this new class of data centers, unlike the data centers of the past, where you have a lot of applications running used by a great many people that are different tenants that are using the same infrastructure and that data center stores a lot of files. These new data centers are very few applications, if not one application, used by basically one tenant and it processes data, it trains models and then generates tokens and generates AI. And we call these new data centers AI factories. We're seeing AI factories being built out everywhere, and just by every country. And so if you look at the way where we are in the expansion, the transition into this new computing approach, the first wave you saw with large language model start-ups, generative AI start-ups and consumer Internet companies, and weren't in the process of ramping that. Meanwhile, while that's being ramped, you see that we're starting to partner with enterprise software companies who would like to build chatbots and copilots and assistants to augment the tools that they have on their platforms. You're seeing GPU specialized CSPs cropping up all over the world and they are dedicated to do really one thing, which is processing AI. You're seeing sovereign AI infrastructures, people -- countries that now recognize that they have to utilize their own data, keep their own data, keep their own culture, process that data and develop their own AI. You see that in India. Several -- about a year ago in Sweden, you are seeing in Japan. Last week, a big announcement in France. But the number of sovereign AI clouds that are being built is really quite significant. And my guess is that almost every major region will have and surely every major country will have their own AI clouds. And so I think you're seeing just new developments as the generative AI wave propagates through every industry, every company, every region. And so we're at the beginning of this inflection, this computing transition.\\nOperator: Your next question comes from the line of Aaron Rakers of Wells Fargo. Your line is open.\\nAaron Rakers: Yeah. Thanks for taking the question. I wanted to ask about kind of the networking side of the business. Given the growth rates that you've now cited, I think, it's 155% year-over-year and strong growth sequentially, it looks like that business is like almost approaching $2.5 billion to $3 billion quarterly level. I'm curious of how you see Ethernet involved evolving and maybe how you would characterize your differentiation of Spectrum-X relative to the traditional Ethernet stack as we start to think about that becoming part of the networking narrative above and maybe beyond just InfiniBand as we look into next year? Thank you.\\nJensen Huang: Yeah. Thanks for the question. Our networking business is already on a $10 billion plus run rate and it's going to get much larger. And as you mentioned, we added a new networking platform to our networking business recently. The vast majority of the dedicated large scale AI factories standardize on InfiniBand. And the reason for that is not only because of its data rate and not only just the latency, but the way that it moves traffic around the network is really important. The way that you process AI and a multi-tenant hyperscale Ethernet environment, the traffic pattern is just radically different. And with InfiniBand and with software defined networks, we could do congestion control, adaptive routing, performance isolation and noise isolation, not to mention, of course, the day rate and the low latency that -- and a very low overhead of InfiniBand that's natural part of InfiniBand. And so, InfiniBand is not so much just the network, it's also a computing fabric. We've put a lot of software-defined capabilities into the fabric including computation. We will do 40-point calculations and computation right on the switch, and right in the fabric itself. And so that's the reason why that difference in Ethernet versus InfiniBand or InfiniBand versus Ethernet for AI factories is so dramatic. And the difference is profound. And the reason for that is because you've just invested in a $2 billion infrastructure for AI factories. A 20%, 25%, 30% difference in overall effectiveness, especially as you scale up is measured in hundreds of millions of dollars of value. And if you will, renting that infrastructure over the course of four to five years, it really, really adds up. And so InfiniBand's value proposition is undeniable for AI factories. However, as we move AI into enterprise. This is enterprise computing what we'd like to enable every company to be able to build their own custom AIs. We're building customer AIs in our company based on our proprietary data, our proprietary type of skills. For example, recently we spoke about one of the models that we're creating, it's called ChipNeMo; we're building many others. There'll be tens, hundreds of custom AI models that we create inside our company. And our company is -- for all of our employee use, doesn't have to be as high performance as the AI factories we used to train the models. And so we would like the AI to be able to run in Ethernet environment. And so what we've done is we invented this new platform that extends Ethernet; doesn't replace Ethernet, it's 100% compliant with Ethernet. And it's optimized for East-West traffic, which is where the computing fabric is. It adds to Ethernet with an end-to-end solution with Bluefield, as well as our Spectrum switch that allows us to perform some of the capabilities that we have in InfiniBand, not all but some. And we achieved excellent results. And the way we go to market is we go to market with our large enterprise partners who already offer our computing solution. And so, HP (NYSE:HPQ), Dell and Lenovo has the NVIDIA AI stack, the NVIDIA AI Enterprise software stack and now they integrate with Bluefield, as well as bundle -- take a market there, Spectrum switch, and they'll be able to offer enterprise customers all over the world with their vast sales force and vast network of resellers a fully integrated, if you will, fully optimized, at least end-to-end AI solution. And so that's basically it, bringing AI to Ethernet for the world's enterprise.\\nOperator: Thank you. Your next question comes from the line of Joe Moore of Morgan Stanley. Your line is open.\\nJoseph Moore: Great. Thank you. I'm wondering if you could talk a little bit more about Grace Hopper and how you see the ability to leverage kind of the microprocessor, how you see that as a TAM expander. And what applications do you see using Grace Hopper versus more traditional H100 applications?\\nJensen Huang: Yeah. Thanks for the question. Grace Hopper is in production -- in high volume production now. We're expecting next year just with all of the design wins that we have in high performance computing and AI infrastructures, we are on a very, very fast ramp with our first data center CPU to a multi-billion dollar product line. This is going to be a very large product line for us. The capability of Grace Hopper is really quite spectacular. It has the ability to create computing nodes that simultaneously has very fast memory, as well as very large memory. In the areas of vector databases or semantic surge, what is called RAG, retrieval augmented generation. So that you could have a generative AI model be able to refer to proprietary data or a factual data before it generates a response, that data is quite large. And you can also have applications or generative models where the context length is very high. You basically store it in entire book into end-to-end system memory before you ask your questions. And so the context length can be quite large this way. The generative models has the ability to still be able to naturally interact with you on one hand. On the other hand, be able to refer to factual data, proprietary data or domain-specific data, you data and be contextually relevant and reduce hallucination. And so that particular use case for example is really quite fantastic for Grace Hopper. It also serves the customers that really care to have a different CPU than x86. Maybe it's a European supercomputing centers or European companies who would like to build up their own ARM ecosystem and like to build up a full stack or CSPs that have decided that they would like to pivot to ARM, because their own custom CPUs are based on ARM. There are variety of different reasons that drives the success of Grace Hopper, but we're off to a just an extraordinary start. This is a home run product.\\nOperator: Your next question comes from the line of Tim Arcuri of UBS. Your line is open.\\nTim Arcuri: Hi. Thanks. I wanted to ask a little bit about the visibility that you have on revenue. I know there's a few moving parts. I guess, on one hand, the purchase commitments went up a lot again. But on the other hand, China bans would arguably pull in when you can fill the demand beyond China. So I know we're not even into 2024 yet and it doesn't sound like, Jensen, you think that next year would be a peak in your Data Center revenue, but I just wanted to sort of explicitly ask you that. Do you think that Data Center can grow even in 2025? Thanks.\\nJensen Huang: Absolutely believe the Data Center can grow through 2025. And there are, of course, several reasons for that. We are expanding our supply quite significantly. We have already one of the broadest and largest and most capable supply chain in the world. Now, remember, people think that the GPU is a chip. But the HGX H100, the Hopper HGX has 35,000 parts, it weighs 70 pounds. Eight of the chips are Hopper. The other 35,000 are not. It is -- even its passive components are incredible. High voltage parts. High frequency parts. High current parts. It is a supercomputer, and therefore, the only way to test a supercomputer is with another supercomputer. Even the manufacturing of it is complicated, the testing of it is complicated, the shipping of it complicated and installation is complicated. And so, every aspect of our HGX supply chain is complicated. And the remarkable team that we have here has really scaled out the supply chain incredibly. Not to mention, all of our HGXs are connected with NVIDIA networking. And the networking, the transceivers, the mix, the cables, the switches, the amount of complexity there is just incredible. And so, I'm just -- first of all, I'm just super proud of the team for scaling up this incredible supply chain. We are absolutely world class. But meanwhile, we're adding new customers and new products. So we have new supply. We have new customers, as I was mentioning earlier. Different regions are standing up GPU specialist clouds, sovereign AI clouds coming out from all over the world, as people realize that they can't afford to export their country's knowledge, their country's culture for somebody else to then resell AI back to them, they have to -- they should, they have the skills and surely with us in combination, we can help them to do that build up their national AI. And so, the first thing that they have to do is, create their AI cloud, national AI cloud. You're also seeing us now growing into enterprise. The enterprise market has two paths. One path -- or if I could say three paths. The first path, of course, just off-the-shelf AI. And there are of course Chat GPT, a fabulous off-the-shelf AI, there'll be others. There's also a proprietary AI, because software companies like ServiceNow and SAP, there are many, many others that can't afford to have their company's intelligence be outsourced to somebody else. And they are about building tools and on top of their tools they should build custom and proprietary and domain-specific copilots and assistants that they can then rent to their customer base. This is -- they're sitting on a goldmine, almost every major tools company in the world is sitting on a goldmine, and they recognize that they have to go build their own custom AIs. We have a new service called an AI foundry, where we leverage NVS (ph) capabilities to be able to serve them in that. And then the next one is enterprises building their own custom AIs, their own custom chatbots, their own custom RAGs. And this capability is spreading all over the world. And the way that we're going to serve that marketplace is with the entire stacks of systems, which includes our compute, our networking and our switches, running our software stack called NVIDIA AI Enterprise, taking it through our market partners, HP, Dell, Lenovo, so on and so forth. And so we're just -- we're seeing the waves of generative AI starting from the start-ups and CSPs, moving to consumer Internet companies, moving to enterprise software platforms, moving to enterprise companies. And then ultimately, one of the areas that you guys have seen us spend a lot of energy on has to do with industrial generative AI. This is where NVIDIA AI and NVIDIA Omniverse comes together and that is a really, really exciting work. And so I think the -- we're at the beginning of a basically across-the-board industrial transition to generative AI to accelerated computing. This is going to affect every company, every industry, every country.\\nOperator: Your next question comes from the line of Toshiya Hari of Goldman Sachs. Your line is open.\\nToshiya Hari: Hi. Thank you. I wanted to clarify something with Colette real quick, and then I had a question for Jensen as well. Colette, you mentioned that you'll be introducing regulation-compliant products over the next couple of months. Yet, the contribution to Q4 revenue should be relatively limited. Is that a timing issue and could it be a source of reacceleration in growth for Data Center in April and beyond or are the price points such that the contribution to revenue going forward should be relatively limited? And then the question for Jensen, the AI foundry service announcement from last week. I just wanted to ask about that, and hopefully, have you expand on it. How is the monetization model going to work? Is it primarily services and software revenue? How should we think about the long term opportunity set? And is this going to be exclusive to Microsoft or do you have plans to expand to other partners as well? Thank you.\\nColette Kress: Thanks, Toshiya. On the question regarding potentially new products that we could provide to our China customers. It's a significant process to both design and develop these new products. As we discussed, we're going to make sure that we are in full discussions with the U.S. government of our intent to move products as well. Given our state about where we are in the quarter, we're already several weeks into the quarter. So it's just going to take some time for us to go through and discussing with our customers the needs and desires of these new products that we have. And moving forward, whether that's medium-term or long-term, it's just hard to say both the [Technical Difficulty] of what we can produce with the U.S. government and what the interest of our China customers in this. So we stay still focused on finding that right balance for our China customers, but it's hard to say at this time.\\nJensen Huang: Toshiya, thanks for the question. There is a glaring opportunity in the world for AI foundry, and it makes so much sense. First, every company has its core intelligence. It makes up our company. Our data, our domain expertise, in the case of many companies, we create tools, and most of the software companies in the world are tool platforms, and those tools are used by people today. And in the future, it's going to be used by people augmented with a whole bunch of AIs that we hire. And these platforms just got to go across the world and you'll see and we've only announced a few; SAP, ServiceNow, Dropbox, Getty, many others are coming. And the reason for that is because they have their own proprietary AI. They want their own proprietary AI. They can't afford to outsource their intelligence and handout their data, and handout their flywheel for other companies to build the AI for them. And so, they come to us. We have several things that are really essential in a foundry. Just as TSMC as a foundry, you have to have AI technology. And as you know, we have just an incredible depth of AI capability -- AI technology capability. And then second, you have to have the best practice known practice, the skills of processing data through the invention of AI models to create AIs that are guardrails, fine-tuned, so on and so forth, that are safe, so on and so forth. And the third thing is you need factories. And that's what DGX Cloud is. Our AI models are called AI Foundations. Our process, if you will, our CAD system for creating AIs are called NeMo and they run on NVIDIA's factories we call DGX Cloud. Our monetization model is that with each one of our partners they rent a sandbox on DGX Cloud, where we work together, they bring their data, they bring their domain expertise, we bring our researchers and engineers, we help them build their custom AI. We help them make that custom AI incredible. Then that custom AI becomes theirs. And they deploy it on the runtime that is enterprise grade, enterprise optimized or outperformance optimized, runs across everything NVIDIA. We have a giant installed base in the cloud, on-prem, anywhere. And it's secure, securely patched, constantly patched and optimized and supported. And we call that NVIDIA AI Enterprise. NVIDIA AI Enterprise is $4,500 per GP per year, that's our business model. Our business model is basically a license. Our customers then with that basic license can build their monetization model on top of. In a lot of ways we're wholesale, they become retail. They could have a per -- they could have subscription license base, they could per instance or they could do per usage, there is a lot of different ways that they could take a -- create their own business model, but ours is basically like a software license, like an operating system. And so our business model is help you create your custom models, you run those custom models on NVIDIA AI Enterprise. And it's off to a great start. NVIDIA AI Enterprise is going to be a very large business for us.\\nOperator: Your next question comes from the line of Stacy Rasgon of Bernstein Research. Your line is open.\\nStacy Rasgon: Hi, guys. Thanks for taking my questions. Colette, I wanted to know if it weren't for the China restrictions would the Q4 guide has been higher or are you supply-constrained in just reshipping stuff that would have gone to China elsewhere? And I guess along those lines you give us a feeling for where your lead times are right now in data center and just the China redirection such as-is, is it lowering those lead times, because you've got parts that are sort of immediately available to ship?\\nColette Kress: Yeah. Stacy, let me see if I can help you understand. Yes, there are still situations where we are working on both improving our supply each and every quarter. We've done a really solid job of ramping every quarter, which has defined our revenue. But with the absence of China for our outlook for Q4, sure, there could have been some things that we are not supply-constrained that we could have sold, but kind of we would no longer can. So could our guidance had been a little higher in our Q4? Yes. We are still working on improving our supply on plan, on continuing growing all throughout next year as well towards that.\\nOperator: Your next question comes from the line of Matt Ramsay of TD Cowen. Your line is open.\\nMatt Ramsay: Thank you very much. Congrats, everybody, on the results. Jensen, I had a two-part question for you, and it comes off of sort of one premise. And the premise is, I still get a lot of questions from investors thinking about AI training as being NVIDIA's dominant domain and somehow inference, even large model inference takes more and more of the TAM that the market will become more competitive. You'll be less differentiated et cetera., et cetera. So I guess the two parts of the question are: number one, maybe you could spend a little bit of time talking about the evolution of the inference workload as we move to LLMs and how your company is positioned for that rather than smaller model inference. And second, up until a month or two ago, I never really got any questions at all about the data processing piece of the AI workloads. So the pieces of manipulating the data before training, between training and inference, after inference and I think that's a large part of the workload now. Maybe you could talk about how CUDA is enabling acceleration of those pieces of the workload. Thanks.\\nJensen Huang: Sure. Inference is complicated. It's actually incredibly complicated. If you -- we this quarter announced one of the most exciting new engines, optimizing compilers called TensorRT-LLM. The reception has been incredible. You got to GitHub, it's been downloaded a ton, a whole lot of stars, integrated into stacks and frameworks all over the world, almost instantaneously. And there are several reasons for that, obviously. We could create TensorRT-LLM, because CUDA is programmable. If CUDA and our GPUs were not so programmable, it would really be hard for us to improve software stacks at the pace that we do. TensorRT-LLM, on the same GPU, without anybody touching anything, improves the performance by a factor of two. And then on top of that, of course, the pace of our innovation is so high. H200 increases it by another factor of two. And so, our inference performance, another way of saying inference cost, just reduced by a factor of four within about a year's time. And so, that's really, really hard to keep up with. The reason why everybody likes our inference engine is because our installed base. We've been dedicated to our installed base for 20 years, 20-plus years. We have an installed base that is not only largest in every single cloud, it's in every available from every enterprise system maker, it's used by companies of just about every industry. And every -- anytime you see a NVIDIA GPU, it runs our stack. It's architecturally compatible, something we've been dedicated to for a very long time. We're very disciplined about it. We make it our, if you will, architecture compatibility is job one. And that has conveyed to the world, the certainty of our platform stability. NVIDIA's platform stability certainty is the reason why everybody builds on us first and the reason why everybody optimizes on us first. All the engineering and all the work that you do, all the invention of technologies that you build on top of NVIDIA accrues to the -- and benefits everybody that uses our GPUs. And we have such a large installed base, large -- millions and millions of GPUs in cloud, 100 million GPUs from people’s PCs just about every workstation in the world, and they all architecturally compatible. And so, if you are an inference platform and you're deploying an inference application, you are basically an application provider. And as a software application provider, you're looking for large installed base. Data processing, before you could train a model, you have to curate the data, you have to dedupe the data, maybe you have to augment the data with synthetic data. So, process the data, clean the data, align the data, normalize the data, all of that data is measured not in bytes or megabytes, it's measured in terabytes and petabytes. And the amount of data processing that you do before data engineering, before that you do training is quite significant. It could represent 30%, 40%, 50% of the amount of work that you ultimately do. And what you -- and ultimately creating a data driven machine learning service. And so data processing is just a massive part. We accelerate Spark, we accelerate Python. One of the coolest things that we just did is called cuDF Pandas. Without one line of code, Pandas, which is the single most successful data science framework in the world. Pandas now is accelerated by NVIDIA CUDA. And just out-of-the box, without the line of code and so the acceleration is really quite terrific and people are just incredibly excited about it. And Pandas was designed for one purpose and one purpose only, really data processing, it's for data science. And so NVIDIA CUDA gives you all of that.\\nOperator: Your final question comes from the line of Harlan Sur of J.P. Morgan. Your line is open.\\nHarlan Sur: Good afternoon. Thanks for taking my question. If you look at the history of the tech industry like those companies that have been successful have always been focused on ecosystem; silicon, hardware, software, strong partnerships and just as importantly, right, an aggressive cadence of new products, more segmentation over time. The team recently announced a more aggressive new product cadence in data center from two years to now every year with higher levels of segmentation, training, optimization in printing CPU, GPU, DPU networking. How do we think about your R&D OpEx growth outlook to support a more aggressive and expanding forward roadmap, but more importantly, what is the team doing to manage and drive execution through all of this complexity?\\nJensen Huang: Gosh. Boy, that's just really excellent. You just wrote NVIDIA's business plan, and so you described our strategy. First of all, there is a fundamental reason why we accelerate our execution. And the reason for that is because it fundamentally drives down cost. When the combination of TensorRT-LLM and H200 reduce the cost for our customers for large model inference by a factor of four, and so that includes, of course, our speeds and feeds, but mostly it's because of our software, mostly the software benefits because of the architecture. And so we want to accelerate our roadmap for that reason. The second reason is to expand the reach of generative AI, the world's number of data center configurations -- this is kind of the amazing thing. NVIDIA is in every cloud, but not one cloud is the same. NVIDIA is working with every single cloud service provider and not one of the networking control plane, security posture is the same. Everybody's platform is different and yet we're integrated into all of their stacks, all of their data centers and we work incredibly well with all of them. And not to mention, we then take the whole thing and we create AI factories that are standalone. We take our platform, we can put them into supercomputers, we can put them into enterprise. Bringing AI to enterprise is something generative AI Enterprise something nobody's ever done before. And we're right now in the process of going to market with all of that. And so the complexity includes, of course, all the technologies and segments and the pace. It includes the fact that we are architecturally compatible across every single one of those. It includes all of the domain specific libraries that we create. The reason why every computer company, without thinking, can integrate NVIDIA into their roadmap and take it to market. And the reason for that is, because there is market demand for it. There is market demand in healthcare, there is market demand in manufacturing, there is market demand, of course, in AI, including financial services, in supercomputing and quantum computing. The list of markets and segments that we have domain specific libraries is incredibly broaden. And then finally, we have an end-to-end solution for data centers; InfiniBand networking, Ethernet networking, x86, ARM, just about every permutation combination of solutions -- technology solutions and software stacks provided. And that translates to having the largest number of ecosystem software developers; the largest ecosystem of system makers; the largest and broadest distribution partnership network; and ultimately, the greatest reach. And that takes -- surely that takes a lot of energy. But the thing that really holds it together, and this is a great decision that we made decades ago, which is everything is architecturally compatible. When we develop a domain specific language that runs on one GPU, it runs on every GPU. When we optimize TensorRT for the cloud, we optimized it for enterprise. When we do something that brings in a new feature, a new library, a new feature or a new developer, they instantly get the benefit of all of our reach. And so that discipline, that architecture compatible discipline that has lasted more than a couple of decades now, is one of the reasons why NVIDIA is still really, really efficient. I mean, we're 28,000 people large and serving just about every single company, every single industry, every single market around the world.\\nOperator: Thank you. I will now turn the call back over to Jensen Huang for closing remarks.\\nJensen Huang: Our strong growth reflects the broad industry platform transition from general purpose to accelerated computing and generative AI. Large language models start-ups consumer Internet companies and global cloud service providers are the first movers. The next waves are starting to build. Nations and regional CSPs are building AI clouds to serve local demand. Enterprise software companies like Adobe and Dropbox, SAP and ServiceNow are adding AI copilots and assistants to their platforms. Enterprises in the world's largest industries are creating custom AIs to automate and boost productivity. The generative AI era is in full steam and has created the need for a new type of data center, an AI factory; optimized for refining data and training, and inference, and generating AI. AI factory workloads are different and incremental to legacy data center workloads supporting IT tasks. AI factories run copilots and AI assistants, which are significant software TAM expansion and are driving significant new investment. Expanding the $1 trillion traditional data center infrastructure installed base, empowering the AI Industrial Revolution. NVIDIA H100 HGX with InfiniBand and the NVIDIA AI software stack define an AI factory today. As we expand our supply chain to meet the world's demand, we are also building new growth drivers for the next wave of AI. We highlighted three elements to our new growth strategy that are hitting their stride: CPU, networking, and software and services. Grace is NVIDIA's first data center CPU. Grace and Grace Hopper are in full production and ramping into a new multi-billion dollar product line next year. Irrespective of the CPU choice, we can help customers build an AI factory. NVIDIA networking now exceeds $10 billion annualized revenue run rate. InfiniBand grew five-fold year-over-year, and is positioned for excellent growth ahead as the networking of AI factories. Enterprises are also racing to adopt AI and Ethernet is the standard networking. This week we announced an Ethernet for AI platform for enterprises. NVIDIA Spectrum-X is an end-to-end solution of Bluefield SuperNIC, Spectrum-4 Ethernet switch and software that boosts Ethernet performance by up to 1.6x for AI workloads. Dell, HPE and Lenovo have joined us to bring a full generative AI solution of NVIDIA AI computing, networking and software to the world's enterprises. NVIDIA software and services is on track to exit the year at an annualized run rate of $1 billion. Enterprise software platforms like ServiceNow and SAP need to build and operate proprietary AI. Enterprises need to build and deploy custom AI copilots. We have the AI technology, expertise and scale to help customers build custom models with their proprietary data on NVIDIA DGX Cloud and deploy the AI applications on enterprise grade NVIDIA AI Enterprise. NVIDIA is essentially an AI foundry. NVIDIA's GPUs, CPUs, networking, AI foundry services and NVIDIA AI Enterprise software are all growth engines in full throttle. Thanks for joining us today. We look forward to updating you on our progress next quarter.\\nOperator: This concludes today's conference call. You may now disconnect.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " \"NVIDIA's latest earnings call revealed record-breaking revenue of $18.1 billion, marking a 34% sequential increase and a 200% year-on-year increase. The company's Data Center segment was a primary driver of this growth, achieving record revenue of $14.5 billion. However, the company anticipates a significant decline in the fourth quarter due to new U.S. export control regulations affecting sales to China and other markets. \\nKey takeaways from the call include:NVIDIA's Data Center segment saw record revenue, driven by the NVIDIA HGX platform and InfiniBand networking. Major companies like Meta (NASDAQ:META) and Adobe (NASDAQ:ADBE) are integrating NVIDIA's technology into their platforms, contributing to the strong demand for AI supercomputers and data center infrastructure.NVIDIA is expanding its Data Center product portfolio to comply with new U.S. export control regulations. Despite expecting a decrease in sales to China and other markets in the fourth quarter, the company is focusing on national investment in sovereign AI infrastructure.The Gaming segment recorded revenue of $2.86 billion, a 15% sequential increase and an 80% year-on-year increase, driven by strong demand for NVIDIA's RTX ray tracing and AI technology.In the Pro Vis segment, revenue reached $416 million, a 10% sequential increase and a 108% year-on-year increase. NVIDIA RTX is the preferred platform for professional design, engineering, and simulation, with AI emerging as a demand driver.NVIDIA's CEO, Jensen Huang, discussed the expansion of AI factories and the transition into a new computing approach. He also expressed confidence in the continued growth of the Data Center business through 2025.During the call, NVIDIA highlighted its product innovations, including the H200 GPU, which offers faster and larger memory for generative AI and language models, and the GH200 Grace Hopper Superchip, a combination of an ARM-based CPU with a Hopper GPU. The Grace Hopper Superchip is being adopted by supercomputing customers, with the combined AI compute capacity of supercomputers built on Grace Hopper expected to exceed 200 exaflops next year. \\nIn addition to its product offerings, NVIDIA is also expanding its networking into the Ethernet space with its new Spectrum-X end-to-end Ethernet offering, which will be available in Q1 next year. This offering can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. \\nNVIDIA also discussed its business strategy, emphasizing the importance of reselling AI technology to customers and its growth in the enterprise market. The company introduced an AI foundry service to help enterprises build custom AI models and announced an aggressive product release schedule in the data center market. \\nLooking ahead, NVIDIA expects total revenue of $20 billion for Q4 2024, driven by strong demand in the Data Center sector. Despite regulatory challenges, the company is confident in its growth potential, citing expanded supply, new customers, and its entrance into the enterprise market.Full transcript -  Nvidia Corp  (NASDAQ:NVDA) Q3 2024:Operator: Good afternoon. My name is JL, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's Third Quarter Earnings Call. All lines have been placed on mute to prevent any background noise. After the speakers’ remarks, there will be a question-and-answer session. [Operator Instructions] Simona Jankowski, you may now begin your conference.\\nSimona Jankowski: Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2024. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter and fiscal 2024. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All statements are made as of today, November 21, 2023, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\\nColette Kress: Thanks, Simona. Q3 was another record quarter. Revenue of $18.1 billion was up 34% sequentially and up more than 200% year-on-year and well above our outlook for $16 billion. Starting with Data Center. The continued ramp of the NVIDIA HGX platform based on our Hopper Tensor Core GPU architecture, along with InfiniBand end-to-end networking, drove record revenue of $14.5 billion, up 41% sequentially and up 279% year-on-year. NVIDIA HGX with InfiniBand together are essentially the reference architecture for AI supercomputers and data center infrastructures. Some of the most exciting generative AI applications are built and run on NVIDIA, including Adobe Firefly, ChatGPT, Microsoft (NASDAQ:MSFT) 365 Copilot, CoAssist, now assist with ServiceNow (NYSE:NOW) and Zoom (NASDAQ:ZM) AI Companion. Our Data Center compute revenue quadrupled from last year and networking revenue nearly tripled. Investments in infrastructure for training and inferencing large language models, deep learning, recommender systems and generative AI applications is fueling strong broad-based demand for NVIDIA accelerated computing. Inferencing is now a major workload for NVIDIA AI computing. Consumer Internet companies and enterprises drove exceptional sequential growth in Q3, comprising approximately half of our Data Center revenue and outpacing total growth. Companies like Meta are in full production with deep learning, recommender systems and also investing in generative AI to help advertisers optimize images and text. Most major consumer Internet companies are racing to ramp up generative AI deployment. The enterprise wave of AI adoption is now beginning. Enterprise software companies such as Adobe, Databricks, Snowflake (NYSE:SNOW) and ServiceNow are adding AI copilots and the systems to their platforms. And broader enterprises are developing custom AI for vertical industry applications such as Tesla (NASDAQ:TSLA) in autonomous driving. Cloud service providers drove roughly the other half of our Data Center revenue in the quarter. Demand was strong from all hyperscale CSPs, as well as from a broadening set of GPU-specialized CSPs globally that are rapidly growing to address the new market opportunities in AI. NVIDIA H100 Tensor Core GPU instances are now generally available in virtually every cloud with instances in high demand. We have significantly increased supply every quarter this year to meet strong demand and expect to continue to do so next year. We will also have a broader and faster product launch cadence to meet the growing and diverse set of AI opportunities. Towards the end of the quarter, the U.S. government announced a new set of export control regulations for China and other markets, including Vietnam and certain countries in the Middle East. These regulations require licenses for the export of a number of our products, including our Hopper and Ampere 100 and 800 series and several others. Our sales to China and other affected destinations derived from products that are now subject to licensing requirements have consistently contributed approximately 20% to 25% of Data Center revenue over the past few quarters. We expect that our sales to these destinations will decline significantly in the fourth quarter. So we believe will be more than offset by strong growth in other regions. The U.S. government designed the regulation to allow the U.S. industry to provide data center compute products to markets worldwide, including China. Continuing to compete worldwide as the regulations encourage, promotes U.S. technology leadership, spurs economic growth and supports U.S. jobs. For the highest performance levels, the government requires licenses. For lower performance levels, the government requires a streamlined prior notification process. And for products even lower performance levels, the government does not require any notice at all. Following the government's clear guidelines, we are working to expand our Data Center product portfolio to offer compliance solutions for each regulatory category, including products for which the U.S. government does not wish to have advance notice before each shipment. We are working with some customers in China and the Middle East to pursue licenses from the U.S. government. It is too early to know whether these will be granted for any significant amount of revenue. Many countries are awakening to the need to invest in sovereign AI infrastructure to support economic growth and industrial innovation. With investments in domestic compute capacity, nations can use their own data to train LLMs and support their local generative AI ecosystems. For example, we are working with India's government and largest tech companies including  Infosys  (NS:INFY), Reliance and Tata to boost their sovereign AI infrastructure. And French private cloud provider, Scaleway, is building a regional AI cloud based on NVIDIA H100 InfiniBand and NVIDIA's AI Enterprise software to fuel advancement across France and Europe. National investment in compute capacity is a new economic imperative and serving the sovereign AI infrastructure market represents a multi-billion dollar opportunity over the next few years. From a product perspective, the vast majority of revenue in Q3 was driven by the NVIDIA HGX platform based on our Hopper GPU architecture with lower contribution from the prior generation Ampere GPU architecture. The new L40S GPU built for industry standard servers began to ship, supporting training and inference workloads across a variety of consumers. This was also the first revenue quarter of our GH200 Grace Hopper Superchip, which combines our ARM-based Grace CPU with a Hopper GPU. Grace and Grace Hopper are ramping into a new multi-billion dollar product line. Grace Hopper instances are now available at GPU specialized cloud providers, and coming soon to Oracle (NYSE:ORCL) Cloud. Grace Hopper is also getting significant traction with supercomputing customers. Initial shipments to Los Alamos National Lab and the Swiss National Supercomputing Center took place in the third quarter. The UK government announced it will build one of the world's fastest AI supercomputers called Isambard-AI with almost 5,500 Grace Hopper Superchips. German supercomputing center, Julich, also announced that it will build its next-generation AI supercomputer with close to 24,000 Grace Hopper Superchips and Quantum-2 InfiniBand, making it the world's most powerful AI supercomputer with over 90 exaflops of AI performance. All-in, we estimate that the combined AI compute capacity of all the supercomputers built on Grace Hopper across the U.S., Europe and Japan next year will exceed 200 exaflops with more wins to come. Inference is contributing significantly to our data center demand, as AI is now in full production for deep learning, recommenders, chatbots, copilots and text to image generation and this is just the beginning. NVIDIA AI offers the best inference performance and versatility, and thus the lower power and cost of ownership. We are also driving a fast cost reduction curve. With the release of TensorRT-LLM, we now achieved more than 2x the inference performance for half the cost of inferencing LLMs on NVIDIA GPUs. We also announced the latest member of the Hopper family, the H200, which will be the first GPU to offer HBM3e, faster, larger memory to further accelerate generative AI and LLMs. It moves inference speed up to another 2x compared to H100 GPUs for running LLMs like Norma2 (ph). Combined, TensorRT-LLM and H200, increased performance or reduced cost by 4x in just one year. With our customers changing their stack, this is a benefit of CUDA and our architecture compatibility. Compared to the A100, H200 delivers an 18x performance increase for inferencing models like GPT-3, allowing customers to move to larger models and with no increase in latency. Amazon (NASDAQ:AMZN) Web Services, Google (NASDAQ:GOOGL) Cloud, Microsoft Azure and Oracle Cloud will be among the first CSPs to offer H200-based instances starting next year. At last week's Microsoft Ignite, we deepened and expanded our collaboration with Microsoft across the entire stock. We introduced an AI foundry service for the development and tuning of custom generative AI enterprise applications running on Azure. Customers can bring their domain knowledge and proprietary data and we help them build their AI models using our AI expertise and software stock in our DGX cloud, all with enterprise grade security and support. SAP and  Amdocs  (NASDAQ:DOX) are the first customers of the NVIDIA AI foundry service on Microsoft Azure. In addition, Microsoft will launch new confidential computing instances based on the H100. The H100 remains the top performing and most versatile platform for AI training and by a wide margin, as shown in the latest MLPerf industry benchmark results. Our training cluster included more than 10,000 H100 GPUs or 3x more than in June, reflecting very efficient scaling. Efficient scaling is a key requirement in generative AI, because LLMs are growing by an order of magnitude every year. Microsoft Azure achieved similar results on a nearly identical cluster, demonstrating the efficiency of NVIDIA AI in public cloud deployments. Networking now exceeds a $10 billion annualized revenue run rate. Strong growth was driven by exceptional demand for InfiniBand, which grew fivefold year-on-year. InfiniBand is critical to gaining the scale and performance needed for training LLMs. Microsoft made this very point last week, highlighting that Azure uses over 29,000 miles of InfiniBand cabling, enough to circle the globe. We are expanding NVIDIA networking into the Ethernet space. Our new Spectrum-X end-to-end Ethernet offering with technologies, purpose built for AI, will be available in Q1 next year. With support from leading OEMs, including Dell (NYSE:DELL), HPE and Lenovo. Spectrum-X can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. Let me also provide an update on our software and services offerings, where we are starting to see excellent adoption. We are on track to exit the year at an annualized revenue run rate of $1 billion for our recurring software, support and services offerings. We see two primary opportunities for growth over the intermediate term with our DGX cloud service and with our NVIDIA AI Enterprise software, each reflects the growth of enterprise AI training and enterprise AI inference, respectively. Our latest DGX cloud customer announcement was this morning as part of an AI research collaboration with Gentech, the biotechnology pioneer also plans to use our BioNeMo LLM framework to help accelerate and optimize their AI drug discovery platform. We now have enterprise AI partnership with Adobe, Dropbox (NASDAQ:DBX), Getty, SAP, ServiceNow, Snowflake and others to come. Okay. Moving to Gaming. Gaming revenue of $2.86 billion was up 15% sequentially and up more than 80% year-on-year with strong demand in the important back-to-school shopping season with NVIDIA RTX ray tracing and AI technology now available at price points as low as $299. We entered the holidays with the best-ever line-up for gamers and creators. Gaming has doubled relative to pre-COVID levels even against the backdrop of lackluster PC market performance. This reflects the significant value we've brought to the gaming ecosystem with innovations like RTX and DLSS. The number of games and applications supporting these technologies has exploded in that period, driving upgrades and attracting new buyers. The RTX ecosystem continues to grow. There are now over 475 RTX-enabled games and applications. Generative AI is quickly emerging as the new pillar app for high performance PCs. NVIDIA RTX GPUs to find the most performance AI PCs and workstations. We just released TensorRT-LLM for Windows, which speeds on-device LLM inference up by 4x. With an installed base of over 100 million, NVIDIA RTX is the natural platform for AI application developers. Finally, our GeForce NOW cloud gaming service continues to build momentum. Its library of PC games surpassed 1,700 titles, including the launches of Alan Wake 2, Baldur's Gate 3, Cyberpunk 2077: Phantom Liberty and Starfield. Moving to the Pro Vis. Revenue of $416 million was up 10% sequentially and up 108% year-on year. NVIDIA RTX is the workstation platform of choice for professional design, engineering and simulation use cases and AI is emerging as a powerful demand driver. Early applications include inference for AI imaging in healthcare and edge AI in smart spaces and the public sector. We launched a new line of desktop workstations based on NVIDIA RTX Ada Lovelace generation GPUs and ConnectX, SmartNICs offering up to 2x the AI processing ray tracing and graphics performance of the previous generations. These powerful new workstations are optimized for AI workloads such as fine tune AI models, training smaller models and running inference locally. We continue to make progress on Omniverse, our software platform for designing, building and operating 3D virtual worlds. Mercedes-Benz (OTC:MBGAF) is using Omniverse powered digital twins to plan, design, build and operate its manufacturing and assembly facilities, helping it increase efficiency and reduce defects. Oxxon (ph) is also incorporating Omniverse into its manufacturing process, including end-to-end simulation for the entire robotics and automation pipeline, saving time and cost. We announced two new Omniverse Cloud services for automotive digitalization available on Microsoft Azure, a virtual factory simulation engine and autonomous vehicle simulation engine. Moving to Automotive. Revenue was $261 million, up 3% sequentially and up 4% year-on year, primarily driven by continued growth in self-driving platforms based on NVIDIA DRIVE Orin SOC and the ramp of AI cockpit solutions with global OEM customers. We extended our automotive partnership of Foxconn to include NVIDIA DRIVE for our next-generation automotive SOC. Foxconn has become the ODM for EVs. Our partnership provides Foxconn with a standard AV sensor and computing platform for their customers to easily build a state-of-an-art safe and secure software defined car. Now we're going to move to the rest of the P&L. GAAP gross margin expanded to 74% and non-GAAP gross margin to 75%, driven by higher Data Center sales and lower net inventory reserve, including a 1 percentage point benefit from the release of previously reserved inventory related to the Ampere GPU architecture products. Sequentially, GAAP operating expenses were up 12% and non-GAAP operating expenses were up 10%, primarily reflecting increased compensation and benefits. Let me turn to the fourth quarter of fiscal 2024. Total revenue is expected to be $20 billion, plus or minus 2%. We expect strong sequential growth to be driven by Data Center, with continued strong demand for both compute and networking. Gaming will likely decline sequentially as it is now more aligned with notebook seasonality. GAAP and non-GAAP gross margins are expected to be 74.5% and 75.5%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $3.17 billion and $2.2 billion, respectively. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $200 million, excluding gains and losses from non-affiliated investments. GAAP and non-GAAP tax rates are expected to be 15%, plus or minus 1% excluding any discrete items. Further financial information are included in the CFO commentary and other information available on our IR website. In closing, let me highlight some upcoming events for the financial community. We will attend the UBS Global Technology Conference in Scottsdale, Arizona, on November 28; the Wells Fargo TMT Summit in Rancho Palos Verdes, California on November 29; the Arete Virtual Tech Conference on December 7; and the J.P. Morgan Health Care Conference in San Francisco on January 8. Our earnings call to discuss the results of our fourth quarter and fiscal 2024 is scheduled for Wednesday, February 21. We will now open the call for questions. Operator, will you please poll for questions.\\nOperator: [Operator Instructions] Your first question comes from the line of Vivek Arya of Bank of America. Your line is open.\\nVivek Arya: Thanks for taking my question. Just, Colette, wanted to clarify what China contributions are you expecting in Q4. And then, Jensen, the main question is for you, where do you think we are in the adoption curve in terms of your shipments into the generative AI market? Because when I just look at the trajectory of your data center, is growth -- it will be close to nearly 30% of all the spending in data center next year. So what metrics are you keeping an eye on to inform you that you can continue to grow? Just where are we in the adoption curve of your products into the generative AI market? Thank you.\\nColette Kress: So, first let me start with your question, Vivek, on export controls and the impacts that we are seeing in our Q4 outlook and guidance that we provided. We had seen historically over the last several quarters that China and some of the other impacted destinations to be about 20% to 25% of our Data Center revenue. We are expecting in our guidance for that to decrease substantially as we move into Q4. The export controls will have a negative effect on our China business. And we do not have good visibility into the magnitude of that impact even over the long-term. We are though working to expand our Data Center product portfolio to possibly offer new regulation compliance solutions that do not require a license, these products, they may become available in the next coming months. However, we don't expect their contribution to be material or meaningful as a percentage of the revenue in Q4.\\nJensen Huang: Generative AI is the largest TAM expansion of software and hardware that we've seen in several decades. At the core of it, what's really exciting is that, what was largely a retrieval based computing approach, almost everything that you do is retrieved off of storage somewhere, has been augmented now, added with a generative method. And it's changed almost everything. You could see that text-to-text, text-to-image, text-to-video, text-to-3D, text-to-protein, text-to-chemicals, these were things that were processed and typed in by humans in the past. And these are now generative approaches. The way that we access data is changed. It used to be based on explicit queries. It is now based on natural language queries, intention queries, semantic queries. And so, we're excited about the work that we're doing with SAP and Dropbox and many others that you're going to hear about. And one of the areas that is really impactful is the software industry, which is about $1 trillion or so, has been building tools that are manually used over the last couple of decades. And now there's a whole new segment of software called copilots and assistants. Instead of manually used, these tools will have copilots to help you use it. And so, instead of licensing software, we will continue to do that, of course, but we will also hire copilots and assistants to help us use these -- use the software. We'll connect all of these copilots and assistants into teams of AIs, which is going to be the modern version of software, modern version of enterprise business software. And so the transformation of software and the way that software has done is driving the hardware underneath. And you can see that it's transforming hardware in two ways. One is something that's largely independent of generative AI. There's two trends: one is related to accelerated computing, general purpose computing is too wasteful of energy and cost. And now that we have much, much better approaches, call it, accelerated computing, you could save an order of magnitude of energy, you can save an order of magnitude of time or you can save an order of magnitudes of cost by using acceleration. And so, accelerated computing is transitioning, if you will, general purpose computing into this new approach. And that's been augmented by a new class of data centers. This is the traditional data centers that you were just talking about where we represent about a third of that. But there is a new class of data centers and this new class of data centers, unlike the data centers of the past, where you have a lot of applications running used by a great many people that are different tenants that are using the same infrastructure and that data center stores a lot of files. These new data centers are very few applications, if not one application, used by basically one tenant and it processes data, it trains models and then generates tokens and generates AI. And we call these new data centers AI factories. We're seeing AI factories being built out everywhere, and just by every country. And so if you look at the way where we are in the expansion, the transition into this new computing approach, the first wave you saw with large language model start-ups, generative AI start-ups and consumer Internet companies, and weren't in the process of ramping that. Meanwhile, while that's being ramped, you see that we're starting to partner with enterprise software companies who would like to build chatbots and copilots and assistants to augment the tools that they have on their platforms. You're seeing GPU specialized CSPs cropping up all over the world and they are dedicated to do really one thing, which is processing AI. You're seeing sovereign AI infrastructures, people -- countries that now recognize that they have to utilize their own data, keep their own data, keep their own culture, process that data and develop their own AI. You see that in India. Several -- about a year ago in Sweden, you are seeing in Japan. Last week, a big announcement in France. But the number of sovereign AI clouds that are being built is really quite significant. And my guess is that almost every major region will have and surely every major country will have their own AI clouds. And so I think you're seeing just new developments as the generative AI wave propagates through every industry, every company, every region. And so we're at the beginning of this inflection, this computing transition.\\nOperator: Your next question comes from the line of Aaron Rakers of Wells Fargo. Your line is open.\\nAaron Rakers: Yeah. Thanks for taking the question. I wanted to ask about kind of the networking side of the business. Given the growth rates that you've now cited, I think, it's 155% year-over-year and strong growth sequentially, it looks like that business is like almost approaching $2.5 billion to $3 billion quarterly level. I'm curious of how you see Ethernet involved evolving and maybe how you would characterize your differentiation of Spectrum-X relative to the traditional Ethernet stack as we start to think about that becoming part of the networking narrative above and maybe beyond just InfiniBand as we look into next year? Thank you.\\nJensen Huang: Yeah. Thanks for the question. Our networking business is already on a $10 billion plus run rate and it's going to get much larger. And as you mentioned, we added a new networking platform to our networking business recently. The vast majority of the dedicated large scale AI factories standardize on InfiniBand. And the reason for that is not only because of its data rate and not only just the latency, but the way that it moves traffic around the network is really important. The way that you process AI and a multi-tenant hyperscale Ethernet environment, the traffic pattern is just radically different. And with InfiniBand and with software defined networks, we could do congestion control, adaptive routing, performance isolation and noise isolation, not to mention, of course, the day rate and the low latency that -- and a very low overhead of InfiniBand that's natural part of InfiniBand. And so, InfiniBand is not so much just the network, it's also a computing fabric. We've put a lot of software-defined capabilities into the fabric including computation. We will do 40-point calculations and computation right on the switch, and right in the fabric itself. And so that's the reason why that difference in Ethernet versus InfiniBand or InfiniBand versus Ethernet for AI factories is so dramatic. And the difference is profound. And the reason for that is because you've just invested in a $2 billion infrastructure for AI factories. A 20%, 25%, 30% difference in overall effectiveness, especially as you scale up is measured in hundreds of millions of dollars of value. And if you will, renting that infrastructure over the course of four to five years, it really, really adds up. And so InfiniBand's value proposition is undeniable for AI factories. However, as we move AI into enterprise. This is enterprise computing what we'd like to enable every company to be able to build their own custom AIs. We're building customer AIs in our company based on our proprietary data, our proprietary type of skills. For example, recently we spoke about one of the models that we're creating, it's called ChipNeMo; we're building many others. There'll be tens, hundreds of custom AI models that we create inside our company. And our company is -- for all of our employee use, doesn't have to be as high performance as the AI factories we used to train the models. And so we would like the AI to be able to run in Ethernet environment. And so what we've done is we invented this new platform that extends Ethernet; doesn't replace Ethernet, it's 100% compliant with Ethernet. And it's optimized for East-West traffic, which is where the computing fabric is. It adds to Ethernet with an end-to-end solution with Bluefield, as well as our Spectrum switch that allows us to perform some of the capabilities that we have in InfiniBand, not all but some. And we achieved excellent results. And the way we go to market is we go to market with our large enterprise partners who already offer our computing solution. And so, HP (NYSE:HPQ), Dell and Lenovo has the NVIDIA AI stack, the NVIDIA AI Enterprise software stack and now they integrate with Bluefield, as well as bundle -- take a market there, Spectrum switch, and they'll be able to offer enterprise customers all over the world with their vast sales force and vast network of resellers a fully integrated, if you will, fully optimized, at least end-to-end AI solution. And so that's basically it, bringing AI to Ethernet for the world's enterprise.\\nOperator: Thank you. Your next question comes from the line of Joe Moore of Morgan Stanley. Your line is open.\\nJoseph Moore: Great. Thank you. I'm wondering if you could talk a little bit more about Grace Hopper and how you see the ability to leverage kind of the microprocessor, how you see that as a TAM expander. And what applications do you see using Grace Hopper versus more traditional H100 applications?\\nJensen Huang: Yeah. Thanks for the question. Grace Hopper is in production -- in high volume production now. We're expecting next year just with all of the design wins that we have in high performance computing and AI infrastructures, we are on a very, very fast ramp with our first data center CPU to a multi-billion dollar product line. This is going to be a very large product line for us. The capability of Grace Hopper is really quite spectacular. It has the ability to create computing nodes that simultaneously has very fast memory, as well as very large memory. In the areas of vector databases or semantic surge, what is called RAG, retrieval augmented generation. So that you could have a generative AI model be able to refer to proprietary data or a factual data before it generates a response, that data is quite large. And you can also have applications or generative models where the context length is very high. You basically store it in entire book into end-to-end system memory before you ask your questions. And so the context length can be quite large this way. The generative models has the ability to still be able to naturally interact with you on one hand. On the other hand, be able to refer to factual data, proprietary data or domain-specific data, you data and be contextually relevant and reduce hallucination. And so that particular use case for example is really quite fantastic for Grace Hopper. It also serves the customers that really care to have a different CPU than x86. Maybe it's a European supercomputing centers or European companies who would like to build up their own ARM ecosystem and like to build up a full stack or CSPs that have decided that they would like to pivot to ARM, because their own custom CPUs are based on ARM. There are variety of different reasons that drives the success of Grace Hopper, but we're off to a just an extraordinary start. This is a home run product.\\nOperator: Your next question comes from the line of Tim Arcuri of UBS. Your line is open.\\nTim Arcuri: Hi. Thanks. I wanted to ask a little bit about the visibility that you have on revenue. I know there's a few moving parts. I guess, on one hand, the purchase commitments went up a lot again. But on the other hand, China bans would arguably pull in when you can fill the demand beyond China. So I know we're not even into 2024 yet and it doesn't sound like, Jensen, you think that next year would be a peak in your Data Center revenue, but I just wanted to sort of explicitly ask you that. Do you think that Data Center can grow even in 2025? Thanks.\\nJensen Huang: Absolutely believe the Data Center can grow through 2025. And there are, of course, several reasons for that. We are expanding our supply quite significantly. We have already one of the broadest and largest and most capable supply chain in the world. Now, remember, people think that the GPU is a chip. But the HGX H100, the Hopper HGX has 35,000 parts, it weighs 70 pounds. Eight of the chips are Hopper. The other 35,000 are not. It is -- even its passive components are incredible. High voltage parts. High frequency parts. High current parts. It is a supercomputer, and therefore, the only way to test a supercomputer is with another supercomputer. Even the manufacturing of it is complicated, the testing of it is complicated, the shipping of it complicated and installation is complicated. And so, every aspect of our HGX supply chain is complicated. And the remarkable team that we have here has really scaled out the supply chain incredibly. Not to mention, all of our HGXs are connected with NVIDIA networking. And the networking, the transceivers, the mix, the cables, the switches, the amount of complexity there is just incredible. And so, I'm just -- first of all, I'm just super proud of the team for scaling up this incredible supply chain. We are absolutely world class. But meanwhile, we're adding new customers and new products. So we have new supply. We have new customers, as I was mentioning earlier. Different regions are standing up GPU specialist clouds, sovereign AI clouds coming out from all over the world, as people realize that they can't afford to export their country's knowledge, their country's culture for somebody else to then resell AI back to them, they have to -- they should, they have the skills and surely with us in combination, we can help them to do that build up their national AI. And so, the first thing that they have to do is, create their AI cloud, national AI cloud. You're also seeing us now growing into enterprise. The enterprise market has two paths. One path -- or if I could say three paths. The first path, of course, just off-the-shelf AI. And there are of course Chat GPT, a fabulous off-the-shelf AI, there'll be others. There's also a proprietary AI, because software companies like ServiceNow and SAP, there are many, many others that can't afford to have their company's intelligence be outsourced to somebody else. And they are about building tools and on top of their tools they should build custom and proprietary and domain-specific copilots and assistants that they can then rent to their customer base. This is -- they're sitting on a goldmine, almost every major tools company in the world is sitting on a goldmine, and they recognize that they have to go build their own custom AIs. We have a new service called an AI foundry, where we leverage NVS (ph) capabilities to be able to serve them in that. And then the next one is enterprises building their own custom AIs, their own custom chatbots, their own custom RAGs. And this capability is spreading all over the world. And the way that we're going to serve that marketplace is with the entire stacks of systems, which includes our compute, our networking and our switches, running our software stack called NVIDIA AI Enterprise, taking it through our market partners, HP, Dell, Lenovo, so on and so forth. And so we're just -- we're seeing the waves of generative AI starting from the start-ups and CSPs, moving to consumer Internet companies, moving to enterprise software platforms, moving to enterprise companies. And then ultimately, one of the areas that you guys have seen us spend a lot of energy on has to do with industrial generative AI. This is where NVIDIA AI and NVIDIA Omniverse comes together and that is a really, really exciting work. And so I think the -- we're at the beginning of a basically across-the-board industrial transition to generative AI to accelerated computing. This is going to affect every company, every industry, every country.\\nOperator: Your next question comes from the line of Toshiya Hari of Goldman Sachs. Your line is open.\\nToshiya Hari: Hi. Thank you. I wanted to clarify something with Colette real quick, and then I had a question for Jensen as well. Colette, you mentioned that you'll be introducing regulation-compliant products over the next couple of months. Yet, the contribution to Q4 revenue should be relatively limited. Is that a timing issue and could it be a source of reacceleration in growth for Data Center in April and beyond or are the price points such that the contribution to revenue going forward should be relatively limited? And then the question for Jensen, the AI foundry service announcement from last week. I just wanted to ask about that, and hopefully, have you expand on it. How is the monetization model going to work? Is it primarily services and software revenue? How should we think about the long term opportunity set? And is this going to be exclusive to Microsoft or do you have plans to expand to other partners as well? Thank you.\\nColette Kress: Thanks, Toshiya. On the question regarding potentially new products that we could provide to our China customers. It's a significant process to both design and develop these new products. As we discussed, we're going to make sure that we are in full discussions with the U.S. government of our intent to move products as well. Given our state about where we are in the quarter, we're already several weeks into the quarter. So it's just going to take some time for us to go through and discussing with our customers the needs and desires of these new products that we have. And moving forward, whether that's medium-term or long-term, it's just hard to say both the [Technical Difficulty] of what we can produce with the U.S. government and what the interest of our China customers in this. So we stay still focused on finding that right balance for our China customers, but it's hard to say at this time.\\nJensen Huang: Toshiya, thanks for the question. There is a glaring opportunity in the world for AI foundry, and it makes so much sense. First, every company has its core intelligence. It makes up our company. Our data, our domain expertise, in the case of many companies, we create tools, and most of the software companies in the world are tool platforms, and those tools are used by people today. And in the future, it's going to be used by people augmented with a whole bunch of AIs that we hire. And these platforms just got to go across the world and you'll see and we've only announced a few; SAP, ServiceNow, Dropbox, Getty, many others are coming. And the reason for that is because they have their own proprietary AI. They want their own proprietary AI. They can't afford to outsource their intelligence and handout their data, and handout their flywheel for other companies to build the AI for them. And so, they come to us. We have several things that are really essential in a foundry. Just as TSMC as a foundry, you have to have AI technology. And as you know, we have just an incredible depth of AI capability -- AI technology capability. And then second, you have to have the best practice known practice, the skills of processing data through the invention of AI models to create AIs that are guardrails, fine-tuned, so on and so forth, that are safe, so on and so forth. And the third thing is you need factories. And that's what DGX Cloud is. Our AI models are called AI Foundations. Our process, if you will, our CAD system for creating AIs are called NeMo and they run on NVIDIA's factories we call DGX Cloud. Our monetization model is that with each one of our partners they rent a sandbox on DGX Cloud, where we work together, they bring their data, they bring their domain expertise, we bring our researchers and engineers, we help them build their custom AI. We help them make that custom AI incredible. Then that custom AI becomes theirs. And they deploy it on the runtime that is enterprise grade, enterprise optimized or outperformance optimized, runs across everything NVIDIA. We have a giant installed base in the cloud, on-prem, anywhere. And it's secure, securely patched, constantly patched and optimized and supported. And we call that NVIDIA AI Enterprise. NVIDIA AI Enterprise is $4,500 per GP per year, that's our business model. Our business model is basically a license. Our customers then with that basic license can build their monetization model on top of. In a lot of ways we're wholesale, they become retail. They could have a per -- they could have subscription license base, they could per instance or they could do per usage, there is a lot of different ways that they could take a -- create their own business model, but ours is basically like a software license, like an operating system. And so our business model is help you create your custom models, you run those custom models on NVIDIA AI Enterprise. And it's off to a great start. NVIDIA AI Enterprise is going to be a very large business for us.\\nOperator: Your next question comes from the line of Stacy Rasgon of Bernstein Research. Your line is open.\\nStacy Rasgon: Hi, guys. Thanks for taking my questions. Colette, I wanted to know if it weren't for the China restrictions would the Q4 guide has been higher or are you supply-constrained in just reshipping stuff that would have gone to China elsewhere? And I guess along those lines you give us a feeling for where your lead times are right now in data center and just the China redirection such as-is, is it lowering those lead times, because you've got parts that are sort of immediately available to ship?\\nColette Kress: Yeah. Stacy, let me see if I can help you understand. Yes, there are still situations where we are working on both improving our supply each and every quarter. We've done a really solid job of ramping every quarter, which has defined our revenue. But with the absence of China for our outlook for Q4, sure, there could have been some things that we are not supply-constrained that we could have sold, but kind of we would no longer can. So could our guidance had been a little higher in our Q4? Yes. We are still working on improving our supply on plan, on continuing growing all throughout next year as well towards that.\\nOperator: Your next question comes from the line of Matt Ramsay of TD Cowen. Your line is open.\\nMatt Ramsay: Thank you very much. Congrats, everybody, on the results. Jensen, I had a two-part question for you, and it comes off of sort of one premise. And the premise is, I still get a lot of questions from investors thinking about AI training as being NVIDIA's dominant domain and somehow inference, even large model inference takes more and more of the TAM that the market will become more competitive. You'll be less differentiated et cetera., et cetera. So I guess the two parts of the question are: number one, maybe you could spend a little bit of time talking about the evolution of the inference workload as we move to LLMs and how your company is positioned for that rather than smaller model inference. And second, up until a month or two ago, I never really got any questions at all about the data processing piece of the AI workloads. So the pieces of manipulating the data before training, between training and inference, after inference and I think that's a large part of the workload now. Maybe you could talk about how CUDA is enabling acceleration of those pieces of the workload. Thanks.\\nJensen Huang: Sure. Inference is complicated. It's actually incredibly complicated. If you -- we this quarter announced one of the most exciting new engines, optimizing compilers called TensorRT-LLM. The reception has been incredible. You got to GitHub, it's been downloaded a ton, a whole lot of stars, integrated into stacks and frameworks all over the world, almost instantaneously. And there are several reasons for that, obviously. We could create TensorRT-LLM, because CUDA is programmable. If CUDA and our GPUs were not so programmable, it would really be hard for us to improve software stacks at the pace that we do. TensorRT-LLM, on the same GPU, without anybody touching anything, improves the performance by a factor of two. And then on top of that, of course, the pace of our innovation is so high. H200 increases it by another factor of two. And so, our inference performance, another way of saying inference cost, just reduced by a factor of four within about a year's time. And so, that's really, really hard to keep up with. The reason why everybody likes our inference engine is because our installed base. We've been dedicated to our installed base for 20 years, 20-plus years. We have an installed base that is not only largest in every single cloud, it's in every available from every enterprise system maker, it's used by companies of just about every industry. And every -- anytime you see a NVIDIA GPU, it runs our stack. It's architecturally compatible, something we've been dedicated to for a very long time. We're very disciplined about it. We make it our, if you will, architecture compatibility is job one. And that has conveyed to the world, the certainty of our platform stability. NVIDIA's platform stability certainty is the reason why everybody builds on us first and the reason why everybody optimizes on us first. All the engineering and all the work that you do, all the invention of technologies that you build on top of NVIDIA accrues to the -- and benefits everybody that uses our GPUs. And we have such a large installed base, large -- millions and millions of GPUs in cloud, 100 million GPUs from people’s PCs just about every workstation in the world, and they all architecturally compatible. And so, if you are an inference platform and you're deploying an inference application, you are basically an application provider. And as a software application provider, you're looking for large installed base. Data processing, before you could train a model, you have to curate the data, you have to dedupe the data, maybe you have to augment the data with synthetic data. So, process the data, clean the data, align the data, normalize the data, all of that data is measured not in bytes or megabytes, it's measured in terabytes and petabytes. And the amount of data processing that you do before data engineering, before that you do training is quite significant. It could represent 30%, 40%, 50% of the amount of work that you ultimately do. And what you -- and ultimately creating a data driven machine learning service. And so data processing is just a massive part. We accelerate Spark, we accelerate Python. One of the coolest things that we just did is called cuDF Pandas. Without one line of code, Pandas, which is the single most successful data science framework in the world. Pandas now is accelerated by NVIDIA CUDA. And just out-of-the box, without the line of code and so the acceleration is really quite terrific and people are just incredibly excited about it. And Pandas was designed for one purpose and one purpose only, really data processing, it's for data science. And so NVIDIA CUDA gives you all of that.\\nOperator: Your final question comes from the line of Harlan Sur of J.P. Morgan. Your line is open.\\nHarlan Sur: Good afternoon. Thanks for taking my question. If you look at the history of the tech industry like those companies that have been successful have always been focused on ecosystem; silicon, hardware, software, strong partnerships and just as importantly, right, an aggressive cadence of new products, more segmentation over time. The team recently announced a more aggressive new product cadence in data center from two years to now every year with higher levels of segmentation, training, optimization in printing CPU, GPU, DPU networking. How do we think about your R&D OpEx growth outlook to support a more aggressive and expanding forward roadmap, but more importantly, what is the team doing to manage and drive execution through all of this complexity?\\nJensen Huang: Gosh. Boy, that's just really excellent. You just wrote NVIDIA's business plan, and so you described our strategy. First of all, there is a fundamental reason why we accelerate our execution. And the reason for that is because it fundamentally drives down cost. When the combination of TensorRT-LLM and H200 reduce the cost for our customers for large model inference by a factor of four, and so that includes, of course, our speeds and feeds, but mostly it's because of our software, mostly the software benefits because of the architecture. And so we want to accelerate our roadmap for that reason. The second reason is to expand the reach of generative AI, the world's number of data center configurations -- this is kind of the amazing thing. NVIDIA is in every cloud, but not one cloud is the same. NVIDIA is working with every single cloud service provider and not one of the networking control plane, security posture is the same. Everybody's platform is different and yet we're integrated into all of their stacks, all of their data centers and we work incredibly well with all of them. And not to mention, we then take the whole thing and we create AI factories that are standalone. We take our platform, we can put them into supercomputers, we can put them into enterprise. Bringing AI to enterprise is something generative AI Enterprise something nobody's ever done before. And we're right now in the process of going to market with all of that. And so the complexity includes, of course, all the technologies and segments and the pace. It includes the fact that we are architecturally compatible across every single one of those. It includes all of the domain specific libraries that we create. The reason why every computer company, without thinking, can integrate NVIDIA into their roadmap and take it to market. And the reason for that is, because there is market demand for it. There is market demand in healthcare, there is market demand in manufacturing, there is market demand, of course, in AI, including financial services, in supercomputing and quantum computing. The list of markets and segments that we have domain specific libraries is incredibly broaden. And then finally, we have an end-to-end solution for data centers; InfiniBand networking, Ethernet networking, x86, ARM, just about every permutation combination of solutions -- technology solutions and software stacks provided. And that translates to having the largest number of ecosystem software developers; the largest ecosystem of system makers; the largest and broadest distribution partnership network; and ultimately, the greatest reach. And that takes -- surely that takes a lot of energy. But the thing that really holds it together, and this is a great decision that we made decades ago, which is everything is architecturally compatible. When we develop a domain specific language that runs on one GPU, it runs on every GPU. When we optimize TensorRT for the cloud, we optimized it for enterprise. When we do something that brings in a new feature, a new library, a new feature or a new developer, they instantly get the benefit of all of our reach. And so that discipline, that architecture compatible discipline that has lasted more than a couple of decades now, is one of the reasons why NVIDIA is still really, really efficient. I mean, we're 28,000 people large and serving just about every single company, every single industry, every single market around the world.\\nOperator: Thank you. I will now turn the call back over to Jensen Huang for closing remarks.\\nJensen Huang: Our strong growth reflects the broad industry platform transition from general purpose to accelerated computing and generative AI. Large language models start-ups consumer Internet companies and global cloud service providers are the first movers. The next waves are starting to build. Nations and regional CSPs are building AI clouds to serve local demand. Enterprise software companies like Adobe and Dropbox, SAP and ServiceNow are adding AI copilots and assistants to their platforms. Enterprises in the world's largest industries are creating custom AIs to automate and boost productivity. The generative AI era is in full steam and has created the need for a new type of data center, an AI factory; optimized for refining data and training, and inference, and generating AI. AI factory workloads are different and incremental to legacy data center workloads supporting IT tasks. AI factories run copilots and AI assistants, which are significant software TAM expansion and are driving significant new investment. Expanding the $1 trillion traditional data center infrastructure installed base, empowering the AI Industrial Revolution. NVIDIA H100 HGX with InfiniBand and the NVIDIA AI software stack define an AI factory today. As we expand our supply chain to meet the world's demand, we are also building new growth drivers for the next wave of AI. We highlighted three elements to our new growth strategy that are hitting their stride: CPU, networking, and software and services. Grace is NVIDIA's first data center CPU. Grace and Grace Hopper are in full production and ramping into a new multi-billion dollar product line next year. Irrespective of the CPU choice, we can help customers build an AI factory. NVIDIA networking now exceeds $10 billion annualized revenue run rate. InfiniBand grew five-fold year-over-year, and is positioned for excellent growth ahead as the networking of AI factories. Enterprises are also racing to adopt AI and Ethernet is the standard networking. This week we announced an Ethernet for AI platform for enterprises. NVIDIA Spectrum-X is an end-to-end solution of Bluefield SuperNIC, Spectrum-4 Ethernet switch and software that boosts Ethernet performance by up to 1.6x for AI workloads. Dell, HPE and Lenovo have joined us to bring a full generative AI solution of NVIDIA AI computing, networking and software to the world's enterprises. NVIDIA software and services is on track to exit the year at an annualized run rate of $1 billion. Enterprise software platforms like ServiceNow and SAP need to build and operate proprietary AI. Enterprises need to build and deploy custom AI copilots. We have the AI technology, expertise and scale to help customers build custom models with their proprietary data on NVIDIA DGX Cloud and deploy the AI applications on enterprise grade NVIDIA AI Enterprise. NVIDIA is essentially an AI foundry. NVIDIA's GPUs, CPUs, networking, AI foundry services and NVIDIA AI Enterprise software are all growth engines in full throttle. Thanks for joining us today. We look forward to updating you on our progress next quarter.\\nOperator: This concludes today's conference call. You may now disconnect.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " \"NVIDIA's latest earnings call revealed record-breaking revenue of $18.1 billion, marking a 34% sequential increase and a 200% year-on-year increase. The company's Data Center segment was a primary driver of this growth, achieving record revenue of $14.5 billion. However, the company anticipates a significant decline in the fourth quarter due to new U.S. export control regulations affecting sales to China and other markets. \\nKey takeaways from the call include:NVIDIA's Data Center segment saw record revenue, driven by the NVIDIA HGX platform and InfiniBand networking. Major companies like Meta (NASDAQ:META) and Adobe (NASDAQ:ADBE) are integrating NVIDIA's technology into their platforms, contributing to the strong demand for AI supercomputers and data center infrastructure.NVIDIA is expanding its Data Center product portfolio to comply with new U.S. export control regulations. Despite expecting a decrease in sales to China and other markets in the fourth quarter, the company is focusing on national investment in sovereign AI infrastructure.The Gaming segment recorded revenue of $2.86 billion, a 15% sequential increase and an 80% year-on-year increase, driven by strong demand for NVIDIA's RTX ray tracing and AI technology.In the Pro Vis segment, revenue reached $416 million, a 10% sequential increase and a 108% year-on-year increase. NVIDIA RTX is the preferred platform for professional design, engineering, and simulation, with AI emerging as a demand driver.NVIDIA's CEO, Jensen Huang, discussed the expansion of AI factories and the transition into a new computing approach. He also expressed confidence in the continued growth of the Data Center business through 2025.During the call, NVIDIA highlighted its product innovations, including the H200 GPU, which offers faster and larger memory for generative AI and language models, and the GH200 Grace Hopper Superchip, a combination of an ARM-based CPU with a Hopper GPU. The Grace Hopper Superchip is being adopted by supercomputing customers, with the combined AI compute capacity of supercomputers built on Grace Hopper expected to exceed 200 exaflops next year. \\nIn addition to its product offerings, NVIDIA is also expanding its networking into the Ethernet space with its new Spectrum-X end-to-end Ethernet offering, which will be available in Q1 next year. This offering can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. \\nNVIDIA also discussed its business strategy, emphasizing the importance of reselling AI technology to customers and its growth in the enterprise market. The company introduced an AI foundry service to help enterprises build custom AI models and announced an aggressive product release schedule in the data center market. \\nLooking ahead, NVIDIA expects total revenue of $20 billion for Q4 2024, driven by strong demand in the Data Center sector. Despite regulatory challenges, the company is confident in its growth potential, citing expanded supply, new customers, and its entrance into the enterprise market.Full transcript -  Nvidia Corp  (NASDAQ:NVDA) Q3 2024:Operator: Good afternoon. My name is JL, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's Third Quarter Earnings Call. All lines have been placed on mute to prevent any background noise. After the speakers’ remarks, there will be a question-and-answer session. [Operator Instructions] Simona Jankowski, you may now begin your conference.\\nSimona Jankowski: Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2024. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter and fiscal 2024. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All statements are made as of today, November 21, 2023, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\\nColette Kress: Thanks, Simona. Q3 was another record quarter. Revenue of $18.1 billion was up 34% sequentially and up more than 200% year-on-year and well above our outlook for $16 billion. Starting with Data Center. The continued ramp of the NVIDIA HGX platform based on our Hopper Tensor Core GPU architecture, along with InfiniBand end-to-end networking, drove record revenue of $14.5 billion, up 41% sequentially and up 279% year-on-year. NVIDIA HGX with InfiniBand together are essentially the reference architecture for AI supercomputers and data center infrastructures. Some of the most exciting generative AI applications are built and run on NVIDIA, including Adobe Firefly, ChatGPT, Microsoft (NASDAQ:MSFT) 365 Copilot, CoAssist, now assist with ServiceNow (NYSE:NOW) and Zoom (NASDAQ:ZM) AI Companion. Our Data Center compute revenue quadrupled from last year and networking revenue nearly tripled. Investments in infrastructure for training and inferencing large language models, deep learning, recommender systems and generative AI applications is fueling strong broad-based demand for NVIDIA accelerated computing. Inferencing is now a major workload for NVIDIA AI computing. Consumer Internet companies and enterprises drove exceptional sequential growth in Q3, comprising approximately half of our Data Center revenue and outpacing total growth. Companies like Meta are in full production with deep learning, recommender systems and also investing in generative AI to help advertisers optimize images and text. Most major consumer Internet companies are racing to ramp up generative AI deployment. The enterprise wave of AI adoption is now beginning. Enterprise software companies such as Adobe, Databricks, Snowflake (NYSE:SNOW) and ServiceNow are adding AI copilots and the systems to their platforms. And broader enterprises are developing custom AI for vertical industry applications such as Tesla (NASDAQ:TSLA) in autonomous driving. Cloud service providers drove roughly the other half of our Data Center revenue in the quarter. Demand was strong from all hyperscale CSPs, as well as from a broadening set of GPU-specialized CSPs globally that are rapidly growing to address the new market opportunities in AI. NVIDIA H100 Tensor Core GPU instances are now generally available in virtually every cloud with instances in high demand. We have significantly increased supply every quarter this year to meet strong demand and expect to continue to do so next year. We will also have a broader and faster product launch cadence to meet the growing and diverse set of AI opportunities. Towards the end of the quarter, the U.S. government announced a new set of export control regulations for China and other markets, including Vietnam and certain countries in the Middle East. These regulations require licenses for the export of a number of our products, including our Hopper and Ampere 100 and 800 series and several others. Our sales to China and other affected destinations derived from products that are now subject to licensing requirements have consistently contributed approximately 20% to 25% of Data Center revenue over the past few quarters. We expect that our sales to these destinations will decline significantly in the fourth quarter. So we believe will be more than offset by strong growth in other regions. The U.S. government designed the regulation to allow the U.S. industry to provide data center compute products to markets worldwide, including China. Continuing to compete worldwide as the regulations encourage, promotes U.S. technology leadership, spurs economic growth and supports U.S. jobs. For the highest performance levels, the government requires licenses. For lower performance levels, the government requires a streamlined prior notification process. And for products even lower performance levels, the government does not require any notice at all. Following the government's clear guidelines, we are working to expand our Data Center product portfolio to offer compliance solutions for each regulatory category, including products for which the U.S. government does not wish to have advance notice before each shipment. We are working with some customers in China and the Middle East to pursue licenses from the U.S. government. It is too early to know whether these will be granted for any significant amount of revenue. Many countries are awakening to the need to invest in sovereign AI infrastructure to support economic growth and industrial innovation. With investments in domestic compute capacity, nations can use their own data to train LLMs and support their local generative AI ecosystems. For example, we are working with India's government and largest tech companies including  Infosys  (NS:INFY), Reliance and Tata to boost their sovereign AI infrastructure. And French private cloud provider, Scaleway, is building a regional AI cloud based on NVIDIA H100 InfiniBand and NVIDIA's AI Enterprise software to fuel advancement across France and Europe. National investment in compute capacity is a new economic imperative and serving the sovereign AI infrastructure market represents a multi-billion dollar opportunity over the next few years. From a product perspective, the vast majority of revenue in Q3 was driven by the NVIDIA HGX platform based on our Hopper GPU architecture with lower contribution from the prior generation Ampere GPU architecture. The new L40S GPU built for industry standard servers began to ship, supporting training and inference workloads across a variety of consumers. This was also the first revenue quarter of our GH200 Grace Hopper Superchip, which combines our ARM-based Grace CPU with a Hopper GPU. Grace and Grace Hopper are ramping into a new multi-billion dollar product line. Grace Hopper instances are now available at GPU specialized cloud providers, and coming soon to Oracle (NYSE:ORCL) Cloud. Grace Hopper is also getting significant traction with supercomputing customers. Initial shipments to Los Alamos National Lab and the Swiss National Supercomputing Center took place in the third quarter. The UK government announced it will build one of the world's fastest AI supercomputers called Isambard-AI with almost 5,500 Grace Hopper Superchips. German supercomputing center, Julich, also announced that it will build its next-generation AI supercomputer with close to 24,000 Grace Hopper Superchips and Quantum-2 InfiniBand, making it the world's most powerful AI supercomputer with over 90 exaflops of AI performance. All-in, we estimate that the combined AI compute capacity of all the supercomputers built on Grace Hopper across the U.S., Europe and Japan next year will exceed 200 exaflops with more wins to come. Inference is contributing significantly to our data center demand, as AI is now in full production for deep learning, recommenders, chatbots, copilots and text to image generation and this is just the beginning. NVIDIA AI offers the best inference performance and versatility, and thus the lower power and cost of ownership. We are also driving a fast cost reduction curve. With the release of TensorRT-LLM, we now achieved more than 2x the inference performance for half the cost of inferencing LLMs on NVIDIA GPUs. We also announced the latest member of the Hopper family, the H200, which will be the first GPU to offer HBM3e, faster, larger memory to further accelerate generative AI and LLMs. It moves inference speed up to another 2x compared to H100 GPUs for running LLMs like Norma2 (ph). Combined, TensorRT-LLM and H200, increased performance or reduced cost by 4x in just one year. With our customers changing their stack, this is a benefit of CUDA and our architecture compatibility. Compared to the A100, H200 delivers an 18x performance increase for inferencing models like GPT-3, allowing customers to move to larger models and with no increase in latency. Amazon (NASDAQ:AMZN) Web Services, Google (NASDAQ:GOOGL) Cloud, Microsoft Azure and Oracle Cloud will be among the first CSPs to offer H200-based instances starting next year. At last week's Microsoft Ignite, we deepened and expanded our collaboration with Microsoft across the entire stock. We introduced an AI foundry service for the development and tuning of custom generative AI enterprise applications running on Azure. Customers can bring their domain knowledge and proprietary data and we help them build their AI models using our AI expertise and software stock in our DGX cloud, all with enterprise grade security and support. SAP and  Amdocs  (NASDAQ:DOX) are the first customers of the NVIDIA AI foundry service on Microsoft Azure. In addition, Microsoft will launch new confidential computing instances based on the H100. The H100 remains the top performing and most versatile platform for AI training and by a wide margin, as shown in the latest MLPerf industry benchmark results. Our training cluster included more than 10,000 H100 GPUs or 3x more than in June, reflecting very efficient scaling. Efficient scaling is a key requirement in generative AI, because LLMs are growing by an order of magnitude every year. Microsoft Azure achieved similar results on a nearly identical cluster, demonstrating the efficiency of NVIDIA AI in public cloud deployments. Networking now exceeds a $10 billion annualized revenue run rate. Strong growth was driven by exceptional demand for InfiniBand, which grew fivefold year-on-year. InfiniBand is critical to gaining the scale and performance needed for training LLMs. Microsoft made this very point last week, highlighting that Azure uses over 29,000 miles of InfiniBand cabling, enough to circle the globe. We are expanding NVIDIA networking into the Ethernet space. Our new Spectrum-X end-to-end Ethernet offering with technologies, purpose built for AI, will be available in Q1 next year. With support from leading OEMs, including Dell (NYSE:DELL), HPE and Lenovo. Spectrum-X can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. Let me also provide an update on our software and services offerings, where we are starting to see excellent adoption. We are on track to exit the year at an annualized revenue run rate of $1 billion for our recurring software, support and services offerings. We see two primary opportunities for growth over the intermediate term with our DGX cloud service and with our NVIDIA AI Enterprise software, each reflects the growth of enterprise AI training and enterprise AI inference, respectively. Our latest DGX cloud customer announcement was this morning as part of an AI research collaboration with Gentech, the biotechnology pioneer also plans to use our BioNeMo LLM framework to help accelerate and optimize their AI drug discovery platform. We now have enterprise AI partnership with Adobe, Dropbox (NASDAQ:DBX), Getty, SAP, ServiceNow, Snowflake and others to come. Okay. Moving to Gaming. Gaming revenue of $2.86 billion was up 15% sequentially and up more than 80% year-on-year with strong demand in the important back-to-school shopping season with NVIDIA RTX ray tracing and AI technology now available at price points as low as $299. We entered the holidays with the best-ever line-up for gamers and creators. Gaming has doubled relative to pre-COVID levels even against the backdrop of lackluster PC market performance. This reflects the significant value we've brought to the gaming ecosystem with innovations like RTX and DLSS. The number of games and applications supporting these technologies has exploded in that period, driving upgrades and attracting new buyers. The RTX ecosystem continues to grow. There are now over 475 RTX-enabled games and applications. Generative AI is quickly emerging as the new pillar app for high performance PCs. NVIDIA RTX GPUs to find the most performance AI PCs and workstations. We just released TensorRT-LLM for Windows, which speeds on-device LLM inference up by 4x. With an installed base of over 100 million, NVIDIA RTX is the natural platform for AI application developers. Finally, our GeForce NOW cloud gaming service continues to build momentum. Its library of PC games surpassed 1,700 titles, including the launches of Alan Wake 2, Baldur's Gate 3, Cyberpunk 2077: Phantom Liberty and Starfield. Moving to the Pro Vis. Revenue of $416 million was up 10% sequentially and up 108% year-on year. NVIDIA RTX is the workstation platform of choice for professional design, engineering and simulation use cases and AI is emerging as a powerful demand driver. Early applications include inference for AI imaging in healthcare and edge AI in smart spaces and the public sector. We launched a new line of desktop workstations based on NVIDIA RTX Ada Lovelace generation GPUs and ConnectX, SmartNICs offering up to 2x the AI processing ray tracing and graphics performance of the previous generations. These powerful new workstations are optimized for AI workloads such as fine tune AI models, training smaller models and running inference locally. We continue to make progress on Omniverse, our software platform for designing, building and operating 3D virtual worlds. Mercedes-Benz (OTC:MBGAF) is using Omniverse powered digital twins to plan, design, build and operate its manufacturing and assembly facilities, helping it increase efficiency and reduce defects. Oxxon (ph) is also incorporating Omniverse into its manufacturing process, including end-to-end simulation for the entire robotics and automation pipeline, saving time and cost. We announced two new Omniverse Cloud services for automotive digitalization available on Microsoft Azure, a virtual factory simulation engine and autonomous vehicle simulation engine. Moving to Automotive. Revenue was $261 million, up 3% sequentially and up 4% year-on year, primarily driven by continued growth in self-driving platforms based on NVIDIA DRIVE Orin SOC and the ramp of AI cockpit solutions with global OEM customers. We extended our automotive partnership of Foxconn to include NVIDIA DRIVE for our next-generation automotive SOC. Foxconn has become the ODM for EVs. Our partnership provides Foxconn with a standard AV sensor and computing platform for their customers to easily build a state-of-an-art safe and secure software defined car. Now we're going to move to the rest of the P&L. GAAP gross margin expanded to 74% and non-GAAP gross margin to 75%, driven by higher Data Center sales and lower net inventory reserve, including a 1 percentage point benefit from the release of previously reserved inventory related to the Ampere GPU architecture products. Sequentially, GAAP operating expenses were up 12% and non-GAAP operating expenses were up 10%, primarily reflecting increased compensation and benefits. Let me turn to the fourth quarter of fiscal 2024. Total revenue is expected to be $20 billion, plus or minus 2%. We expect strong sequential growth to be driven by Data Center, with continued strong demand for both compute and networking. Gaming will likely decline sequentially as it is now more aligned with notebook seasonality. GAAP and non-GAAP gross margins are expected to be 74.5% and 75.5%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $3.17 billion and $2.2 billion, respectively. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $200 million, excluding gains and losses from non-affiliated investments. GAAP and non-GAAP tax rates are expected to be 15%, plus or minus 1% excluding any discrete items. Further financial information are included in the CFO commentary and other information available on our IR website. In closing, let me highlight some upcoming events for the financial community. We will attend the UBS Global Technology Conference in Scottsdale, Arizona, on November 28; the Wells Fargo TMT Summit in Rancho Palos Verdes, California on November 29; the Arete Virtual Tech Conference on December 7; and the J.P. Morgan Health Care Conference in San Francisco on January 8. Our earnings call to discuss the results of our fourth quarter and fiscal 2024 is scheduled for Wednesday, February 21. We will now open the call for questions. Operator, will you please poll for questions.\\nOperator: [Operator Instructions] Your first question comes from the line of Vivek Arya of Bank of America. Your line is open.\\nVivek Arya: Thanks for taking my question. Just, Colette, wanted to clarify what China contributions are you expecting in Q4. And then, Jensen, the main question is for you, where do you think we are in the adoption curve in terms of your shipments into the generative AI market? Because when I just look at the trajectory of your data center, is growth -- it will be close to nearly 30% of all the spending in data center next year. So what metrics are you keeping an eye on to inform you that you can continue to grow? Just where are we in the adoption curve of your products into the generative AI market? Thank you.\\nColette Kress: So, first let me start with your question, Vivek, on export controls and the impacts that we are seeing in our Q4 outlook and guidance that we provided. We had seen historically over the last several quarters that China and some of the other impacted destinations to be about 20% to 25% of our Data Center revenue. We are expecting in our guidance for that to decrease substantially as we move into Q4. The export controls will have a negative effect on our China business. And we do not have good visibility into the magnitude of that impact even over the long-term. We are though working to expand our Data Center product portfolio to possibly offer new regulation compliance solutions that do not require a license, these products, they may become available in the next coming months. However, we don't expect their contribution to be material or meaningful as a percentage of the revenue in Q4.\\nJensen Huang: Generative AI is the largest TAM expansion of software and hardware that we've seen in several decades. At the core of it, what's really exciting is that, what was largely a retrieval based computing approach, almost everything that you do is retrieved off of storage somewhere, has been augmented now, added with a generative method. And it's changed almost everything. You could see that text-to-text, text-to-image, text-to-video, text-to-3D, text-to-protein, text-to-chemicals, these were things that were processed and typed in by humans in the past. And these are now generative approaches. The way that we access data is changed. It used to be based on explicit queries. It is now based on natural language queries, intention queries, semantic queries. And so, we're excited about the work that we're doing with SAP and Dropbox and many others that you're going to hear about. And one of the areas that is really impactful is the software industry, which is about $1 trillion or so, has been building tools that are manually used over the last couple of decades. And now there's a whole new segment of software called copilots and assistants. Instead of manually used, these tools will have copilots to help you use it. And so, instead of licensing software, we will continue to do that, of course, but we will also hire copilots and assistants to help us use these -- use the software. We'll connect all of these copilots and assistants into teams of AIs, which is going to be the modern version of software, modern version of enterprise business software. And so the transformation of software and the way that software has done is driving the hardware underneath. And you can see that it's transforming hardware in two ways. One is something that's largely independent of generative AI. There's two trends: one is related to accelerated computing, general purpose computing is too wasteful of energy and cost. And now that we have much, much better approaches, call it, accelerated computing, you could save an order of magnitude of energy, you can save an order of magnitude of time or you can save an order of magnitudes of cost by using acceleration. And so, accelerated computing is transitioning, if you will, general purpose computing into this new approach. And that's been augmented by a new class of data centers. This is the traditional data centers that you were just talking about where we represent about a third of that. But there is a new class of data centers and this new class of data centers, unlike the data centers of the past, where you have a lot of applications running used by a great many people that are different tenants that are using the same infrastructure and that data center stores a lot of files. These new data centers are very few applications, if not one application, used by basically one tenant and it processes data, it trains models and then generates tokens and generates AI. And we call these new data centers AI factories. We're seeing AI factories being built out everywhere, and just by every country. And so if you look at the way where we are in the expansion, the transition into this new computing approach, the first wave you saw with large language model start-ups, generative AI start-ups and consumer Internet companies, and weren't in the process of ramping that. Meanwhile, while that's being ramped, you see that we're starting to partner with enterprise software companies who would like to build chatbots and copilots and assistants to augment the tools that they have on their platforms. You're seeing GPU specialized CSPs cropping up all over the world and they are dedicated to do really one thing, which is processing AI. You're seeing sovereign AI infrastructures, people -- countries that now recognize that they have to utilize their own data, keep their own data, keep their own culture, process that data and develop their own AI. You see that in India. Several -- about a year ago in Sweden, you are seeing in Japan. Last week, a big announcement in France. But the number of sovereign AI clouds that are being built is really quite significant. And my guess is that almost every major region will have and surely every major country will have their own AI clouds. And so I think you're seeing just new developments as the generative AI wave propagates through every industry, every company, every region. And so we're at the beginning of this inflection, this computing transition.\\nOperator: Your next question comes from the line of Aaron Rakers of Wells Fargo. Your line is open.\\nAaron Rakers: Yeah. Thanks for taking the question. I wanted to ask about kind of the networking side of the business. Given the growth rates that you've now cited, I think, it's 155% year-over-year and strong growth sequentially, it looks like that business is like almost approaching $2.5 billion to $3 billion quarterly level. I'm curious of how you see Ethernet involved evolving and maybe how you would characterize your differentiation of Spectrum-X relative to the traditional Ethernet stack as we start to think about that becoming part of the networking narrative above and maybe beyond just InfiniBand as we look into next year? Thank you.\\nJensen Huang: Yeah. Thanks for the question. Our networking business is already on a $10 billion plus run rate and it's going to get much larger. And as you mentioned, we added a new networking platform to our networking business recently. The vast majority of the dedicated large scale AI factories standardize on InfiniBand. And the reason for that is not only because of its data rate and not only just the latency, but the way that it moves traffic around the network is really important. The way that you process AI and a multi-tenant hyperscale Ethernet environment, the traffic pattern is just radically different. And with InfiniBand and with software defined networks, we could do congestion control, adaptive routing, performance isolation and noise isolation, not to mention, of course, the day rate and the low latency that -- and a very low overhead of InfiniBand that's natural part of InfiniBand. And so, InfiniBand is not so much just the network, it's also a computing fabric. We've put a lot of software-defined capabilities into the fabric including computation. We will do 40-point calculations and computation right on the switch, and right in the fabric itself. And so that's the reason why that difference in Ethernet versus InfiniBand or InfiniBand versus Ethernet for AI factories is so dramatic. And the difference is profound. And the reason for that is because you've just invested in a $2 billion infrastructure for AI factories. A 20%, 25%, 30% difference in overall effectiveness, especially as you scale up is measured in hundreds of millions of dollars of value. And if you will, renting that infrastructure over the course of four to five years, it really, really adds up. And so InfiniBand's value proposition is undeniable for AI factories. However, as we move AI into enterprise. This is enterprise computing what we'd like to enable every company to be able to build their own custom AIs. We're building customer AIs in our company based on our proprietary data, our proprietary type of skills. For example, recently we spoke about one of the models that we're creating, it's called ChipNeMo; we're building many others. There'll be tens, hundreds of custom AI models that we create inside our company. And our company is -- for all of our employee use, doesn't have to be as high performance as the AI factories we used to train the models. And so we would like the AI to be able to run in Ethernet environment. And so what we've done is we invented this new platform that extends Ethernet; doesn't replace Ethernet, it's 100% compliant with Ethernet. And it's optimized for East-West traffic, which is where the computing fabric is. It adds to Ethernet with an end-to-end solution with Bluefield, as well as our Spectrum switch that allows us to perform some of the capabilities that we have in InfiniBand, not all but some. And we achieved excellent results. And the way we go to market is we go to market with our large enterprise partners who already offer our computing solution. And so, HP (NYSE:HPQ), Dell and Lenovo has the NVIDIA AI stack, the NVIDIA AI Enterprise software stack and now they integrate with Bluefield, as well as bundle -- take a market there, Spectrum switch, and they'll be able to offer enterprise customers all over the world with their vast sales force and vast network of resellers a fully integrated, if you will, fully optimized, at least end-to-end AI solution. And so that's basically it, bringing AI to Ethernet for the world's enterprise.\\nOperator: Thank you. Your next question comes from the line of Joe Moore of Morgan Stanley. Your line is open.\\nJoseph Moore: Great. Thank you. I'm wondering if you could talk a little bit more about Grace Hopper and how you see the ability to leverage kind of the microprocessor, how you see that as a TAM expander. And what applications do you see using Grace Hopper versus more traditional H100 applications?\\nJensen Huang: Yeah. Thanks for the question. Grace Hopper is in production -- in high volume production now. We're expecting next year just with all of the design wins that we have in high performance computing and AI infrastructures, we are on a very, very fast ramp with our first data center CPU to a multi-billion dollar product line. This is going to be a very large product line for us. The capability of Grace Hopper is really quite spectacular. It has the ability to create computing nodes that simultaneously has very fast memory, as well as very large memory. In the areas of vector databases or semantic surge, what is called RAG, retrieval augmented generation. So that you could have a generative AI model be able to refer to proprietary data or a factual data before it generates a response, that data is quite large. And you can also have applications or generative models where the context length is very high. You basically store it in entire book into end-to-end system memory before you ask your questions. And so the context length can be quite large this way. The generative models has the ability to still be able to naturally interact with you on one hand. On the other hand, be able to refer to factual data, proprietary data or domain-specific data, you data and be contextually relevant and reduce hallucination. And so that particular use case for example is really quite fantastic for Grace Hopper. It also serves the customers that really care to have a different CPU than x86. Maybe it's a European supercomputing centers or European companies who would like to build up their own ARM ecosystem and like to build up a full stack or CSPs that have decided that they would like to pivot to ARM, because their own custom CPUs are based on ARM. There are variety of different reasons that drives the success of Grace Hopper, but we're off to a just an extraordinary start. This is a home run product.\\nOperator: Your next question comes from the line of Tim Arcuri of UBS. Your line is open.\\nTim Arcuri: Hi. Thanks. I wanted to ask a little bit about the visibility that you have on revenue. I know there's a few moving parts. I guess, on one hand, the purchase commitments went up a lot again. But on the other hand, China bans would arguably pull in when you can fill the demand beyond China. So I know we're not even into 2024 yet and it doesn't sound like, Jensen, you think that next year would be a peak in your Data Center revenue, but I just wanted to sort of explicitly ask you that. Do you think that Data Center can grow even in 2025? Thanks.\\nJensen Huang: Absolutely believe the Data Center can grow through 2025. And there are, of course, several reasons for that. We are expanding our supply quite significantly. We have already one of the broadest and largest and most capable supply chain in the world. Now, remember, people think that the GPU is a chip. But the HGX H100, the Hopper HGX has 35,000 parts, it weighs 70 pounds. Eight of the chips are Hopper. The other 35,000 are not. It is -- even its passive components are incredible. High voltage parts. High frequency parts. High current parts. It is a supercomputer, and therefore, the only way to test a supercomputer is with another supercomputer. Even the manufacturing of it is complicated, the testing of it is complicated, the shipping of it complicated and installation is complicated. And so, every aspect of our HGX supply chain is complicated. And the remarkable team that we have here has really scaled out the supply chain incredibly. Not to mention, all of our HGXs are connected with NVIDIA networking. And the networking, the transceivers, the mix, the cables, the switches, the amount of complexity there is just incredible. And so, I'm just -- first of all, I'm just super proud of the team for scaling up this incredible supply chain. We are absolutely world class. But meanwhile, we're adding new customers and new products. So we have new supply. We have new customers, as I was mentioning earlier. Different regions are standing up GPU specialist clouds, sovereign AI clouds coming out from all over the world, as people realize that they can't afford to export their country's knowledge, their country's culture for somebody else to then resell AI back to them, they have to -- they should, they have the skills and surely with us in combination, we can help them to do that build up their national AI. And so, the first thing that they have to do is, create their AI cloud, national AI cloud. You're also seeing us now growing into enterprise. The enterprise market has two paths. One path -- or if I could say three paths. The first path, of course, just off-the-shelf AI. And there are of course Chat GPT, a fabulous off-the-shelf AI, there'll be others. There's also a proprietary AI, because software companies like ServiceNow and SAP, there are many, many others that can't afford to have their company's intelligence be outsourced to somebody else. And they are about building tools and on top of their tools they should build custom and proprietary and domain-specific copilots and assistants that they can then rent to their customer base. This is -- they're sitting on a goldmine, almost every major tools company in the world is sitting on a goldmine, and they recognize that they have to go build their own custom AIs. We have a new service called an AI foundry, where we leverage NVS (ph) capabilities to be able to serve them in that. And then the next one is enterprises building their own custom AIs, their own custom chatbots, their own custom RAGs. And this capability is spreading all over the world. And the way that we're going to serve that marketplace is with the entire stacks of systems, which includes our compute, our networking and our switches, running our software stack called NVIDIA AI Enterprise, taking it through our market partners, HP, Dell, Lenovo, so on and so forth. And so we're just -- we're seeing the waves of generative AI starting from the start-ups and CSPs, moving to consumer Internet companies, moving to enterprise software platforms, moving to enterprise companies. And then ultimately, one of the areas that you guys have seen us spend a lot of energy on has to do with industrial generative AI. This is where NVIDIA AI and NVIDIA Omniverse comes together and that is a really, really exciting work. And so I think the -- we're at the beginning of a basically across-the-board industrial transition to generative AI to accelerated computing. This is going to affect every company, every industry, every country.\\nOperator: Your next question comes from the line of Toshiya Hari of Goldman Sachs. Your line is open.\\nToshiya Hari: Hi. Thank you. I wanted to clarify something with Colette real quick, and then I had a question for Jensen as well. Colette, you mentioned that you'll be introducing regulation-compliant products over the next couple of months. Yet, the contribution to Q4 revenue should be relatively limited. Is that a timing issue and could it be a source of reacceleration in growth for Data Center in April and beyond or are the price points such that the contribution to revenue going forward should be relatively limited? And then the question for Jensen, the AI foundry service announcement from last week. I just wanted to ask about that, and hopefully, have you expand on it. How is the monetization model going to work? Is it primarily services and software revenue? How should we think about the long term opportunity set? And is this going to be exclusive to Microsoft or do you have plans to expand to other partners as well? Thank you.\\nColette Kress: Thanks, Toshiya. On the question regarding potentially new products that we could provide to our China customers. It's a significant process to both design and develop these new products. As we discussed, we're going to make sure that we are in full discussions with the U.S. government of our intent to move products as well. Given our state about where we are in the quarter, we're already several weeks into the quarter. So it's just going to take some time for us to go through and discussing with our customers the needs and desires of these new products that we have. And moving forward, whether that's medium-term or long-term, it's just hard to say both the [Technical Difficulty] of what we can produce with the U.S. government and what the interest of our China customers in this. So we stay still focused on finding that right balance for our China customers, but it's hard to say at this time.\\nJensen Huang: Toshiya, thanks for the question. There is a glaring opportunity in the world for AI foundry, and it makes so much sense. First, every company has its core intelligence. It makes up our company. Our data, our domain expertise, in the case of many companies, we create tools, and most of the software companies in the world are tool platforms, and those tools are used by people today. And in the future, it's going to be used by people augmented with a whole bunch of AIs that we hire. And these platforms just got to go across the world and you'll see and we've only announced a few; SAP, ServiceNow, Dropbox, Getty, many others are coming. And the reason for that is because they have their own proprietary AI. They want their own proprietary AI. They can't afford to outsource their intelligence and handout their data, and handout their flywheel for other companies to build the AI for them. And so, they come to us. We have several things that are really essential in a foundry. Just as TSMC as a foundry, you have to have AI technology. And as you know, we have just an incredible depth of AI capability -- AI technology capability. And then second, you have to have the best practice known practice, the skills of processing data through the invention of AI models to create AIs that are guardrails, fine-tuned, so on and so forth, that are safe, so on and so forth. And the third thing is you need factories. And that's what DGX Cloud is. Our AI models are called AI Foundations. Our process, if you will, our CAD system for creating AIs are called NeMo and they run on NVIDIA's factories we call DGX Cloud. Our monetization model is that with each one of our partners they rent a sandbox on DGX Cloud, where we work together, they bring their data, they bring their domain expertise, we bring our researchers and engineers, we help them build their custom AI. We help them make that custom AI incredible. Then that custom AI becomes theirs. And they deploy it on the runtime that is enterprise grade, enterprise optimized or outperformance optimized, runs across everything NVIDIA. We have a giant installed base in the cloud, on-prem, anywhere. And it's secure, securely patched, constantly patched and optimized and supported. And we call that NVIDIA AI Enterprise. NVIDIA AI Enterprise is $4,500 per GP per year, that's our business model. Our business model is basically a license. Our customers then with that basic license can build their monetization model on top of. In a lot of ways we're wholesale, they become retail. They could have a per -- they could have subscription license base, they could per instance or they could do per usage, there is a lot of different ways that they could take a -- create their own business model, but ours is basically like a software license, like an operating system. And so our business model is help you create your custom models, you run those custom models on NVIDIA AI Enterprise. And it's off to a great start. NVIDIA AI Enterprise is going to be a very large business for us.\\nOperator: Your next question comes from the line of Stacy Rasgon of Bernstein Research. Your line is open.\\nStacy Rasgon: Hi, guys. Thanks for taking my questions. Colette, I wanted to know if it weren't for the China restrictions would the Q4 guide has been higher or are you supply-constrained in just reshipping stuff that would have gone to China elsewhere? And I guess along those lines you give us a feeling for where your lead times are right now in data center and just the China redirection such as-is, is it lowering those lead times, because you've got parts that are sort of immediately available to ship?\\nColette Kress: Yeah. Stacy, let me see if I can help you understand. Yes, there are still situations where we are working on both improving our supply each and every quarter. We've done a really solid job of ramping every quarter, which has defined our revenue. But with the absence of China for our outlook for Q4, sure, there could have been some things that we are not supply-constrained that we could have sold, but kind of we would no longer can. So could our guidance had been a little higher in our Q4? Yes. We are still working on improving our supply on plan, on continuing growing all throughout next year as well towards that.\\nOperator: Your next question comes from the line of Matt Ramsay of TD Cowen. Your line is open.\\nMatt Ramsay: Thank you very much. Congrats, everybody, on the results. Jensen, I had a two-part question for you, and it comes off of sort of one premise. And the premise is, I still get a lot of questions from investors thinking about AI training as being NVIDIA's dominant domain and somehow inference, even large model inference takes more and more of the TAM that the market will become more competitive. You'll be less differentiated et cetera., et cetera. So I guess the two parts of the question are: number one, maybe you could spend a little bit of time talking about the evolution of the inference workload as we move to LLMs and how your company is positioned for that rather than smaller model inference. And second, up until a month or two ago, I never really got any questions at all about the data processing piece of the AI workloads. So the pieces of manipulating the data before training, between training and inference, after inference and I think that's a large part of the workload now. Maybe you could talk about how CUDA is enabling acceleration of those pieces of the workload. Thanks.\\nJensen Huang: Sure. Inference is complicated. It's actually incredibly complicated. If you -- we this quarter announced one of the most exciting new engines, optimizing compilers called TensorRT-LLM. The reception has been incredible. You got to GitHub, it's been downloaded a ton, a whole lot of stars, integrated into stacks and frameworks all over the world, almost instantaneously. And there are several reasons for that, obviously. We could create TensorRT-LLM, because CUDA is programmable. If CUDA and our GPUs were not so programmable, it would really be hard for us to improve software stacks at the pace that we do. TensorRT-LLM, on the same GPU, without anybody touching anything, improves the performance by a factor of two. And then on top of that, of course, the pace of our innovation is so high. H200 increases it by another factor of two. And so, our inference performance, another way of saying inference cost, just reduced by a factor of four within about a year's time. And so, that's really, really hard to keep up with. The reason why everybody likes our inference engine is because our installed base. We've been dedicated to our installed base for 20 years, 20-plus years. We have an installed base that is not only largest in every single cloud, it's in every available from every enterprise system maker, it's used by companies of just about every industry. And every -- anytime you see a NVIDIA GPU, it runs our stack. It's architecturally compatible, something we've been dedicated to for a very long time. We're very disciplined about it. We make it our, if you will, architecture compatibility is job one. And that has conveyed to the world, the certainty of our platform stability. NVIDIA's platform stability certainty is the reason why everybody builds on us first and the reason why everybody optimizes on us first. All the engineering and all the work that you do, all the invention of technologies that you build on top of NVIDIA accrues to the -- and benefits everybody that uses our GPUs. And we have such a large installed base, large -- millions and millions of GPUs in cloud, 100 million GPUs from people’s PCs just about every workstation in the world, and they all architecturally compatible. And so, if you are an inference platform and you're deploying an inference application, you are basically an application provider. And as a software application provider, you're looking for large installed base. Data processing, before you could train a model, you have to curate the data, you have to dedupe the data, maybe you have to augment the data with synthetic data. So, process the data, clean the data, align the data, normalize the data, all of that data is measured not in bytes or megabytes, it's measured in terabytes and petabytes. And the amount of data processing that you do before data engineering, before that you do training is quite significant. It could represent 30%, 40%, 50% of the amount of work that you ultimately do. And what you -- and ultimately creating a data driven machine learning service. And so data processing is just a massive part. We accelerate Spark, we accelerate Python. One of the coolest things that we just did is called cuDF Pandas. Without one line of code, Pandas, which is the single most successful data science framework in the world. Pandas now is accelerated by NVIDIA CUDA. And just out-of-the box, without the line of code and so the acceleration is really quite terrific and people are just incredibly excited about it. And Pandas was designed for one purpose and one purpose only, really data processing, it's for data science. And so NVIDIA CUDA gives you all of that.\\nOperator: Your final question comes from the line of Harlan Sur of J.P. Morgan. Your line is open.\\nHarlan Sur: Good afternoon. Thanks for taking my question. If you look at the history of the tech industry like those companies that have been successful have always been focused on ecosystem; silicon, hardware, software, strong partnerships and just as importantly, right, an aggressive cadence of new products, more segmentation over time. The team recently announced a more aggressive new product cadence in data center from two years to now every year with higher levels of segmentation, training, optimization in printing CPU, GPU, DPU networking. How do we think about your R&D OpEx growth outlook to support a more aggressive and expanding forward roadmap, but more importantly, what is the team doing to manage and drive execution through all of this complexity?\\nJensen Huang: Gosh. Boy, that's just really excellent. You just wrote NVIDIA's business plan, and so you described our strategy. First of all, there is a fundamental reason why we accelerate our execution. And the reason for that is because it fundamentally drives down cost. When the combination of TensorRT-LLM and H200 reduce the cost for our customers for large model inference by a factor of four, and so that includes, of course, our speeds and feeds, but mostly it's because of our software, mostly the software benefits because of the architecture. And so we want to accelerate our roadmap for that reason. The second reason is to expand the reach of generative AI, the world's number of data center configurations -- this is kind of the amazing thing. NVIDIA is in every cloud, but not one cloud is the same. NVIDIA is working with every single cloud service provider and not one of the networking control plane, security posture is the same. Everybody's platform is different and yet we're integrated into all of their stacks, all of their data centers and we work incredibly well with all of them. And not to mention, we then take the whole thing and we create AI factories that are standalone. We take our platform, we can put them into supercomputers, we can put them into enterprise. Bringing AI to enterprise is something generative AI Enterprise something nobody's ever done before. And we're right now in the process of going to market with all of that. And so the complexity includes, of course, all the technologies and segments and the pace. It includes the fact that we are architecturally compatible across every single one of those. It includes all of the domain specific libraries that we create. The reason why every computer company, without thinking, can integrate NVIDIA into their roadmap and take it to market. And the reason for that is, because there is market demand for it. There is market demand in healthcare, there is market demand in manufacturing, there is market demand, of course, in AI, including financial services, in supercomputing and quantum computing. The list of markets and segments that we have domain specific libraries is incredibly broaden. And then finally, we have an end-to-end solution for data centers; InfiniBand networking, Ethernet networking, x86, ARM, just about every permutation combination of solutions -- technology solutions and software stacks provided. And that translates to having the largest number of ecosystem software developers; the largest ecosystem of system makers; the largest and broadest distribution partnership network; and ultimately, the greatest reach. And that takes -- surely that takes a lot of energy. But the thing that really holds it together, and this is a great decision that we made decades ago, which is everything is architecturally compatible. When we develop a domain specific language that runs on one GPU, it runs on every GPU. When we optimize TensorRT for the cloud, we optimized it for enterprise. When we do something that brings in a new feature, a new library, a new feature or a new developer, they instantly get the benefit of all of our reach. And so that discipline, that architecture compatible discipline that has lasted more than a couple of decades now, is one of the reasons why NVIDIA is still really, really efficient. I mean, we're 28,000 people large and serving just about every single company, every single industry, every single market around the world.\\nOperator: Thank you. I will now turn the call back over to Jensen Huang for closing remarks.\\nJensen Huang: Our strong growth reflects the broad industry platform transition from general purpose to accelerated computing and generative AI. Large language models start-ups consumer Internet companies and global cloud service providers are the first movers. The next waves are starting to build. Nations and regional CSPs are building AI clouds to serve local demand. Enterprise software companies like Adobe and Dropbox, SAP and ServiceNow are adding AI copilots and assistants to their platforms. Enterprises in the world's largest industries are creating custom AIs to automate and boost productivity. The generative AI era is in full steam and has created the need for a new type of data center, an AI factory; optimized for refining data and training, and inference, and generating AI. AI factory workloads are different and incremental to legacy data center workloads supporting IT tasks. AI factories run copilots and AI assistants, which are significant software TAM expansion and are driving significant new investment. Expanding the $1 trillion traditional data center infrastructure installed base, empowering the AI Industrial Revolution. NVIDIA H100 HGX with InfiniBand and the NVIDIA AI software stack define an AI factory today. As we expand our supply chain to meet the world's demand, we are also building new growth drivers for the next wave of AI. We highlighted three elements to our new growth strategy that are hitting their stride: CPU, networking, and software and services. Grace is NVIDIA's first data center CPU. Grace and Grace Hopper are in full production and ramping into a new multi-billion dollar product line next year. Irrespective of the CPU choice, we can help customers build an AI factory. NVIDIA networking now exceeds $10 billion annualized revenue run rate. InfiniBand grew five-fold year-over-year, and is positioned for excellent growth ahead as the networking of AI factories. Enterprises are also racing to adopt AI and Ethernet is the standard networking. This week we announced an Ethernet for AI platform for enterprises. NVIDIA Spectrum-X is an end-to-end solution of Bluefield SuperNIC, Spectrum-4 Ethernet switch and software that boosts Ethernet performance by up to 1.6x for AI workloads. Dell, HPE and Lenovo have joined us to bring a full generative AI solution of NVIDIA AI computing, networking and software to the world's enterprises. NVIDIA software and services is on track to exit the year at an annualized run rate of $1 billion. Enterprise software platforms like ServiceNow and SAP need to build and operate proprietary AI. Enterprises need to build and deploy custom AI copilots. We have the AI technology, expertise and scale to help customers build custom models with their proprietary data on NVIDIA DGX Cloud and deploy the AI applications on enterprise grade NVIDIA AI Enterprise. NVIDIA is essentially an AI foundry. NVIDIA's GPUs, CPUs, networking, AI foundry services and NVIDIA AI Enterprise software are all growth engines in full throttle. Thanks for joining us today. We look forward to updating you on our progress next quarter.\\nOperator: This concludes today's conference call. You may now disconnect.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " \"NVIDIA's latest earnings call revealed record-breaking revenue of $18.1 billion, marking a 34% sequential increase and a 200% year-on-year increase. The company's Data Center segment was a primary driver of this growth, achieving record revenue of $14.5 billion. However, the company anticipates a significant decline in the fourth quarter due to new U.S. export control regulations affecting sales to China and other markets. \\nKey takeaways from the call include:NVIDIA's Data Center segment saw record revenue, driven by the NVIDIA HGX platform and InfiniBand networking. Major companies like Meta (NASDAQ:META) and Adobe (NASDAQ:ADBE) are integrating NVIDIA's technology into their platforms, contributing to the strong demand for AI supercomputers and data center infrastructure.NVIDIA is expanding its Data Center product portfolio to comply with new U.S. export control regulations. Despite expecting a decrease in sales to China and other markets in the fourth quarter, the company is focusing on national investment in sovereign AI infrastructure.The Gaming segment recorded revenue of $2.86 billion, a 15% sequential increase and an 80% year-on-year increase, driven by strong demand for NVIDIA's RTX ray tracing and AI technology.In the Pro Vis segment, revenue reached $416 million, a 10% sequential increase and a 108% year-on-year increase. NVIDIA RTX is the preferred platform for professional design, engineering, and simulation, with AI emerging as a demand driver.NVIDIA's CEO, Jensen Huang, discussed the expansion of AI factories and the transition into a new computing approach. He also expressed confidence in the continued growth of the Data Center business through 2025.During the call, NVIDIA highlighted its product innovations, including the H200 GPU, which offers faster and larger memory for generative AI and language models, and the GH200 Grace Hopper Superchip, a combination of an ARM-based CPU with a Hopper GPU. The Grace Hopper Superchip is being adopted by supercomputing customers, with the combined AI compute capacity of supercomputers built on Grace Hopper expected to exceed 200 exaflops next year. \\nIn addition to its product offerings, NVIDIA is also expanding its networking into the Ethernet space with its new Spectrum-X end-to-end Ethernet offering, which will be available in Q1 next year. This offering can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. \\nNVIDIA also discussed its business strategy, emphasizing the importance of reselling AI technology to customers and its growth in the enterprise market. The company introduced an AI foundry service to help enterprises build custom AI models and announced an aggressive product release schedule in the data center market. \\nLooking ahead, NVIDIA expects total revenue of $20 billion for Q4 2024, driven by strong demand in the Data Center sector. Despite regulatory challenges, the company is confident in its growth potential, citing expanded supply, new customers, and its entrance into the enterprise market.Full transcript -  Nvidia Corp  (NASDAQ:NVDA) Q3 2024:Operator: Good afternoon. My name is JL, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's Third Quarter Earnings Call. All lines have been placed on mute to prevent any background noise. After the speakers’ remarks, there will be a question-and-answer session. [Operator Instructions] Simona Jankowski, you may now begin your conference.\\nSimona Jankowski: Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2024. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter and fiscal 2024. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All statements are made as of today, November 21, 2023, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\\nColette Kress: Thanks, Simona. Q3 was another record quarter. Revenue of $18.1 billion was up 34% sequentially and up more than 200% year-on-year and well above our outlook for $16 billion. Starting with Data Center. The continued ramp of the NVIDIA HGX platform based on our Hopper Tensor Core GPU architecture, along with InfiniBand end-to-end networking, drove record revenue of $14.5 billion, up 41% sequentially and up 279% year-on-year. NVIDIA HGX with InfiniBand together are essentially the reference architecture for AI supercomputers and data center infrastructures. Some of the most exciting generative AI applications are built and run on NVIDIA, including Adobe Firefly, ChatGPT, Microsoft (NASDAQ:MSFT) 365 Copilot, CoAssist, now assist with ServiceNow (NYSE:NOW) and Zoom (NASDAQ:ZM) AI Companion. Our Data Center compute revenue quadrupled from last year and networking revenue nearly tripled. Investments in infrastructure for training and inferencing large language models, deep learning, recommender systems and generative AI applications is fueling strong broad-based demand for NVIDIA accelerated computing. Inferencing is now a major workload for NVIDIA AI computing. Consumer Internet companies and enterprises drove exceptional sequential growth in Q3, comprising approximately half of our Data Center revenue and outpacing total growth. Companies like Meta are in full production with deep learning, recommender systems and also investing in generative AI to help advertisers optimize images and text. Most major consumer Internet companies are racing to ramp up generative AI deployment. The enterprise wave of AI adoption is now beginning. Enterprise software companies such as Adobe, Databricks, Snowflake (NYSE:SNOW) and ServiceNow are adding AI copilots and the systems to their platforms. And broader enterprises are developing custom AI for vertical industry applications such as Tesla (NASDAQ:TSLA) in autonomous driving. Cloud service providers drove roughly the other half of our Data Center revenue in the quarter. Demand was strong from all hyperscale CSPs, as well as from a broadening set of GPU-specialized CSPs globally that are rapidly growing to address the new market opportunities in AI. NVIDIA H100 Tensor Core GPU instances are now generally available in virtually every cloud with instances in high demand. We have significantly increased supply every quarter this year to meet strong demand and expect to continue to do so next year. We will also have a broader and faster product launch cadence to meet the growing and diverse set of AI opportunities. Towards the end of the quarter, the U.S. government announced a new set of export control regulations for China and other markets, including Vietnam and certain countries in the Middle East. These regulations require licenses for the export of a number of our products, including our Hopper and Ampere 100 and 800 series and several others. Our sales to China and other affected destinations derived from products that are now subject to licensing requirements have consistently contributed approximately 20% to 25% of Data Center revenue over the past few quarters. We expect that our sales to these destinations will decline significantly in the fourth quarter. So we believe will be more than offset by strong growth in other regions. The U.S. government designed the regulation to allow the U.S. industry to provide data center compute products to markets worldwide, including China. Continuing to compete worldwide as the regulations encourage, promotes U.S. technology leadership, spurs economic growth and supports U.S. jobs. For the highest performance levels, the government requires licenses. For lower performance levels, the government requires a streamlined prior notification process. And for products even lower performance levels, the government does not require any notice at all. Following the government's clear guidelines, we are working to expand our Data Center product portfolio to offer compliance solutions for each regulatory category, including products for which the U.S. government does not wish to have advance notice before each shipment. We are working with some customers in China and the Middle East to pursue licenses from the U.S. government. It is too early to know whether these will be granted for any significant amount of revenue. Many countries are awakening to the need to invest in sovereign AI infrastructure to support economic growth and industrial innovation. With investments in domestic compute capacity, nations can use their own data to train LLMs and support their local generative AI ecosystems. For example, we are working with India's government and largest tech companies including  Infosys  (NS:INFY), Reliance and Tata to boost their sovereign AI infrastructure. And French private cloud provider, Scaleway, is building a regional AI cloud based on NVIDIA H100 InfiniBand and NVIDIA's AI Enterprise software to fuel advancement across France and Europe. National investment in compute capacity is a new economic imperative and serving the sovereign AI infrastructure market represents a multi-billion dollar opportunity over the next few years. From a product perspective, the vast majority of revenue in Q3 was driven by the NVIDIA HGX platform based on our Hopper GPU architecture with lower contribution from the prior generation Ampere GPU architecture. The new L40S GPU built for industry standard servers began to ship, supporting training and inference workloads across a variety of consumers. This was also the first revenue quarter of our GH200 Grace Hopper Superchip, which combines our ARM-based Grace CPU with a Hopper GPU. Grace and Grace Hopper are ramping into a new multi-billion dollar product line. Grace Hopper instances are now available at GPU specialized cloud providers, and coming soon to Oracle (NYSE:ORCL) Cloud. Grace Hopper is also getting significant traction with supercomputing customers. Initial shipments to Los Alamos National Lab and the Swiss National Supercomputing Center took place in the third quarter. The UK government announced it will build one of the world's fastest AI supercomputers called Isambard-AI with almost 5,500 Grace Hopper Superchips. German supercomputing center, Julich, also announced that it will build its next-generation AI supercomputer with close to 24,000 Grace Hopper Superchips and Quantum-2 InfiniBand, making it the world's most powerful AI supercomputer with over 90 exaflops of AI performance. All-in, we estimate that the combined AI compute capacity of all the supercomputers built on Grace Hopper across the U.S., Europe and Japan next year will exceed 200 exaflops with more wins to come. Inference is contributing significantly to our data center demand, as AI is now in full production for deep learning, recommenders, chatbots, copilots and text to image generation and this is just the beginning. NVIDIA AI offers the best inference performance and versatility, and thus the lower power and cost of ownership. We are also driving a fast cost reduction curve. With the release of TensorRT-LLM, we now achieved more than 2x the inference performance for half the cost of inferencing LLMs on NVIDIA GPUs. We also announced the latest member of the Hopper family, the H200, which will be the first GPU to offer HBM3e, faster, larger memory to further accelerate generative AI and LLMs. It moves inference speed up to another 2x compared to H100 GPUs for running LLMs like Norma2 (ph). Combined, TensorRT-LLM and H200, increased performance or reduced cost by 4x in just one year. With our customers changing their stack, this is a benefit of CUDA and our architecture compatibility. Compared to the A100, H200 delivers an 18x performance increase for inferencing models like GPT-3, allowing customers to move to larger models and with no increase in latency. Amazon (NASDAQ:AMZN) Web Services, Google (NASDAQ:GOOGL) Cloud, Microsoft Azure and Oracle Cloud will be among the first CSPs to offer H200-based instances starting next year. At last week's Microsoft Ignite, we deepened and expanded our collaboration with Microsoft across the entire stock. We introduced an AI foundry service for the development and tuning of custom generative AI enterprise applications running on Azure. Customers can bring their domain knowledge and proprietary data and we help them build their AI models using our AI expertise and software stock in our DGX cloud, all with enterprise grade security and support. SAP and  Amdocs  (NASDAQ:DOX) are the first customers of the NVIDIA AI foundry service on Microsoft Azure. In addition, Microsoft will launch new confidential computing instances based on the H100. The H100 remains the top performing and most versatile platform for AI training and by a wide margin, as shown in the latest MLPerf industry benchmark results. Our training cluster included more than 10,000 H100 GPUs or 3x more than in June, reflecting very efficient scaling. Efficient scaling is a key requirement in generative AI, because LLMs are growing by an order of magnitude every year. Microsoft Azure achieved similar results on a nearly identical cluster, demonstrating the efficiency of NVIDIA AI in public cloud deployments. Networking now exceeds a $10 billion annualized revenue run rate. Strong growth was driven by exceptional demand for InfiniBand, which grew fivefold year-on-year. InfiniBand is critical to gaining the scale and performance needed for training LLMs. Microsoft made this very point last week, highlighting that Azure uses over 29,000 miles of InfiniBand cabling, enough to circle the globe. We are expanding NVIDIA networking into the Ethernet space. Our new Spectrum-X end-to-end Ethernet offering with technologies, purpose built for AI, will be available in Q1 next year. With support from leading OEMs, including Dell (NYSE:DELL), HPE and Lenovo. Spectrum-X can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. Let me also provide an update on our software and services offerings, where we are starting to see excellent adoption. We are on track to exit the year at an annualized revenue run rate of $1 billion for our recurring software, support and services offerings. We see two primary opportunities for growth over the intermediate term with our DGX cloud service and with our NVIDIA AI Enterprise software, each reflects the growth of enterprise AI training and enterprise AI inference, respectively. Our latest DGX cloud customer announcement was this morning as part of an AI research collaboration with Gentech, the biotechnology pioneer also plans to use our BioNeMo LLM framework to help accelerate and optimize their AI drug discovery platform. We now have enterprise AI partnership with Adobe, Dropbox (NASDAQ:DBX), Getty, SAP, ServiceNow, Snowflake and others to come. Okay. Moving to Gaming. Gaming revenue of $2.86 billion was up 15% sequentially and up more than 80% year-on-year with strong demand in the important back-to-school shopping season with NVIDIA RTX ray tracing and AI technology now available at price points as low as $299. We entered the holidays with the best-ever line-up for gamers and creators. Gaming has doubled relative to pre-COVID levels even against the backdrop of lackluster PC market performance. This reflects the significant value we've brought to the gaming ecosystem with innovations like RTX and DLSS. The number of games and applications supporting these technologies has exploded in that period, driving upgrades and attracting new buyers. The RTX ecosystem continues to grow. There are now over 475 RTX-enabled games and applications. Generative AI is quickly emerging as the new pillar app for high performance PCs. NVIDIA RTX GPUs to find the most performance AI PCs and workstations. We just released TensorRT-LLM for Windows, which speeds on-device LLM inference up by 4x. With an installed base of over 100 million, NVIDIA RTX is the natural platform for AI application developers. Finally, our GeForce NOW cloud gaming service continues to build momentum. Its library of PC games surpassed 1,700 titles, including the launches of Alan Wake 2, Baldur's Gate 3, Cyberpunk 2077: Phantom Liberty and Starfield. Moving to the Pro Vis. Revenue of $416 million was up 10% sequentially and up 108% year-on year. NVIDIA RTX is the workstation platform of choice for professional design, engineering and simulation use cases and AI is emerging as a powerful demand driver. Early applications include inference for AI imaging in healthcare and edge AI in smart spaces and the public sector. We launched a new line of desktop workstations based on NVIDIA RTX Ada Lovelace generation GPUs and ConnectX, SmartNICs offering up to 2x the AI processing ray tracing and graphics performance of the previous generations. These powerful new workstations are optimized for AI workloads such as fine tune AI models, training smaller models and running inference locally. We continue to make progress on Omniverse, our software platform for designing, building and operating 3D virtual worlds. Mercedes-Benz (OTC:MBGAF) is using Omniverse powered digital twins to plan, design, build and operate its manufacturing and assembly facilities, helping it increase efficiency and reduce defects. Oxxon (ph) is also incorporating Omniverse into its manufacturing process, including end-to-end simulation for the entire robotics and automation pipeline, saving time and cost. We announced two new Omniverse Cloud services for automotive digitalization available on Microsoft Azure, a virtual factory simulation engine and autonomous vehicle simulation engine. Moving to Automotive. Revenue was $261 million, up 3% sequentially and up 4% year-on year, primarily driven by continued growth in self-driving platforms based on NVIDIA DRIVE Orin SOC and the ramp of AI cockpit solutions with global OEM customers. We extended our automotive partnership of Foxconn to include NVIDIA DRIVE for our next-generation automotive SOC. Foxconn has become the ODM for EVs. Our partnership provides Foxconn with a standard AV sensor and computing platform for their customers to easily build a state-of-an-art safe and secure software defined car. Now we're going to move to the rest of the P&L. GAAP gross margin expanded to 74% and non-GAAP gross margin to 75%, driven by higher Data Center sales and lower net inventory reserve, including a 1 percentage point benefit from the release of previously reserved inventory related to the Ampere GPU architecture products. Sequentially, GAAP operating expenses were up 12% and non-GAAP operating expenses were up 10%, primarily reflecting increased compensation and benefits. Let me turn to the fourth quarter of fiscal 2024. Total revenue is expected to be $20 billion, plus or minus 2%. We expect strong sequential growth to be driven by Data Center, with continued strong demand for both compute and networking. Gaming will likely decline sequentially as it is now more aligned with notebook seasonality. GAAP and non-GAAP gross margins are expected to be 74.5% and 75.5%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $3.17 billion and $2.2 billion, respectively. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $200 million, excluding gains and losses from non-affiliated investments. GAAP and non-GAAP tax rates are expected to be 15%, plus or minus 1% excluding any discrete items. Further financial information are included in the CFO commentary and other information available on our IR website. In closing, let me highlight some upcoming events for the financial community. We will attend the UBS Global Technology Conference in Scottsdale, Arizona, on November 28; the Wells Fargo TMT Summit in Rancho Palos Verdes, California on November 29; the Arete Virtual Tech Conference on December 7; and the J.P. Morgan Health Care Conference in San Francisco on January 8. Our earnings call to discuss the results of our fourth quarter and fiscal 2024 is scheduled for Wednesday, February 21. We will now open the call for questions. Operator, will you please poll for questions.\\nOperator: [Operator Instructions] Your first question comes from the line of Vivek Arya of Bank of America. Your line is open.\\nVivek Arya: Thanks for taking my question. Just, Colette, wanted to clarify what China contributions are you expecting in Q4. And then, Jensen, the main question is for you, where do you think we are in the adoption curve in terms of your shipments into the generative AI market? Because when I just look at the trajectory of your data center, is growth -- it will be close to nearly 30% of all the spending in data center next year. So what metrics are you keeping an eye on to inform you that you can continue to grow? Just where are we in the adoption curve of your products into the generative AI market? Thank you.\\nColette Kress: So, first let me start with your question, Vivek, on export controls and the impacts that we are seeing in our Q4 outlook and guidance that we provided. We had seen historically over the last several quarters that China and some of the other impacted destinations to be about 20% to 25% of our Data Center revenue. We are expecting in our guidance for that to decrease substantially as we move into Q4. The export controls will have a negative effect on our China business. And we do not have good visibility into the magnitude of that impact even over the long-term. We are though working to expand our Data Center product portfolio to possibly offer new regulation compliance solutions that do not require a license, these products, they may become available in the next coming months. However, we don't expect their contribution to be material or meaningful as a percentage of the revenue in Q4.\\nJensen Huang: Generative AI is the largest TAM expansion of software and hardware that we've seen in several decades. At the core of it, what's really exciting is that, what was largely a retrieval based computing approach, almost everything that you do is retrieved off of storage somewhere, has been augmented now, added with a generative method. And it's changed almost everything. You could see that text-to-text, text-to-image, text-to-video, text-to-3D, text-to-protein, text-to-chemicals, these were things that were processed and typed in by humans in the past. And these are now generative approaches. The way that we access data is changed. It used to be based on explicit queries. It is now based on natural language queries, intention queries, semantic queries. And so, we're excited about the work that we're doing with SAP and Dropbox and many others that you're going to hear about. And one of the areas that is really impactful is the software industry, which is about $1 trillion or so, has been building tools that are manually used over the last couple of decades. And now there's a whole new segment of software called copilots and assistants. Instead of manually used, these tools will have copilots to help you use it. And so, instead of licensing software, we will continue to do that, of course, but we will also hire copilots and assistants to help us use these -- use the software. We'll connect all of these copilots and assistants into teams of AIs, which is going to be the modern version of software, modern version of enterprise business software. And so the transformation of software and the way that software has done is driving the hardware underneath. And you can see that it's transforming hardware in two ways. One is something that's largely independent of generative AI. There's two trends: one is related to accelerated computing, general purpose computing is too wasteful of energy and cost. And now that we have much, much better approaches, call it, accelerated computing, you could save an order of magnitude of energy, you can save an order of magnitude of time or you can save an order of magnitudes of cost by using acceleration. And so, accelerated computing is transitioning, if you will, general purpose computing into this new approach. And that's been augmented by a new class of data centers. This is the traditional data centers that you were just talking about where we represent about a third of that. But there is a new class of data centers and this new class of data centers, unlike the data centers of the past, where you have a lot of applications running used by a great many people that are different tenants that are using the same infrastructure and that data center stores a lot of files. These new data centers are very few applications, if not one application, used by basically one tenant and it processes data, it trains models and then generates tokens and generates AI. And we call these new data centers AI factories. We're seeing AI factories being built out everywhere, and just by every country. And so if you look at the way where we are in the expansion, the transition into this new computing approach, the first wave you saw with large language model start-ups, generative AI start-ups and consumer Internet companies, and weren't in the process of ramping that. Meanwhile, while that's being ramped, you see that we're starting to partner with enterprise software companies who would like to build chatbots and copilots and assistants to augment the tools that they have on their platforms. You're seeing GPU specialized CSPs cropping up all over the world and they are dedicated to do really one thing, which is processing AI. You're seeing sovereign AI infrastructures, people -- countries that now recognize that they have to utilize their own data, keep their own data, keep their own culture, process that data and develop their own AI. You see that in India. Several -- about a year ago in Sweden, you are seeing in Japan. Last week, a big announcement in France. But the number of sovereign AI clouds that are being built is really quite significant. And my guess is that almost every major region will have and surely every major country will have their own AI clouds. And so I think you're seeing just new developments as the generative AI wave propagates through every industry, every company, every region. And so we're at the beginning of this inflection, this computing transition.\\nOperator: Your next question comes from the line of Aaron Rakers of Wells Fargo. Your line is open.\\nAaron Rakers: Yeah. Thanks for taking the question. I wanted to ask about kind of the networking side of the business. Given the growth rates that you've now cited, I think, it's 155% year-over-year and strong growth sequentially, it looks like that business is like almost approaching $2.5 billion to $3 billion quarterly level. I'm curious of how you see Ethernet involved evolving and maybe how you would characterize your differentiation of Spectrum-X relative to the traditional Ethernet stack as we start to think about that becoming part of the networking narrative above and maybe beyond just InfiniBand as we look into next year? Thank you.\\nJensen Huang: Yeah. Thanks for the question. Our networking business is already on a $10 billion plus run rate and it's going to get much larger. And as you mentioned, we added a new networking platform to our networking business recently. The vast majority of the dedicated large scale AI factories standardize on InfiniBand. And the reason for that is not only because of its data rate and not only just the latency, but the way that it moves traffic around the network is really important. The way that you process AI and a multi-tenant hyperscale Ethernet environment, the traffic pattern is just radically different. And with InfiniBand and with software defined networks, we could do congestion control, adaptive routing, performance isolation and noise isolation, not to mention, of course, the day rate and the low latency that -- and a very low overhead of InfiniBand that's natural part of InfiniBand. And so, InfiniBand is not so much just the network, it's also a computing fabric. We've put a lot of software-defined capabilities into the fabric including computation. We will do 40-point calculations and computation right on the switch, and right in the fabric itself. And so that's the reason why that difference in Ethernet versus InfiniBand or InfiniBand versus Ethernet for AI factories is so dramatic. And the difference is profound. And the reason for that is because you've just invested in a $2 billion infrastructure for AI factories. A 20%, 25%, 30% difference in overall effectiveness, especially as you scale up is measured in hundreds of millions of dollars of value. And if you will, renting that infrastructure over the course of four to five years, it really, really adds up. And so InfiniBand's value proposition is undeniable for AI factories. However, as we move AI into enterprise. This is enterprise computing what we'd like to enable every company to be able to build their own custom AIs. We're building customer AIs in our company based on our proprietary data, our proprietary type of skills. For example, recently we spoke about one of the models that we're creating, it's called ChipNeMo; we're building many others. There'll be tens, hundreds of custom AI models that we create inside our company. And our company is -- for all of our employee use, doesn't have to be as high performance as the AI factories we used to train the models. And so we would like the AI to be able to run in Ethernet environment. And so what we've done is we invented this new platform that extends Ethernet; doesn't replace Ethernet, it's 100% compliant with Ethernet. And it's optimized for East-West traffic, which is where the computing fabric is. It adds to Ethernet with an end-to-end solution with Bluefield, as well as our Spectrum switch that allows us to perform some of the capabilities that we have in InfiniBand, not all but some. And we achieved excellent results. And the way we go to market is we go to market with our large enterprise partners who already offer our computing solution. And so, HP (NYSE:HPQ), Dell and Lenovo has the NVIDIA AI stack, the NVIDIA AI Enterprise software stack and now they integrate with Bluefield, as well as bundle -- take a market there, Spectrum switch, and they'll be able to offer enterprise customers all over the world with their vast sales force and vast network of resellers a fully integrated, if you will, fully optimized, at least end-to-end AI solution. And so that's basically it, bringing AI to Ethernet for the world's enterprise.\\nOperator: Thank you. Your next question comes from the line of Joe Moore of Morgan Stanley. Your line is open.\\nJoseph Moore: Great. Thank you. I'm wondering if you could talk a little bit more about Grace Hopper and how you see the ability to leverage kind of the microprocessor, how you see that as a TAM expander. And what applications do you see using Grace Hopper versus more traditional H100 applications?\\nJensen Huang: Yeah. Thanks for the question. Grace Hopper is in production -- in high volume production now. We're expecting next year just with all of the design wins that we have in high performance computing and AI infrastructures, we are on a very, very fast ramp with our first data center CPU to a multi-billion dollar product line. This is going to be a very large product line for us. The capability of Grace Hopper is really quite spectacular. It has the ability to create computing nodes that simultaneously has very fast memory, as well as very large memory. In the areas of vector databases or semantic surge, what is called RAG, retrieval augmented generation. So that you could have a generative AI model be able to refer to proprietary data or a factual data before it generates a response, that data is quite large. And you can also have applications or generative models where the context length is very high. You basically store it in entire book into end-to-end system memory before you ask your questions. And so the context length can be quite large this way. The generative models has the ability to still be able to naturally interact with you on one hand. On the other hand, be able to refer to factual data, proprietary data or domain-specific data, you data and be contextually relevant and reduce hallucination. And so that particular use case for example is really quite fantastic for Grace Hopper. It also serves the customers that really care to have a different CPU than x86. Maybe it's a European supercomputing centers or European companies who would like to build up their own ARM ecosystem and like to build up a full stack or CSPs that have decided that they would like to pivot to ARM, because their own custom CPUs are based on ARM. There are variety of different reasons that drives the success of Grace Hopper, but we're off to a just an extraordinary start. This is a home run product.\\nOperator: Your next question comes from the line of Tim Arcuri of UBS. Your line is open.\\nTim Arcuri: Hi. Thanks. I wanted to ask a little bit about the visibility that you have on revenue. I know there's a few moving parts. I guess, on one hand, the purchase commitments went up a lot again. But on the other hand, China bans would arguably pull in when you can fill the demand beyond China. So I know we're not even into 2024 yet and it doesn't sound like, Jensen, you think that next year would be a peak in your Data Center revenue, but I just wanted to sort of explicitly ask you that. Do you think that Data Center can grow even in 2025? Thanks.\\nJensen Huang: Absolutely believe the Data Center can grow through 2025. And there are, of course, several reasons for that. We are expanding our supply quite significantly. We have already one of the broadest and largest and most capable supply chain in the world. Now, remember, people think that the GPU is a chip. But the HGX H100, the Hopper HGX has 35,000 parts, it weighs 70 pounds. Eight of the chips are Hopper. The other 35,000 are not. It is -- even its passive components are incredible. High voltage parts. High frequency parts. High current parts. It is a supercomputer, and therefore, the only way to test a supercomputer is with another supercomputer. Even the manufacturing of it is complicated, the testing of it is complicated, the shipping of it complicated and installation is complicated. And so, every aspect of our HGX supply chain is complicated. And the remarkable team that we have here has really scaled out the supply chain incredibly. Not to mention, all of our HGXs are connected with NVIDIA networking. And the networking, the transceivers, the mix, the cables, the switches, the amount of complexity there is just incredible. And so, I'm just -- first of all, I'm just super proud of the team for scaling up this incredible supply chain. We are absolutely world class. But meanwhile, we're adding new customers and new products. So we have new supply. We have new customers, as I was mentioning earlier. Different regions are standing up GPU specialist clouds, sovereign AI clouds coming out from all over the world, as people realize that they can't afford to export their country's knowledge, their country's culture for somebody else to then resell AI back to them, they have to -- they should, they have the skills and surely with us in combination, we can help them to do that build up their national AI. And so, the first thing that they have to do is, create their AI cloud, national AI cloud. You're also seeing us now growing into enterprise. The enterprise market has two paths. One path -- or if I could say three paths. The first path, of course, just off-the-shelf AI. And there are of course Chat GPT, a fabulous off-the-shelf AI, there'll be others. There's also a proprietary AI, because software companies like ServiceNow and SAP, there are many, many others that can't afford to have their company's intelligence be outsourced to somebody else. And they are about building tools and on top of their tools they should build custom and proprietary and domain-specific copilots and assistants that they can then rent to their customer base. This is -- they're sitting on a goldmine, almost every major tools company in the world is sitting on a goldmine, and they recognize that they have to go build their own custom AIs. We have a new service called an AI foundry, where we leverage NVS (ph) capabilities to be able to serve them in that. And then the next one is enterprises building their own custom AIs, their own custom chatbots, their own custom RAGs. And this capability is spreading all over the world. And the way that we're going to serve that marketplace is with the entire stacks of systems, which includes our compute, our networking and our switches, running our software stack called NVIDIA AI Enterprise, taking it through our market partners, HP, Dell, Lenovo, so on and so forth. And so we're just -- we're seeing the waves of generative AI starting from the start-ups and CSPs, moving to consumer Internet companies, moving to enterprise software platforms, moving to enterprise companies. And then ultimately, one of the areas that you guys have seen us spend a lot of energy on has to do with industrial generative AI. This is where NVIDIA AI and NVIDIA Omniverse comes together and that is a really, really exciting work. And so I think the -- we're at the beginning of a basically across-the-board industrial transition to generative AI to accelerated computing. This is going to affect every company, every industry, every country.\\nOperator: Your next question comes from the line of Toshiya Hari of Goldman Sachs. Your line is open.\\nToshiya Hari: Hi. Thank you. I wanted to clarify something with Colette real quick, and then I had a question for Jensen as well. Colette, you mentioned that you'll be introducing regulation-compliant products over the next couple of months. Yet, the contribution to Q4 revenue should be relatively limited. Is that a timing issue and could it be a source of reacceleration in growth for Data Center in April and beyond or are the price points such that the contribution to revenue going forward should be relatively limited? And then the question for Jensen, the AI foundry service announcement from last week. I just wanted to ask about that, and hopefully, have you expand on it. How is the monetization model going to work? Is it primarily services and software revenue? How should we think about the long term opportunity set? And is this going to be exclusive to Microsoft or do you have plans to expand to other partners as well? Thank you.\\nColette Kress: Thanks, Toshiya. On the question regarding potentially new products that we could provide to our China customers. It's a significant process to both design and develop these new products. As we discussed, we're going to make sure that we are in full discussions with the U.S. government of our intent to move products as well. Given our state about where we are in the quarter, we're already several weeks into the quarter. So it's just going to take some time for us to go through and discussing with our customers the needs and desires of these new products that we have. And moving forward, whether that's medium-term or long-term, it's just hard to say both the [Technical Difficulty] of what we can produce with the U.S. government and what the interest of our China customers in this. So we stay still focused on finding that right balance for our China customers, but it's hard to say at this time.\\nJensen Huang: Toshiya, thanks for the question. There is a glaring opportunity in the world for AI foundry, and it makes so much sense. First, every company has its core intelligence. It makes up our company. Our data, our domain expertise, in the case of many companies, we create tools, and most of the software companies in the world are tool platforms, and those tools are used by people today. And in the future, it's going to be used by people augmented with a whole bunch of AIs that we hire. And these platforms just got to go across the world and you'll see and we've only announced a few; SAP, ServiceNow, Dropbox, Getty, many others are coming. And the reason for that is because they have their own proprietary AI. They want their own proprietary AI. They can't afford to outsource their intelligence and handout their data, and handout their flywheel for other companies to build the AI for them. And so, they come to us. We have several things that are really essential in a foundry. Just as TSMC as a foundry, you have to have AI technology. And as you know, we have just an incredible depth of AI capability -- AI technology capability. And then second, you have to have the best practice known practice, the skills of processing data through the invention of AI models to create AIs that are guardrails, fine-tuned, so on and so forth, that are safe, so on and so forth. And the third thing is you need factories. And that's what DGX Cloud is. Our AI models are called AI Foundations. Our process, if you will, our CAD system for creating AIs are called NeMo and they run on NVIDIA's factories we call DGX Cloud. Our monetization model is that with each one of our partners they rent a sandbox on DGX Cloud, where we work together, they bring their data, they bring their domain expertise, we bring our researchers and engineers, we help them build their custom AI. We help them make that custom AI incredible. Then that custom AI becomes theirs. And they deploy it on the runtime that is enterprise grade, enterprise optimized or outperformance optimized, runs across everything NVIDIA. We have a giant installed base in the cloud, on-prem, anywhere. And it's secure, securely patched, constantly patched and optimized and supported. And we call that NVIDIA AI Enterprise. NVIDIA AI Enterprise is $4,500 per GP per year, that's our business model. Our business model is basically a license. Our customers then with that basic license can build their monetization model on top of. In a lot of ways we're wholesale, they become retail. They could have a per -- they could have subscription license base, they could per instance or they could do per usage, there is a lot of different ways that they could take a -- create their own business model, but ours is basically like a software license, like an operating system. And so our business model is help you create your custom models, you run those custom models on NVIDIA AI Enterprise. And it's off to a great start. NVIDIA AI Enterprise is going to be a very large business for us.\\nOperator: Your next question comes from the line of Stacy Rasgon of Bernstein Research. Your line is open.\\nStacy Rasgon: Hi, guys. Thanks for taking my questions. Colette, I wanted to know if it weren't for the China restrictions would the Q4 guide has been higher or are you supply-constrained in just reshipping stuff that would have gone to China elsewhere? And I guess along those lines you give us a feeling for where your lead times are right now in data center and just the China redirection such as-is, is it lowering those lead times, because you've got parts that are sort of immediately available to ship?\\nColette Kress: Yeah. Stacy, let me see if I can help you understand. Yes, there are still situations where we are working on both improving our supply each and every quarter. We've done a really solid job of ramping every quarter, which has defined our revenue. But with the absence of China for our outlook for Q4, sure, there could have been some things that we are not supply-constrained that we could have sold, but kind of we would no longer can. So could our guidance had been a little higher in our Q4? Yes. We are still working on improving our supply on plan, on continuing growing all throughout next year as well towards that.\\nOperator: Your next question comes from the line of Matt Ramsay of TD Cowen. Your line is open.\\nMatt Ramsay: Thank you very much. Congrats, everybody, on the results. Jensen, I had a two-part question for you, and it comes off of sort of one premise. And the premise is, I still get a lot of questions from investors thinking about AI training as being NVIDIA's dominant domain and somehow inference, even large model inference takes more and more of the TAM that the market will become more competitive. You'll be less differentiated et cetera., et cetera. So I guess the two parts of the question are: number one, maybe you could spend a little bit of time talking about the evolution of the inference workload as we move to LLMs and how your company is positioned for that rather than smaller model inference. And second, up until a month or two ago, I never really got any questions at all about the data processing piece of the AI workloads. So the pieces of manipulating the data before training, between training and inference, after inference and I think that's a large part of the workload now. Maybe you could talk about how CUDA is enabling acceleration of those pieces of the workload. Thanks.\\nJensen Huang: Sure. Inference is complicated. It's actually incredibly complicated. If you -- we this quarter announced one of the most exciting new engines, optimizing compilers called TensorRT-LLM. The reception has been incredible. You got to GitHub, it's been downloaded a ton, a whole lot of stars, integrated into stacks and frameworks all over the world, almost instantaneously. And there are several reasons for that, obviously. We could create TensorRT-LLM, because CUDA is programmable. If CUDA and our GPUs were not so programmable, it would really be hard for us to improve software stacks at the pace that we do. TensorRT-LLM, on the same GPU, without anybody touching anything, improves the performance by a factor of two. And then on top of that, of course, the pace of our innovation is so high. H200 increases it by another factor of two. And so, our inference performance, another way of saying inference cost, just reduced by a factor of four within about a year's time. And so, that's really, really hard to keep up with. The reason why everybody likes our inference engine is because our installed base. We've been dedicated to our installed base for 20 years, 20-plus years. We have an installed base that is not only largest in every single cloud, it's in every available from every enterprise system maker, it's used by companies of just about every industry. And every -- anytime you see a NVIDIA GPU, it runs our stack. It's architecturally compatible, something we've been dedicated to for a very long time. We're very disciplined about it. We make it our, if you will, architecture compatibility is job one. And that has conveyed to the world, the certainty of our platform stability. NVIDIA's platform stability certainty is the reason why everybody builds on us first and the reason why everybody optimizes on us first. All the engineering and all the work that you do, all the invention of technologies that you build on top of NVIDIA accrues to the -- and benefits everybody that uses our GPUs. And we have such a large installed base, large -- millions and millions of GPUs in cloud, 100 million GPUs from people’s PCs just about every workstation in the world, and they all architecturally compatible. And so, if you are an inference platform and you're deploying an inference application, you are basically an application provider. And as a software application provider, you're looking for large installed base. Data processing, before you could train a model, you have to curate the data, you have to dedupe the data, maybe you have to augment the data with synthetic data. So, process the data, clean the data, align the data, normalize the data, all of that data is measured not in bytes or megabytes, it's measured in terabytes and petabytes. And the amount of data processing that you do before data engineering, before that you do training is quite significant. It could represent 30%, 40%, 50% of the amount of work that you ultimately do. And what you -- and ultimately creating a data driven machine learning service. And so data processing is just a massive part. We accelerate Spark, we accelerate Python. One of the coolest things that we just did is called cuDF Pandas. Without one line of code, Pandas, which is the single most successful data science framework in the world. Pandas now is accelerated by NVIDIA CUDA. And just out-of-the box, without the line of code and so the acceleration is really quite terrific and people are just incredibly excited about it. And Pandas was designed for one purpose and one purpose only, really data processing, it's for data science. And so NVIDIA CUDA gives you all of that.\\nOperator: Your final question comes from the line of Harlan Sur of J.P. Morgan. Your line is open.\\nHarlan Sur: Good afternoon. Thanks for taking my question. If you look at the history of the tech industry like those companies that have been successful have always been focused on ecosystem; silicon, hardware, software, strong partnerships and just as importantly, right, an aggressive cadence of new products, more segmentation over time. The team recently announced a more aggressive new product cadence in data center from two years to now every year with higher levels of segmentation, training, optimization in printing CPU, GPU, DPU networking. How do we think about your R&D OpEx growth outlook to support a more aggressive and expanding forward roadmap, but more importantly, what is the team doing to manage and drive execution through all of this complexity?\\nJensen Huang: Gosh. Boy, that's just really excellent. You just wrote NVIDIA's business plan, and so you described our strategy. First of all, there is a fundamental reason why we accelerate our execution. And the reason for that is because it fundamentally drives down cost. When the combination of TensorRT-LLM and H200 reduce the cost for our customers for large model inference by a factor of four, and so that includes, of course, our speeds and feeds, but mostly it's because of our software, mostly the software benefits because of the architecture. And so we want to accelerate our roadmap for that reason. The second reason is to expand the reach of generative AI, the world's number of data center configurations -- this is kind of the amazing thing. NVIDIA is in every cloud, but not one cloud is the same. NVIDIA is working with every single cloud service provider and not one of the networking control plane, security posture is the same. Everybody's platform is different and yet we're integrated into all of their stacks, all of their data centers and we work incredibly well with all of them. And not to mention, we then take the whole thing and we create AI factories that are standalone. We take our platform, we can put them into supercomputers, we can put them into enterprise. Bringing AI to enterprise is something generative AI Enterprise something nobody's ever done before. And we're right now in the process of going to market with all of that. And so the complexity includes, of course, all the technologies and segments and the pace. It includes the fact that we are architecturally compatible across every single one of those. It includes all of the domain specific libraries that we create. The reason why every computer company, without thinking, can integrate NVIDIA into their roadmap and take it to market. And the reason for that is, because there is market demand for it. There is market demand in healthcare, there is market demand in manufacturing, there is market demand, of course, in AI, including financial services, in supercomputing and quantum computing. The list of markets and segments that we have domain specific libraries is incredibly broaden. And then finally, we have an end-to-end solution for data centers; InfiniBand networking, Ethernet networking, x86, ARM, just about every permutation combination of solutions -- technology solutions and software stacks provided. And that translates to having the largest number of ecosystem software developers; the largest ecosystem of system makers; the largest and broadest distribution partnership network; and ultimately, the greatest reach. And that takes -- surely that takes a lot of energy. But the thing that really holds it together, and this is a great decision that we made decades ago, which is everything is architecturally compatible. When we develop a domain specific language that runs on one GPU, it runs on every GPU. When we optimize TensorRT for the cloud, we optimized it for enterprise. When we do something that brings in a new feature, a new library, a new feature or a new developer, they instantly get the benefit of all of our reach. And so that discipline, that architecture compatible discipline that has lasted more than a couple of decades now, is one of the reasons why NVIDIA is still really, really efficient. I mean, we're 28,000 people large and serving just about every single company, every single industry, every single market around the world.\\nOperator: Thank you. I will now turn the call back over to Jensen Huang for closing remarks.\\nJensen Huang: Our strong growth reflects the broad industry platform transition from general purpose to accelerated computing and generative AI. Large language models start-ups consumer Internet companies and global cloud service providers are the first movers. The next waves are starting to build. Nations and regional CSPs are building AI clouds to serve local demand. Enterprise software companies like Adobe and Dropbox, SAP and ServiceNow are adding AI copilots and assistants to their platforms. Enterprises in the world's largest industries are creating custom AIs to automate and boost productivity. The generative AI era is in full steam and has created the need for a new type of data center, an AI factory; optimized for refining data and training, and inference, and generating AI. AI factory workloads are different and incremental to legacy data center workloads supporting IT tasks. AI factories run copilots and AI assistants, which are significant software TAM expansion and are driving significant new investment. Expanding the $1 trillion traditional data center infrastructure installed base, empowering the AI Industrial Revolution. NVIDIA H100 HGX with InfiniBand and the NVIDIA AI software stack define an AI factory today. As we expand our supply chain to meet the world's demand, we are also building new growth drivers for the next wave of AI. We highlighted three elements to our new growth strategy that are hitting their stride: CPU, networking, and software and services. Grace is NVIDIA's first data center CPU. Grace and Grace Hopper are in full production and ramping into a new multi-billion dollar product line next year. Irrespective of the CPU choice, we can help customers build an AI factory. NVIDIA networking now exceeds $10 billion annualized revenue run rate. InfiniBand grew five-fold year-over-year, and is positioned for excellent growth ahead as the networking of AI factories. Enterprises are also racing to adopt AI and Ethernet is the standard networking. This week we announced an Ethernet for AI platform for enterprises. NVIDIA Spectrum-X is an end-to-end solution of Bluefield SuperNIC, Spectrum-4 Ethernet switch and software that boosts Ethernet performance by up to 1.6x for AI workloads. Dell, HPE and Lenovo have joined us to bring a full generative AI solution of NVIDIA AI computing, networking and software to the world's enterprises. NVIDIA software and services is on track to exit the year at an annualized run rate of $1 billion. Enterprise software platforms like ServiceNow and SAP need to build and operate proprietary AI. Enterprises need to build and deploy custom AI copilots. We have the AI technology, expertise and scale to help customers build custom models with their proprietary data on NVIDIA DGX Cloud and deploy the AI applications on enterprise grade NVIDIA AI Enterprise. NVIDIA is essentially an AI foundry. NVIDIA's GPUs, CPUs, networking, AI foundry services and NVIDIA AI Enterprise software are all growth engines in full throttle. Thanks for joining us today. We look forward to updating you on our progress next quarter.\\nOperator: This concludes today's conference call. You may now disconnect.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " \"NVIDIA's latest earnings call revealed record-breaking revenue of $18.1 billion, marking a 34% sequential increase and a 200% year-on-year increase. The company's Data Center segment was a primary driver of this growth, achieving record revenue of $14.5 billion. However, the company anticipates a significant decline in the fourth quarter due to new U.S. export control regulations affecting sales to China and other markets. \\nKey takeaways from the call include:NVIDIA's Data Center segment saw record revenue, driven by the NVIDIA HGX platform and InfiniBand networking. Major companies like Meta (NASDAQ:META) and Adobe (NASDAQ:ADBE) are integrating NVIDIA's technology into their platforms, contributing to the strong demand for AI supercomputers and data center infrastructure.NVIDIA is expanding its Data Center product portfolio to comply with new U.S. export control regulations. Despite expecting a decrease in sales to China and other markets in the fourth quarter, the company is focusing on national investment in sovereign AI infrastructure.The Gaming segment recorded revenue of $2.86 billion, a 15% sequential increase and an 80% year-on-year increase, driven by strong demand for NVIDIA's RTX ray tracing and AI technology.In the Pro Vis segment, revenue reached $416 million, a 10% sequential increase and a 108% year-on-year increase. NVIDIA RTX is the preferred platform for professional design, engineering, and simulation, with AI emerging as a demand driver.NVIDIA's CEO, Jensen Huang, discussed the expansion of AI factories and the transition into a new computing approach. He also expressed confidence in the continued growth of the Data Center business through 2025.During the call, NVIDIA highlighted its product innovations, including the H200 GPU, which offers faster and larger memory for generative AI and language models, and the GH200 Grace Hopper Superchip, a combination of an ARM-based CPU with a Hopper GPU. The Grace Hopper Superchip is being adopted by supercomputing customers, with the combined AI compute capacity of supercomputers built on Grace Hopper expected to exceed 200 exaflops next year. \\nIn addition to its product offerings, NVIDIA is also expanding its networking into the Ethernet space with its new Spectrum-X end-to-end Ethernet offering, which will be available in Q1 next year. This offering can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. \\nNVIDIA also discussed its business strategy, emphasizing the importance of reselling AI technology to customers and its growth in the enterprise market. The company introduced an AI foundry service to help enterprises build custom AI models and announced an aggressive product release schedule in the data center market. \\nLooking ahead, NVIDIA expects total revenue of $20 billion for Q4 2024, driven by strong demand in the Data Center sector. Despite regulatory challenges, the company is confident in its growth potential, citing expanded supply, new customers, and its entrance into the enterprise market.Full transcript -  Nvidia Corp  (NASDAQ:NVDA) Q3 2024:Operator: Good afternoon. My name is JL, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's Third Quarter Earnings Call. All lines have been placed on mute to prevent any background noise. After the speakers’ remarks, there will be a question-and-answer session. [Operator Instructions] Simona Jankowski, you may now begin your conference.\\nSimona Jankowski: Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2024. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter and fiscal 2024. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All statements are made as of today, November 21, 2023, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\\nColette Kress: Thanks, Simona. Q3 was another record quarter. Revenue of $18.1 billion was up 34% sequentially and up more than 200% year-on-year and well above our outlook for $16 billion. Starting with Data Center. The continued ramp of the NVIDIA HGX platform based on our Hopper Tensor Core GPU architecture, along with InfiniBand end-to-end networking, drove record revenue of $14.5 billion, up 41% sequentially and up 279% year-on-year. NVIDIA HGX with InfiniBand together are essentially the reference architecture for AI supercomputers and data center infrastructures. Some of the most exciting generative AI applications are built and run on NVIDIA, including Adobe Firefly, ChatGPT, Microsoft (NASDAQ:MSFT) 365 Copilot, CoAssist, now assist with ServiceNow (NYSE:NOW) and Zoom (NASDAQ:ZM) AI Companion. Our Data Center compute revenue quadrupled from last year and networking revenue nearly tripled. Investments in infrastructure for training and inferencing large language models, deep learning, recommender systems and generative AI applications is fueling strong broad-based demand for NVIDIA accelerated computing. Inferencing is now a major workload for NVIDIA AI computing. Consumer Internet companies and enterprises drove exceptional sequential growth in Q3, comprising approximately half of our Data Center revenue and outpacing total growth. Companies like Meta are in full production with deep learning, recommender systems and also investing in generative AI to help advertisers optimize images and text. Most major consumer Internet companies are racing to ramp up generative AI deployment. The enterprise wave of AI adoption is now beginning. Enterprise software companies such as Adobe, Databricks, Snowflake (NYSE:SNOW) and ServiceNow are adding AI copilots and the systems to their platforms. And broader enterprises are developing custom AI for vertical industry applications such as Tesla (NASDAQ:TSLA) in autonomous driving. Cloud service providers drove roughly the other half of our Data Center revenue in the quarter. Demand was strong from all hyperscale CSPs, as well as from a broadening set of GPU-specialized CSPs globally that are rapidly growing to address the new market opportunities in AI. NVIDIA H100 Tensor Core GPU instances are now generally available in virtually every cloud with instances in high demand. We have significantly increased supply every quarter this year to meet strong demand and expect to continue to do so next year. We will also have a broader and faster product launch cadence to meet the growing and diverse set of AI opportunities. Towards the end of the quarter, the U.S. government announced a new set of export control regulations for China and other markets, including Vietnam and certain countries in the Middle East. These regulations require licenses for the export of a number of our products, including our Hopper and Ampere 100 and 800 series and several others. Our sales to China and other affected destinations derived from products that are now subject to licensing requirements have consistently contributed approximately 20% to 25% of Data Center revenue over the past few quarters. We expect that our sales to these destinations will decline significantly in the fourth quarter. So we believe will be more than offset by strong growth in other regions. The U.S. government designed the regulation to allow the U.S. industry to provide data center compute products to markets worldwide, including China. Continuing to compete worldwide as the regulations encourage, promotes U.S. technology leadership, spurs economic growth and supports U.S. jobs. For the highest performance levels, the government requires licenses. For lower performance levels, the government requires a streamlined prior notification process. And for products even lower performance levels, the government does not require any notice at all. Following the government's clear guidelines, we are working to expand our Data Center product portfolio to offer compliance solutions for each regulatory category, including products for which the U.S. government does not wish to have advance notice before each shipment. We are working with some customers in China and the Middle East to pursue licenses from the U.S. government. It is too early to know whether these will be granted for any significant amount of revenue. Many countries are awakening to the need to invest in sovereign AI infrastructure to support economic growth and industrial innovation. With investments in domestic compute capacity, nations can use their own data to train LLMs and support their local generative AI ecosystems. For example, we are working with India's government and largest tech companies including  Infosys  (NS:INFY), Reliance and Tata to boost their sovereign AI infrastructure. And French private cloud provider, Scaleway, is building a regional AI cloud based on NVIDIA H100 InfiniBand and NVIDIA's AI Enterprise software to fuel advancement across France and Europe. National investment in compute capacity is a new economic imperative and serving the sovereign AI infrastructure market represents a multi-billion dollar opportunity over the next few years. From a product perspective, the vast majority of revenue in Q3 was driven by the NVIDIA HGX platform based on our Hopper GPU architecture with lower contribution from the prior generation Ampere GPU architecture. The new L40S GPU built for industry standard servers began to ship, supporting training and inference workloads across a variety of consumers. This was also the first revenue quarter of our GH200 Grace Hopper Superchip, which combines our ARM-based Grace CPU with a Hopper GPU. Grace and Grace Hopper are ramping into a new multi-billion dollar product line. Grace Hopper instances are now available at GPU specialized cloud providers, and coming soon to Oracle (NYSE:ORCL) Cloud. Grace Hopper is also getting significant traction with supercomputing customers. Initial shipments to Los Alamos National Lab and the Swiss National Supercomputing Center took place in the third quarter. The UK government announced it will build one of the world's fastest AI supercomputers called Isambard-AI with almost 5,500 Grace Hopper Superchips. German supercomputing center, Julich, also announced that it will build its next-generation AI supercomputer with close to 24,000 Grace Hopper Superchips and Quantum-2 InfiniBand, making it the world's most powerful AI supercomputer with over 90 exaflops of AI performance. All-in, we estimate that the combined AI compute capacity of all the supercomputers built on Grace Hopper across the U.S., Europe and Japan next year will exceed 200 exaflops with more wins to come. Inference is contributing significantly to our data center demand, as AI is now in full production for deep learning, recommenders, chatbots, copilots and text to image generation and this is just the beginning. NVIDIA AI offers the best inference performance and versatility, and thus the lower power and cost of ownership. We are also driving a fast cost reduction curve. With the release of TensorRT-LLM, we now achieved more than 2x the inference performance for half the cost of inferencing LLMs on NVIDIA GPUs. We also announced the latest member of the Hopper family, the H200, which will be the first GPU to offer HBM3e, faster, larger memory to further accelerate generative AI and LLMs. It moves inference speed up to another 2x compared to H100 GPUs for running LLMs like Norma2 (ph). Combined, TensorRT-LLM and H200, increased performance or reduced cost by 4x in just one year. With our customers changing their stack, this is a benefit of CUDA and our architecture compatibility. Compared to the A100, H200 delivers an 18x performance increase for inferencing models like GPT-3, allowing customers to move to larger models and with no increase in latency. Amazon (NASDAQ:AMZN) Web Services, Google (NASDAQ:GOOGL) Cloud, Microsoft Azure and Oracle Cloud will be among the first CSPs to offer H200-based instances starting next year. At last week's Microsoft Ignite, we deepened and expanded our collaboration with Microsoft across the entire stock. We introduced an AI foundry service for the development and tuning of custom generative AI enterprise applications running on Azure. Customers can bring their domain knowledge and proprietary data and we help them build their AI models using our AI expertise and software stock in our DGX cloud, all with enterprise grade security and support. SAP and  Amdocs  (NASDAQ:DOX) are the first customers of the NVIDIA AI foundry service on Microsoft Azure. In addition, Microsoft will launch new confidential computing instances based on the H100. The H100 remains the top performing and most versatile platform for AI training and by a wide margin, as shown in the latest MLPerf industry benchmark results. Our training cluster included more than 10,000 H100 GPUs or 3x more than in June, reflecting very efficient scaling. Efficient scaling is a key requirement in generative AI, because LLMs are growing by an order of magnitude every year. Microsoft Azure achieved similar results on a nearly identical cluster, demonstrating the efficiency of NVIDIA AI in public cloud deployments. Networking now exceeds a $10 billion annualized revenue run rate. Strong growth was driven by exceptional demand for InfiniBand, which grew fivefold year-on-year. InfiniBand is critical to gaining the scale and performance needed for training LLMs. Microsoft made this very point last week, highlighting that Azure uses over 29,000 miles of InfiniBand cabling, enough to circle the globe. We are expanding NVIDIA networking into the Ethernet space. Our new Spectrum-X end-to-end Ethernet offering with technologies, purpose built for AI, will be available in Q1 next year. With support from leading OEMs, including Dell (NYSE:DELL), HPE and Lenovo. Spectrum-X can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. Let me also provide an update on our software and services offerings, where we are starting to see excellent adoption. We are on track to exit the year at an annualized revenue run rate of $1 billion for our recurring software, support and services offerings. We see two primary opportunities for growth over the intermediate term with our DGX cloud service and with our NVIDIA AI Enterprise software, each reflects the growth of enterprise AI training and enterprise AI inference, respectively. Our latest DGX cloud customer announcement was this morning as part of an AI research collaboration with Gentech, the biotechnology pioneer also plans to use our BioNeMo LLM framework to help accelerate and optimize their AI drug discovery platform. We now have enterprise AI partnership with Adobe, Dropbox (NASDAQ:DBX), Getty, SAP, ServiceNow, Snowflake and others to come. Okay. Moving to Gaming. Gaming revenue of $2.86 billion was up 15% sequentially and up more than 80% year-on-year with strong demand in the important back-to-school shopping season with NVIDIA RTX ray tracing and AI technology now available at price points as low as $299. We entered the holidays with the best-ever line-up for gamers and creators. Gaming has doubled relative to pre-COVID levels even against the backdrop of lackluster PC market performance. This reflects the significant value we've brought to the gaming ecosystem with innovations like RTX and DLSS. The number of games and applications supporting these technologies has exploded in that period, driving upgrades and attracting new buyers. The RTX ecosystem continues to grow. There are now over 475 RTX-enabled games and applications. Generative AI is quickly emerging as the new pillar app for high performance PCs. NVIDIA RTX GPUs to find the most performance AI PCs and workstations. We just released TensorRT-LLM for Windows, which speeds on-device LLM inference up by 4x. With an installed base of over 100 million, NVIDIA RTX is the natural platform for AI application developers. Finally, our GeForce NOW cloud gaming service continues to build momentum. Its library of PC games surpassed 1,700 titles, including the launches of Alan Wake 2, Baldur's Gate 3, Cyberpunk 2077: Phantom Liberty and Starfield. Moving to the Pro Vis. Revenue of $416 million was up 10% sequentially and up 108% year-on year. NVIDIA RTX is the workstation platform of choice for professional design, engineering and simulation use cases and AI is emerging as a powerful demand driver. Early applications include inference for AI imaging in healthcare and edge AI in smart spaces and the public sector. We launched a new line of desktop workstations based on NVIDIA RTX Ada Lovelace generation GPUs and ConnectX, SmartNICs offering up to 2x the AI processing ray tracing and graphics performance of the previous generations. These powerful new workstations are optimized for AI workloads such as fine tune AI models, training smaller models and running inference locally. We continue to make progress on Omniverse, our software platform for designing, building and operating 3D virtual worlds. Mercedes-Benz (OTC:MBGAF) is using Omniverse powered digital twins to plan, design, build and operate its manufacturing and assembly facilities, helping it increase efficiency and reduce defects. Oxxon (ph) is also incorporating Omniverse into its manufacturing process, including end-to-end simulation for the entire robotics and automation pipeline, saving time and cost. We announced two new Omniverse Cloud services for automotive digitalization available on Microsoft Azure, a virtual factory simulation engine and autonomous vehicle simulation engine. Moving to Automotive. Revenue was $261 million, up 3% sequentially and up 4% year-on year, primarily driven by continued growth in self-driving platforms based on NVIDIA DRIVE Orin SOC and the ramp of AI cockpit solutions with global OEM customers. We extended our automotive partnership of Foxconn to include NVIDIA DRIVE for our next-generation automotive SOC. Foxconn has become the ODM for EVs. Our partnership provides Foxconn with a standard AV sensor and computing platform for their customers to easily build a state-of-an-art safe and secure software defined car. Now we're going to move to the rest of the P&L. GAAP gross margin expanded to 74% and non-GAAP gross margin to 75%, driven by higher Data Center sales and lower net inventory reserve, including a 1 percentage point benefit from the release of previously reserved inventory related to the Ampere GPU architecture products. Sequentially, GAAP operating expenses were up 12% and non-GAAP operating expenses were up 10%, primarily reflecting increased compensation and benefits. Let me turn to the fourth quarter of fiscal 2024. Total revenue is expected to be $20 billion, plus or minus 2%. We expect strong sequential growth to be driven by Data Center, with continued strong demand for both compute and networking. Gaming will likely decline sequentially as it is now more aligned with notebook seasonality. GAAP and non-GAAP gross margins are expected to be 74.5% and 75.5%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $3.17 billion and $2.2 billion, respectively. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $200 million, excluding gains and losses from non-affiliated investments. GAAP and non-GAAP tax rates are expected to be 15%, plus or minus 1% excluding any discrete items. Further financial information are included in the CFO commentary and other information available on our IR website. In closing, let me highlight some upcoming events for the financial community. We will attend the UBS Global Technology Conference in Scottsdale, Arizona, on November 28; the Wells Fargo TMT Summit in Rancho Palos Verdes, California on November 29; the Arete Virtual Tech Conference on December 7; and the J.P. Morgan Health Care Conference in San Francisco on January 8. Our earnings call to discuss the results of our fourth quarter and fiscal 2024 is scheduled for Wednesday, February 21. We will now open the call for questions. Operator, will you please poll for questions.\\nOperator: [Operator Instructions] Your first question comes from the line of Vivek Arya of Bank of America. Your line is open.\\nVivek Arya: Thanks for taking my question. Just, Colette, wanted to clarify what China contributions are you expecting in Q4. And then, Jensen, the main question is for you, where do you think we are in the adoption curve in terms of your shipments into the generative AI market? Because when I just look at the trajectory of your data center, is growth -- it will be close to nearly 30% of all the spending in data center next year. So what metrics are you keeping an eye on to inform you that you can continue to grow? Just where are we in the adoption curve of your products into the generative AI market? Thank you.\\nColette Kress: So, first let me start with your question, Vivek, on export controls and the impacts that we are seeing in our Q4 outlook and guidance that we provided. We had seen historically over the last several quarters that China and some of the other impacted destinations to be about 20% to 25% of our Data Center revenue. We are expecting in our guidance for that to decrease substantially as we move into Q4. The export controls will have a negative effect on our China business. And we do not have good visibility into the magnitude of that impact even over the long-term. We are though working to expand our Data Center product portfolio to possibly offer new regulation compliance solutions that do not require a license, these products, they may become available in the next coming months. However, we don't expect their contribution to be material or meaningful as a percentage of the revenue in Q4.\\nJensen Huang: Generative AI is the largest TAM expansion of software and hardware that we've seen in several decades. At the core of it, what's really exciting is that, what was largely a retrieval based computing approach, almost everything that you do is retrieved off of storage somewhere, has been augmented now, added with a generative method. And it's changed almost everything. You could see that text-to-text, text-to-image, text-to-video, text-to-3D, text-to-protein, text-to-chemicals, these were things that were processed and typed in by humans in the past. And these are now generative approaches. The way that we access data is changed. It used to be based on explicit queries. It is now based on natural language queries, intention queries, semantic queries. And so, we're excited about the work that we're doing with SAP and Dropbox and many others that you're going to hear about. And one of the areas that is really impactful is the software industry, which is about $1 trillion or so, has been building tools that are manually used over the last couple of decades. And now there's a whole new segment of software called copilots and assistants. Instead of manually used, these tools will have copilots to help you use it. And so, instead of licensing software, we will continue to do that, of course, but we will also hire copilots and assistants to help us use these -- use the software. We'll connect all of these copilots and assistants into teams of AIs, which is going to be the modern version of software, modern version of enterprise business software. And so the transformation of software and the way that software has done is driving the hardware underneath. And you can see that it's transforming hardware in two ways. One is something that's largely independent of generative AI. There's two trends: one is related to accelerated computing, general purpose computing is too wasteful of energy and cost. And now that we have much, much better approaches, call it, accelerated computing, you could save an order of magnitude of energy, you can save an order of magnitude of time or you can save an order of magnitudes of cost by using acceleration. And so, accelerated computing is transitioning, if you will, general purpose computing into this new approach. And that's been augmented by a new class of data centers. This is the traditional data centers that you were just talking about where we represent about a third of that. But there is a new class of data centers and this new class of data centers, unlike the data centers of the past, where you have a lot of applications running used by a great many people that are different tenants that are using the same infrastructure and that data center stores a lot of files. These new data centers are very few applications, if not one application, used by basically one tenant and it processes data, it trains models and then generates tokens and generates AI. And we call these new data centers AI factories. We're seeing AI factories being built out everywhere, and just by every country. And so if you look at the way where we are in the expansion, the transition into this new computing approach, the first wave you saw with large language model start-ups, generative AI start-ups and consumer Internet companies, and weren't in the process of ramping that. Meanwhile, while that's being ramped, you see that we're starting to partner with enterprise software companies who would like to build chatbots and copilots and assistants to augment the tools that they have on their platforms. You're seeing GPU specialized CSPs cropping up all over the world and they are dedicated to do really one thing, which is processing AI. You're seeing sovereign AI infrastructures, people -- countries that now recognize that they have to utilize their own data, keep their own data, keep their own culture, process that data and develop their own AI. You see that in India. Several -- about a year ago in Sweden, you are seeing in Japan. Last week, a big announcement in France. But the number of sovereign AI clouds that are being built is really quite significant. And my guess is that almost every major region will have and surely every major country will have their own AI clouds. And so I think you're seeing just new developments as the generative AI wave propagates through every industry, every company, every region. And so we're at the beginning of this inflection, this computing transition.\\nOperator: Your next question comes from the line of Aaron Rakers of Wells Fargo. Your line is open.\\nAaron Rakers: Yeah. Thanks for taking the question. I wanted to ask about kind of the networking side of the business. Given the growth rates that you've now cited, I think, it's 155% year-over-year and strong growth sequentially, it looks like that business is like almost approaching $2.5 billion to $3 billion quarterly level. I'm curious of how you see Ethernet involved evolving and maybe how you would characterize your differentiation of Spectrum-X relative to the traditional Ethernet stack as we start to think about that becoming part of the networking narrative above and maybe beyond just InfiniBand as we look into next year? Thank you.\\nJensen Huang: Yeah. Thanks for the question. Our networking business is already on a $10 billion plus run rate and it's going to get much larger. And as you mentioned, we added a new networking platform to our networking business recently. The vast majority of the dedicated large scale AI factories standardize on InfiniBand. And the reason for that is not only because of its data rate and not only just the latency, but the way that it moves traffic around the network is really important. The way that you process AI and a multi-tenant hyperscale Ethernet environment, the traffic pattern is just radically different. And with InfiniBand and with software defined networks, we could do congestion control, adaptive routing, performance isolation and noise isolation, not to mention, of course, the day rate and the low latency that -- and a very low overhead of InfiniBand that's natural part of InfiniBand. And so, InfiniBand is not so much just the network, it's also a computing fabric. We've put a lot of software-defined capabilities into the fabric including computation. We will do 40-point calculations and computation right on the switch, and right in the fabric itself. And so that's the reason why that difference in Ethernet versus InfiniBand or InfiniBand versus Ethernet for AI factories is so dramatic. And the difference is profound. And the reason for that is because you've just invested in a $2 billion infrastructure for AI factories. A 20%, 25%, 30% difference in overall effectiveness, especially as you scale up is measured in hundreds of millions of dollars of value. And if you will, renting that infrastructure over the course of four to five years, it really, really adds up. And so InfiniBand's value proposition is undeniable for AI factories. However, as we move AI into enterprise. This is enterprise computing what we'd like to enable every company to be able to build their own custom AIs. We're building customer AIs in our company based on our proprietary data, our proprietary type of skills. For example, recently we spoke about one of the models that we're creating, it's called ChipNeMo; we're building many others. There'll be tens, hundreds of custom AI models that we create inside our company. And our company is -- for all of our employee use, doesn't have to be as high performance as the AI factories we used to train the models. And so we would like the AI to be able to run in Ethernet environment. And so what we've done is we invented this new platform that extends Ethernet; doesn't replace Ethernet, it's 100% compliant with Ethernet. And it's optimized for East-West traffic, which is where the computing fabric is. It adds to Ethernet with an end-to-end solution with Bluefield, as well as our Spectrum switch that allows us to perform some of the capabilities that we have in InfiniBand, not all but some. And we achieved excellent results. And the way we go to market is we go to market with our large enterprise partners who already offer our computing solution. And so, HP (NYSE:HPQ), Dell and Lenovo has the NVIDIA AI stack, the NVIDIA AI Enterprise software stack and now they integrate with Bluefield, as well as bundle -- take a market there, Spectrum switch, and they'll be able to offer enterprise customers all over the world with their vast sales force and vast network of resellers a fully integrated, if you will, fully optimized, at least end-to-end AI solution. And so that's basically it, bringing AI to Ethernet for the world's enterprise.\\nOperator: Thank you. Your next question comes from the line of Joe Moore of Morgan Stanley. Your line is open.\\nJoseph Moore: Great. Thank you. I'm wondering if you could talk a little bit more about Grace Hopper and how you see the ability to leverage kind of the microprocessor, how you see that as a TAM expander. And what applications do you see using Grace Hopper versus more traditional H100 applications?\\nJensen Huang: Yeah. Thanks for the question. Grace Hopper is in production -- in high volume production now. We're expecting next year just with all of the design wins that we have in high performance computing and AI infrastructures, we are on a very, very fast ramp with our first data center CPU to a multi-billion dollar product line. This is going to be a very large product line for us. The capability of Grace Hopper is really quite spectacular. It has the ability to create computing nodes that simultaneously has very fast memory, as well as very large memory. In the areas of vector databases or semantic surge, what is called RAG, retrieval augmented generation. So that you could have a generative AI model be able to refer to proprietary data or a factual data before it generates a response, that data is quite large. And you can also have applications or generative models where the context length is very high. You basically store it in entire book into end-to-end system memory before you ask your questions. And so the context length can be quite large this way. The generative models has the ability to still be able to naturally interact with you on one hand. On the other hand, be able to refer to factual data, proprietary data or domain-specific data, you data and be contextually relevant and reduce hallucination. And so that particular use case for example is really quite fantastic for Grace Hopper. It also serves the customers that really care to have a different CPU than x86. Maybe it's a European supercomputing centers or European companies who would like to build up their own ARM ecosystem and like to build up a full stack or CSPs that have decided that they would like to pivot to ARM, because their own custom CPUs are based on ARM. There are variety of different reasons that drives the success of Grace Hopper, but we're off to a just an extraordinary start. This is a home run product.\\nOperator: Your next question comes from the line of Tim Arcuri of UBS. Your line is open.\\nTim Arcuri: Hi. Thanks. I wanted to ask a little bit about the visibility that you have on revenue. I know there's a few moving parts. I guess, on one hand, the purchase commitments went up a lot again. But on the other hand, China bans would arguably pull in when you can fill the demand beyond China. So I know we're not even into 2024 yet and it doesn't sound like, Jensen, you think that next year would be a peak in your Data Center revenue, but I just wanted to sort of explicitly ask you that. Do you think that Data Center can grow even in 2025? Thanks.\\nJensen Huang: Absolutely believe the Data Center can grow through 2025. And there are, of course, several reasons for that. We are expanding our supply quite significantly. We have already one of the broadest and largest and most capable supply chain in the world. Now, remember, people think that the GPU is a chip. But the HGX H100, the Hopper HGX has 35,000 parts, it weighs 70 pounds. Eight of the chips are Hopper. The other 35,000 are not. It is -- even its passive components are incredible. High voltage parts. High frequency parts. High current parts. It is a supercomputer, and therefore, the only way to test a supercomputer is with another supercomputer. Even the manufacturing of it is complicated, the testing of it is complicated, the shipping of it complicated and installation is complicated. And so, every aspect of our HGX supply chain is complicated. And the remarkable team that we have here has really scaled out the supply chain incredibly. Not to mention, all of our HGXs are connected with NVIDIA networking. And the networking, the transceivers, the mix, the cables, the switches, the amount of complexity there is just incredible. And so, I'm just -- first of all, I'm just super proud of the team for scaling up this incredible supply chain. We are absolutely world class. But meanwhile, we're adding new customers and new products. So we have new supply. We have new customers, as I was mentioning earlier. Different regions are standing up GPU specialist clouds, sovereign AI clouds coming out from all over the world, as people realize that they can't afford to export their country's knowledge, their country's culture for somebody else to then resell AI back to them, they have to -- they should, they have the skills and surely with us in combination, we can help them to do that build up their national AI. And so, the first thing that they have to do is, create their AI cloud, national AI cloud. You're also seeing us now growing into enterprise. The enterprise market has two paths. One path -- or if I could say three paths. The first path, of course, just off-the-shelf AI. And there are of course Chat GPT, a fabulous off-the-shelf AI, there'll be others. There's also a proprietary AI, because software companies like ServiceNow and SAP, there are many, many others that can't afford to have their company's intelligence be outsourced to somebody else. And they are about building tools and on top of their tools they should build custom and proprietary and domain-specific copilots and assistants that they can then rent to their customer base. This is -- they're sitting on a goldmine, almost every major tools company in the world is sitting on a goldmine, and they recognize that they have to go build their own custom AIs. We have a new service called an AI foundry, where we leverage NVS (ph) capabilities to be able to serve them in that. And then the next one is enterprises building their own custom AIs, their own custom chatbots, their own custom RAGs. And this capability is spreading all over the world. And the way that we're going to serve that marketplace is with the entire stacks of systems, which includes our compute, our networking and our switches, running our software stack called NVIDIA AI Enterprise, taking it through our market partners, HP, Dell, Lenovo, so on and so forth. And so we're just -- we're seeing the waves of generative AI starting from the start-ups and CSPs, moving to consumer Internet companies, moving to enterprise software platforms, moving to enterprise companies. And then ultimately, one of the areas that you guys have seen us spend a lot of energy on has to do with industrial generative AI. This is where NVIDIA AI and NVIDIA Omniverse comes together and that is a really, really exciting work. And so I think the -- we're at the beginning of a basically across-the-board industrial transition to generative AI to accelerated computing. This is going to affect every company, every industry, every country.\\nOperator: Your next question comes from the line of Toshiya Hari of Goldman Sachs. Your line is open.\\nToshiya Hari: Hi. Thank you. I wanted to clarify something with Colette real quick, and then I had a question for Jensen as well. Colette, you mentioned that you'll be introducing regulation-compliant products over the next couple of months. Yet, the contribution to Q4 revenue should be relatively limited. Is that a timing issue and could it be a source of reacceleration in growth for Data Center in April and beyond or are the price points such that the contribution to revenue going forward should be relatively limited? And then the question for Jensen, the AI foundry service announcement from last week. I just wanted to ask about that, and hopefully, have you expand on it. How is the monetization model going to work? Is it primarily services and software revenue? How should we think about the long term opportunity set? And is this going to be exclusive to Microsoft or do you have plans to expand to other partners as well? Thank you.\\nColette Kress: Thanks, Toshiya. On the question regarding potentially new products that we could provide to our China customers. It's a significant process to both design and develop these new products. As we discussed, we're going to make sure that we are in full discussions with the U.S. government of our intent to move products as well. Given our state about where we are in the quarter, we're already several weeks into the quarter. So it's just going to take some time for us to go through and discussing with our customers the needs and desires of these new products that we have. And moving forward, whether that's medium-term or long-term, it's just hard to say both the [Technical Difficulty] of what we can produce with the U.S. government and what the interest of our China customers in this. So we stay still focused on finding that right balance for our China customers, but it's hard to say at this time.\\nJensen Huang: Toshiya, thanks for the question. There is a glaring opportunity in the world for AI foundry, and it makes so much sense. First, every company has its core intelligence. It makes up our company. Our data, our domain expertise, in the case of many companies, we create tools, and most of the software companies in the world are tool platforms, and those tools are used by people today. And in the future, it's going to be used by people augmented with a whole bunch of AIs that we hire. And these platforms just got to go across the world and you'll see and we've only announced a few; SAP, ServiceNow, Dropbox, Getty, many others are coming. And the reason for that is because they have their own proprietary AI. They want their own proprietary AI. They can't afford to outsource their intelligence and handout their data, and handout their flywheel for other companies to build the AI for them. And so, they come to us. We have several things that are really essential in a foundry. Just as TSMC as a foundry, you have to have AI technology. And as you know, we have just an incredible depth of AI capability -- AI technology capability. And then second, you have to have the best practice known practice, the skills of processing data through the invention of AI models to create AIs that are guardrails, fine-tuned, so on and so forth, that are safe, so on and so forth. And the third thing is you need factories. And that's what DGX Cloud is. Our AI models are called AI Foundations. Our process, if you will, our CAD system for creating AIs are called NeMo and they run on NVIDIA's factories we call DGX Cloud. Our monetization model is that with each one of our partners they rent a sandbox on DGX Cloud, where we work together, they bring their data, they bring their domain expertise, we bring our researchers and engineers, we help them build their custom AI. We help them make that custom AI incredible. Then that custom AI becomes theirs. And they deploy it on the runtime that is enterprise grade, enterprise optimized or outperformance optimized, runs across everything NVIDIA. We have a giant installed base in the cloud, on-prem, anywhere. And it's secure, securely patched, constantly patched and optimized and supported. And we call that NVIDIA AI Enterprise. NVIDIA AI Enterprise is $4,500 per GP per year, that's our business model. Our business model is basically a license. Our customers then with that basic license can build their monetization model on top of. In a lot of ways we're wholesale, they become retail. They could have a per -- they could have subscription license base, they could per instance or they could do per usage, there is a lot of different ways that they could take a -- create their own business model, but ours is basically like a software license, like an operating system. And so our business model is help you create your custom models, you run those custom models on NVIDIA AI Enterprise. And it's off to a great start. NVIDIA AI Enterprise is going to be a very large business for us.\\nOperator: Your next question comes from the line of Stacy Rasgon of Bernstein Research. Your line is open.\\nStacy Rasgon: Hi, guys. Thanks for taking my questions. Colette, I wanted to know if it weren't for the China restrictions would the Q4 guide has been higher or are you supply-constrained in just reshipping stuff that would have gone to China elsewhere? And I guess along those lines you give us a feeling for where your lead times are right now in data center and just the China redirection such as-is, is it lowering those lead times, because you've got parts that are sort of immediately available to ship?\\nColette Kress: Yeah. Stacy, let me see if I can help you understand. Yes, there are still situations where we are working on both improving our supply each and every quarter. We've done a really solid job of ramping every quarter, which has defined our revenue. But with the absence of China for our outlook for Q4, sure, there could have been some things that we are not supply-constrained that we could have sold, but kind of we would no longer can. So could our guidance had been a little higher in our Q4? Yes. We are still working on improving our supply on plan, on continuing growing all throughout next year as well towards that.\\nOperator: Your next question comes from the line of Matt Ramsay of TD Cowen. Your line is open.\\nMatt Ramsay: Thank you very much. Congrats, everybody, on the results. Jensen, I had a two-part question for you, and it comes off of sort of one premise. And the premise is, I still get a lot of questions from investors thinking about AI training as being NVIDIA's dominant domain and somehow inference, even large model inference takes more and more of the TAM that the market will become more competitive. You'll be less differentiated et cetera., et cetera. So I guess the two parts of the question are: number one, maybe you could spend a little bit of time talking about the evolution of the inference workload as we move to LLMs and how your company is positioned for that rather than smaller model inference. And second, up until a month or two ago, I never really got any questions at all about the data processing piece of the AI workloads. So the pieces of manipulating the data before training, between training and inference, after inference and I think that's a large part of the workload now. Maybe you could talk about how CUDA is enabling acceleration of those pieces of the workload. Thanks.\\nJensen Huang: Sure. Inference is complicated. It's actually incredibly complicated. If you -- we this quarter announced one of the most exciting new engines, optimizing compilers called TensorRT-LLM. The reception has been incredible. You got to GitHub, it's been downloaded a ton, a whole lot of stars, integrated into stacks and frameworks all over the world, almost instantaneously. And there are several reasons for that, obviously. We could create TensorRT-LLM, because CUDA is programmable. If CUDA and our GPUs were not so programmable, it would really be hard for us to improve software stacks at the pace that we do. TensorRT-LLM, on the same GPU, without anybody touching anything, improves the performance by a factor of two. And then on top of that, of course, the pace of our innovation is so high. H200 increases it by another factor of two. And so, our inference performance, another way of saying inference cost, just reduced by a factor of four within about a year's time. And so, that's really, really hard to keep up with. The reason why everybody likes our inference engine is because our installed base. We've been dedicated to our installed base for 20 years, 20-plus years. We have an installed base that is not only largest in every single cloud, it's in every available from every enterprise system maker, it's used by companies of just about every industry. And every -- anytime you see a NVIDIA GPU, it runs our stack. It's architecturally compatible, something we've been dedicated to for a very long time. We're very disciplined about it. We make it our, if you will, architecture compatibility is job one. And that has conveyed to the world, the certainty of our platform stability. NVIDIA's platform stability certainty is the reason why everybody builds on us first and the reason why everybody optimizes on us first. All the engineering and all the work that you do, all the invention of technologies that you build on top of NVIDIA accrues to the -- and benefits everybody that uses our GPUs. And we have such a large installed base, large -- millions and millions of GPUs in cloud, 100 million GPUs from people’s PCs just about every workstation in the world, and they all architecturally compatible. And so, if you are an inference platform and you're deploying an inference application, you are basically an application provider. And as a software application provider, you're looking for large installed base. Data processing, before you could train a model, you have to curate the data, you have to dedupe the data, maybe you have to augment the data with synthetic data. So, process the data, clean the data, align the data, normalize the data, all of that data is measured not in bytes or megabytes, it's measured in terabytes and petabytes. And the amount of data processing that you do before data engineering, before that you do training is quite significant. It could represent 30%, 40%, 50% of the amount of work that you ultimately do. And what you -- and ultimately creating a data driven machine learning service. And so data processing is just a massive part. We accelerate Spark, we accelerate Python. One of the coolest things that we just did is called cuDF Pandas. Without one line of code, Pandas, which is the single most successful data science framework in the world. Pandas now is accelerated by NVIDIA CUDA. And just out-of-the box, without the line of code and so the acceleration is really quite terrific and people are just incredibly excited about it. And Pandas was designed for one purpose and one purpose only, really data processing, it's for data science. And so NVIDIA CUDA gives you all of that.\\nOperator: Your final question comes from the line of Harlan Sur of J.P. Morgan. Your line is open.\\nHarlan Sur: Good afternoon. Thanks for taking my question. If you look at the history of the tech industry like those companies that have been successful have always been focused on ecosystem; silicon, hardware, software, strong partnerships and just as importantly, right, an aggressive cadence of new products, more segmentation over time. The team recently announced a more aggressive new product cadence in data center from two years to now every year with higher levels of segmentation, training, optimization in printing CPU, GPU, DPU networking. How do we think about your R&D OpEx growth outlook to support a more aggressive and expanding forward roadmap, but more importantly, what is the team doing to manage and drive execution through all of this complexity?\\nJensen Huang: Gosh. Boy, that's just really excellent. You just wrote NVIDIA's business plan, and so you described our strategy. First of all, there is a fundamental reason why we accelerate our execution. And the reason for that is because it fundamentally drives down cost. When the combination of TensorRT-LLM and H200 reduce the cost for our customers for large model inference by a factor of four, and so that includes, of course, our speeds and feeds, but mostly it's because of our software, mostly the software benefits because of the architecture. And so we want to accelerate our roadmap for that reason. The second reason is to expand the reach of generative AI, the world's number of data center configurations -- this is kind of the amazing thing. NVIDIA is in every cloud, but not one cloud is the same. NVIDIA is working with every single cloud service provider and not one of the networking control plane, security posture is the same. Everybody's platform is different and yet we're integrated into all of their stacks, all of their data centers and we work incredibly well with all of them. And not to mention, we then take the whole thing and we create AI factories that are standalone. We take our platform, we can put them into supercomputers, we can put them into enterprise. Bringing AI to enterprise is something generative AI Enterprise something nobody's ever done before. And we're right now in the process of going to market with all of that. And so the complexity includes, of course, all the technologies and segments and the pace. It includes the fact that we are architecturally compatible across every single one of those. It includes all of the domain specific libraries that we create. The reason why every computer company, without thinking, can integrate NVIDIA into their roadmap and take it to market. And the reason for that is, because there is market demand for it. There is market demand in healthcare, there is market demand in manufacturing, there is market demand, of course, in AI, including financial services, in supercomputing and quantum computing. The list of markets and segments that we have domain specific libraries is incredibly broaden. And then finally, we have an end-to-end solution for data centers; InfiniBand networking, Ethernet networking, x86, ARM, just about every permutation combination of solutions -- technology solutions and software stacks provided. And that translates to having the largest number of ecosystem software developers; the largest ecosystem of system makers; the largest and broadest distribution partnership network; and ultimately, the greatest reach. And that takes -- surely that takes a lot of energy. But the thing that really holds it together, and this is a great decision that we made decades ago, which is everything is architecturally compatible. When we develop a domain specific language that runs on one GPU, it runs on every GPU. When we optimize TensorRT for the cloud, we optimized it for enterprise. When we do something that brings in a new feature, a new library, a new feature or a new developer, they instantly get the benefit of all of our reach. And so that discipline, that architecture compatible discipline that has lasted more than a couple of decades now, is one of the reasons why NVIDIA is still really, really efficient. I mean, we're 28,000 people large and serving just about every single company, every single industry, every single market around the world.\\nOperator: Thank you. I will now turn the call back over to Jensen Huang for closing remarks.\\nJensen Huang: Our strong growth reflects the broad industry platform transition from general purpose to accelerated computing and generative AI. Large language models start-ups consumer Internet companies and global cloud service providers are the first movers. The next waves are starting to build. Nations and regional CSPs are building AI clouds to serve local demand. Enterprise software companies like Adobe and Dropbox, SAP and ServiceNow are adding AI copilots and assistants to their platforms. Enterprises in the world's largest industries are creating custom AIs to automate and boost productivity. The generative AI era is in full steam and has created the need for a new type of data center, an AI factory; optimized for refining data and training, and inference, and generating AI. AI factory workloads are different and incremental to legacy data center workloads supporting IT tasks. AI factories run copilots and AI assistants, which are significant software TAM expansion and are driving significant new investment. Expanding the $1 trillion traditional data center infrastructure installed base, empowering the AI Industrial Revolution. NVIDIA H100 HGX with InfiniBand and the NVIDIA AI software stack define an AI factory today. As we expand our supply chain to meet the world's demand, we are also building new growth drivers for the next wave of AI. We highlighted three elements to our new growth strategy that are hitting their stride: CPU, networking, and software and services. Grace is NVIDIA's first data center CPU. Grace and Grace Hopper are in full production and ramping into a new multi-billion dollar product line next year. Irrespective of the CPU choice, we can help customers build an AI factory. NVIDIA networking now exceeds $10 billion annualized revenue run rate. InfiniBand grew five-fold year-over-year, and is positioned for excellent growth ahead as the networking of AI factories. Enterprises are also racing to adopt AI and Ethernet is the standard networking. This week we announced an Ethernet for AI platform for enterprises. NVIDIA Spectrum-X is an end-to-end solution of Bluefield SuperNIC, Spectrum-4 Ethernet switch and software that boosts Ethernet performance by up to 1.6x for AI workloads. Dell, HPE and Lenovo have joined us to bring a full generative AI solution of NVIDIA AI computing, networking and software to the world's enterprises. NVIDIA software and services is on track to exit the year at an annualized run rate of $1 billion. Enterprise software platforms like ServiceNow and SAP need to build and operate proprietary AI. Enterprises need to build and deploy custom AI copilots. We have the AI technology, expertise and scale to help customers build custom models with their proprietary data on NVIDIA DGX Cloud and deploy the AI applications on enterprise grade NVIDIA AI Enterprise. NVIDIA is essentially an AI foundry. NVIDIA's GPUs, CPUs, networking, AI foundry services and NVIDIA AI Enterprise software are all growth engines in full throttle. Thanks for joining us today. We look forward to updating you on our progress next quarter.\\nOperator: This concludes today's conference call. You may now disconnect.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " \"NVIDIA's latest earnings call revealed record-breaking revenue of $18.1 billion, marking a 34% sequential increase and a 200% year-on-year increase. The company's Data Center segment was a primary driver of this growth, achieving record revenue of $14.5 billion. However, the company anticipates a significant decline in the fourth quarter due to new U.S. export control regulations affecting sales to China and other markets. \\nKey takeaways from the call include:NVIDIA's Data Center segment saw record revenue, driven by the NVIDIA HGX platform and InfiniBand networking. Major companies like Meta (NASDAQ:META) and Adobe (NASDAQ:ADBE) are integrating NVIDIA's technology into their platforms, contributing to the strong demand for AI supercomputers and data center infrastructure.NVIDIA is expanding its Data Center product portfolio to comply with new U.S. export control regulations. Despite expecting a decrease in sales to China and other markets in the fourth quarter, the company is focusing on national investment in sovereign AI infrastructure.The Gaming segment recorded revenue of $2.86 billion, a 15% sequential increase and an 80% year-on-year increase, driven by strong demand for NVIDIA's RTX ray tracing and AI technology.In the Pro Vis segment, revenue reached $416 million, a 10% sequential increase and a 108% year-on-year increase. NVIDIA RTX is the preferred platform for professional design, engineering, and simulation, with AI emerging as a demand driver.NVIDIA's CEO, Jensen Huang, discussed the expansion of AI factories and the transition into a new computing approach. He also expressed confidence in the continued growth of the Data Center business through 2025.During the call, NVIDIA highlighted its product innovations, including the H200 GPU, which offers faster and larger memory for generative AI and language models, and the GH200 Grace Hopper Superchip, a combination of an ARM-based CPU with a Hopper GPU. The Grace Hopper Superchip is being adopted by supercomputing customers, with the combined AI compute capacity of supercomputers built on Grace Hopper expected to exceed 200 exaflops next year. \\nIn addition to its product offerings, NVIDIA is also expanding its networking into the Ethernet space with its new Spectrum-X end-to-end Ethernet offering, which will be available in Q1 next year. This offering can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. \\nNVIDIA also discussed its business strategy, emphasizing the importance of reselling AI technology to customers and its growth in the enterprise market. The company introduced an AI foundry service to help enterprises build custom AI models and announced an aggressive product release schedule in the data center market. \\nLooking ahead, NVIDIA expects total revenue of $20 billion for Q4 2024, driven by strong demand in the Data Center sector. Despite regulatory challenges, the company is confident in its growth potential, citing expanded supply, new customers, and its entrance into the enterprise market.Full transcript -  Nvidia Corp  (NASDAQ:NVDA) Q3 2024:Operator: Good afternoon. My name is JL, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's Third Quarter Earnings Call. All lines have been placed on mute to prevent any background noise. After the speakers’ remarks, there will be a question-and-answer session. [Operator Instructions] Simona Jankowski, you may now begin your conference.\\nSimona Jankowski: Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2024. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter and fiscal 2024. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All statements are made as of today, November 21, 2023, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\\nColette Kress: Thanks, Simona. Q3 was another record quarter. Revenue of $18.1 billion was up 34% sequentially and up more than 200% year-on-year and well above our outlook for $16 billion. Starting with Data Center. The continued ramp of the NVIDIA HGX platform based on our Hopper Tensor Core GPU architecture, along with InfiniBand end-to-end networking, drove record revenue of $14.5 billion, up 41% sequentially and up 279% year-on-year. NVIDIA HGX with InfiniBand together are essentially the reference architecture for AI supercomputers and data center infrastructures. Some of the most exciting generative AI applications are built and run on NVIDIA, including Adobe Firefly, ChatGPT, Microsoft (NASDAQ:MSFT) 365 Copilot, CoAssist, now assist with ServiceNow (NYSE:NOW) and Zoom (NASDAQ:ZM) AI Companion. Our Data Center compute revenue quadrupled from last year and networking revenue nearly tripled. Investments in infrastructure for training and inferencing large language models, deep learning, recommender systems and generative AI applications is fueling strong broad-based demand for NVIDIA accelerated computing. Inferencing is now a major workload for NVIDIA AI computing. Consumer Internet companies and enterprises drove exceptional sequential growth in Q3, comprising approximately half of our Data Center revenue and outpacing total growth. Companies like Meta are in full production with deep learning, recommender systems and also investing in generative AI to help advertisers optimize images and text. Most major consumer Internet companies are racing to ramp up generative AI deployment. The enterprise wave of AI adoption is now beginning. Enterprise software companies such as Adobe, Databricks, Snowflake (NYSE:SNOW) and ServiceNow are adding AI copilots and the systems to their platforms. And broader enterprises are developing custom AI for vertical industry applications such as Tesla (NASDAQ:TSLA) in autonomous driving. Cloud service providers drove roughly the other half of our Data Center revenue in the quarter. Demand was strong from all hyperscale CSPs, as well as from a broadening set of GPU-specialized CSPs globally that are rapidly growing to address the new market opportunities in AI. NVIDIA H100 Tensor Core GPU instances are now generally available in virtually every cloud with instances in high demand. We have significantly increased supply every quarter this year to meet strong demand and expect to continue to do so next year. We will also have a broader and faster product launch cadence to meet the growing and diverse set of AI opportunities. Towards the end of the quarter, the U.S. government announced a new set of export control regulations for China and other markets, including Vietnam and certain countries in the Middle East. These regulations require licenses for the export of a number of our products, including our Hopper and Ampere 100 and 800 series and several others. Our sales to China and other affected destinations derived from products that are now subject to licensing requirements have consistently contributed approximately 20% to 25% of Data Center revenue over the past few quarters. We expect that our sales to these destinations will decline significantly in the fourth quarter. So we believe will be more than offset by strong growth in other regions. The U.S. government designed the regulation to allow the U.S. industry to provide data center compute products to markets worldwide, including China. Continuing to compete worldwide as the regulations encourage, promotes U.S. technology leadership, spurs economic growth and supports U.S. jobs. For the highest performance levels, the government requires licenses. For lower performance levels, the government requires a streamlined prior notification process. And for products even lower performance levels, the government does not require any notice at all. Following the government's clear guidelines, we are working to expand our Data Center product portfolio to offer compliance solutions for each regulatory category, including products for which the U.S. government does not wish to have advance notice before each shipment. We are working with some customers in China and the Middle East to pursue licenses from the U.S. government. It is too early to know whether these will be granted for any significant amount of revenue. Many countries are awakening to the need to invest in sovereign AI infrastructure to support economic growth and industrial innovation. With investments in domestic compute capacity, nations can use their own data to train LLMs and support their local generative AI ecosystems. For example, we are working with India's government and largest tech companies including  Infosys  (NS:INFY), Reliance and Tata to boost their sovereign AI infrastructure. And French private cloud provider, Scaleway, is building a regional AI cloud based on NVIDIA H100 InfiniBand and NVIDIA's AI Enterprise software to fuel advancement across France and Europe. National investment in compute capacity is a new economic imperative and serving the sovereign AI infrastructure market represents a multi-billion dollar opportunity over the next few years. From a product perspective, the vast majority of revenue in Q3 was driven by the NVIDIA HGX platform based on our Hopper GPU architecture with lower contribution from the prior generation Ampere GPU architecture. The new L40S GPU built for industry standard servers began to ship, supporting training and inference workloads across a variety of consumers. This was also the first revenue quarter of our GH200 Grace Hopper Superchip, which combines our ARM-based Grace CPU with a Hopper GPU. Grace and Grace Hopper are ramping into a new multi-billion dollar product line. Grace Hopper instances are now available at GPU specialized cloud providers, and coming soon to Oracle (NYSE:ORCL) Cloud. Grace Hopper is also getting significant traction with supercomputing customers. Initial shipments to Los Alamos National Lab and the Swiss National Supercomputing Center took place in the third quarter. The UK government announced it will build one of the world's fastest AI supercomputers called Isambard-AI with almost 5,500 Grace Hopper Superchips. German supercomputing center, Julich, also announced that it will build its next-generation AI supercomputer with close to 24,000 Grace Hopper Superchips and Quantum-2 InfiniBand, making it the world's most powerful AI supercomputer with over 90 exaflops of AI performance. All-in, we estimate that the combined AI compute capacity of all the supercomputers built on Grace Hopper across the U.S., Europe and Japan next year will exceed 200 exaflops with more wins to come. Inference is contributing significantly to our data center demand, as AI is now in full production for deep learning, recommenders, chatbots, copilots and text to image generation and this is just the beginning. NVIDIA AI offers the best inference performance and versatility, and thus the lower power and cost of ownership. We are also driving a fast cost reduction curve. With the release of TensorRT-LLM, we now achieved more than 2x the inference performance for half the cost of inferencing LLMs on NVIDIA GPUs. We also announced the latest member of the Hopper family, the H200, which will be the first GPU to offer HBM3e, faster, larger memory to further accelerate generative AI and LLMs. It moves inference speed up to another 2x compared to H100 GPUs for running LLMs like Norma2 (ph). Combined, TensorRT-LLM and H200, increased performance or reduced cost by 4x in just one year. With our customers changing their stack, this is a benefit of CUDA and our architecture compatibility. Compared to the A100, H200 delivers an 18x performance increase for inferencing models like GPT-3, allowing customers to move to larger models and with no increase in latency. Amazon (NASDAQ:AMZN) Web Services, Google (NASDAQ:GOOGL) Cloud, Microsoft Azure and Oracle Cloud will be among the first CSPs to offer H200-based instances starting next year. At last week's Microsoft Ignite, we deepened and expanded our collaboration with Microsoft across the entire stock. We introduced an AI foundry service for the development and tuning of custom generative AI enterprise applications running on Azure. Customers can bring their domain knowledge and proprietary data and we help them build their AI models using our AI expertise and software stock in our DGX cloud, all with enterprise grade security and support. SAP and  Amdocs  (NASDAQ:DOX) are the first customers of the NVIDIA AI foundry service on Microsoft Azure. In addition, Microsoft will launch new confidential computing instances based on the H100. The H100 remains the top performing and most versatile platform for AI training and by a wide margin, as shown in the latest MLPerf industry benchmark results. Our training cluster included more than 10,000 H100 GPUs or 3x more than in June, reflecting very efficient scaling. Efficient scaling is a key requirement in generative AI, because LLMs are growing by an order of magnitude every year. Microsoft Azure achieved similar results on a nearly identical cluster, demonstrating the efficiency of NVIDIA AI in public cloud deployments. Networking now exceeds a $10 billion annualized revenue run rate. Strong growth was driven by exceptional demand for InfiniBand, which grew fivefold year-on-year. InfiniBand is critical to gaining the scale and performance needed for training LLMs. Microsoft made this very point last week, highlighting that Azure uses over 29,000 miles of InfiniBand cabling, enough to circle the globe. We are expanding NVIDIA networking into the Ethernet space. Our new Spectrum-X end-to-end Ethernet offering with technologies, purpose built for AI, will be available in Q1 next year. With support from leading OEMs, including Dell (NYSE:DELL), HPE and Lenovo. Spectrum-X can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. Let me also provide an update on our software and services offerings, where we are starting to see excellent adoption. We are on track to exit the year at an annualized revenue run rate of $1 billion for our recurring software, support and services offerings. We see two primary opportunities for growth over the intermediate term with our DGX cloud service and with our NVIDIA AI Enterprise software, each reflects the growth of enterprise AI training and enterprise AI inference, respectively. Our latest DGX cloud customer announcement was this morning as part of an AI research collaboration with Gentech, the biotechnology pioneer also plans to use our BioNeMo LLM framework to help accelerate and optimize their AI drug discovery platform. We now have enterprise AI partnership with Adobe, Dropbox (NASDAQ:DBX), Getty, SAP, ServiceNow, Snowflake and others to come. Okay. Moving to Gaming. Gaming revenue of $2.86 billion was up 15% sequentially and up more than 80% year-on-year with strong demand in the important back-to-school shopping season with NVIDIA RTX ray tracing and AI technology now available at price points as low as $299. We entered the holidays with the best-ever line-up for gamers and creators. Gaming has doubled relative to pre-COVID levels even against the backdrop of lackluster PC market performance. This reflects the significant value we've brought to the gaming ecosystem with innovations like RTX and DLSS. The number of games and applications supporting these technologies has exploded in that period, driving upgrades and attracting new buyers. The RTX ecosystem continues to grow. There are now over 475 RTX-enabled games and applications. Generative AI is quickly emerging as the new pillar app for high performance PCs. NVIDIA RTX GPUs to find the most performance AI PCs and workstations. We just released TensorRT-LLM for Windows, which speeds on-device LLM inference up by 4x. With an installed base of over 100 million, NVIDIA RTX is the natural platform for AI application developers. Finally, our GeForce NOW cloud gaming service continues to build momentum. Its library of PC games surpassed 1,700 titles, including the launches of Alan Wake 2, Baldur's Gate 3, Cyberpunk 2077: Phantom Liberty and Starfield. Moving to the Pro Vis. Revenue of $416 million was up 10% sequentially and up 108% year-on year. NVIDIA RTX is the workstation platform of choice for professional design, engineering and simulation use cases and AI is emerging as a powerful demand driver. Early applications include inference for AI imaging in healthcare and edge AI in smart spaces and the public sector. We launched a new line of desktop workstations based on NVIDIA RTX Ada Lovelace generation GPUs and ConnectX, SmartNICs offering up to 2x the AI processing ray tracing and graphics performance of the previous generations. These powerful new workstations are optimized for AI workloads such as fine tune AI models, training smaller models and running inference locally. We continue to make progress on Omniverse, our software platform for designing, building and operating 3D virtual worlds. Mercedes-Benz (OTC:MBGAF) is using Omniverse powered digital twins to plan, design, build and operate its manufacturing and assembly facilities, helping it increase efficiency and reduce defects. Oxxon (ph) is also incorporating Omniverse into its manufacturing process, including end-to-end simulation for the entire robotics and automation pipeline, saving time and cost. We announced two new Omniverse Cloud services for automotive digitalization available on Microsoft Azure, a virtual factory simulation engine and autonomous vehicle simulation engine. Moving to Automotive. Revenue was $261 million, up 3% sequentially and up 4% year-on year, primarily driven by continued growth in self-driving platforms based on NVIDIA DRIVE Orin SOC and the ramp of AI cockpit solutions with global OEM customers. We extended our automotive partnership of Foxconn to include NVIDIA DRIVE for our next-generation automotive SOC. Foxconn has become the ODM for EVs. Our partnership provides Foxconn with a standard AV sensor and computing platform for their customers to easily build a state-of-an-art safe and secure software defined car. Now we're going to move to the rest of the P&L. GAAP gross margin expanded to 74% and non-GAAP gross margin to 75%, driven by higher Data Center sales and lower net inventory reserve, including a 1 percentage point benefit from the release of previously reserved inventory related to the Ampere GPU architecture products. Sequentially, GAAP operating expenses were up 12% and non-GAAP operating expenses were up 10%, primarily reflecting increased compensation and benefits. Let me turn to the fourth quarter of fiscal 2024. Total revenue is expected to be $20 billion, plus or minus 2%. We expect strong sequential growth to be driven by Data Center, with continued strong demand for both compute and networking. Gaming will likely decline sequentially as it is now more aligned with notebook seasonality. GAAP and non-GAAP gross margins are expected to be 74.5% and 75.5%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $3.17 billion and $2.2 billion, respectively. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $200 million, excluding gains and losses from non-affiliated investments. GAAP and non-GAAP tax rates are expected to be 15%, plus or minus 1% excluding any discrete items. Further financial information are included in the CFO commentary and other information available on our IR website. In closing, let me highlight some upcoming events for the financial community. We will attend the UBS Global Technology Conference in Scottsdale, Arizona, on November 28; the Wells Fargo TMT Summit in Rancho Palos Verdes, California on November 29; the Arete Virtual Tech Conference on December 7; and the J.P. Morgan Health Care Conference in San Francisco on January 8. Our earnings call to discuss the results of our fourth quarter and fiscal 2024 is scheduled for Wednesday, February 21. We will now open the call for questions. Operator, will you please poll for questions.\\nOperator: [Operator Instructions] Your first question comes from the line of Vivek Arya of Bank of America. Your line is open.\\nVivek Arya: Thanks for taking my question. Just, Colette, wanted to clarify what China contributions are you expecting in Q4. And then, Jensen, the main question is for you, where do you think we are in the adoption curve in terms of your shipments into the generative AI market? Because when I just look at the trajectory of your data center, is growth -- it will be close to nearly 30% of all the spending in data center next year. So what metrics are you keeping an eye on to inform you that you can continue to grow? Just where are we in the adoption curve of your products into the generative AI market? Thank you.\\nColette Kress: So, first let me start with your question, Vivek, on export controls and the impacts that we are seeing in our Q4 outlook and guidance that we provided. We had seen historically over the last several quarters that China and some of the other impacted destinations to be about 20% to 25% of our Data Center revenue. We are expecting in our guidance for that to decrease substantially as we move into Q4. The export controls will have a negative effect on our China business. And we do not have good visibility into the magnitude of that impact even over the long-term. We are though working to expand our Data Center product portfolio to possibly offer new regulation compliance solutions that do not require a license, these products, they may become available in the next coming months. However, we don't expect their contribution to be material or meaningful as a percentage of the revenue in Q4.\\nJensen Huang: Generative AI is the largest TAM expansion of software and hardware that we've seen in several decades. At the core of it, what's really exciting is that, what was largely a retrieval based computing approach, almost everything that you do is retrieved off of storage somewhere, has been augmented now, added with a generative method. And it's changed almost everything. You could see that text-to-text, text-to-image, text-to-video, text-to-3D, text-to-protein, text-to-chemicals, these were things that were processed and typed in by humans in the past. And these are now generative approaches. The way that we access data is changed. It used to be based on explicit queries. It is now based on natural language queries, intention queries, semantic queries. And so, we're excited about the work that we're doing with SAP and Dropbox and many others that you're going to hear about. And one of the areas that is really impactful is the software industry, which is about $1 trillion or so, has been building tools that are manually used over the last couple of decades. And now there's a whole new segment of software called copilots and assistants. Instead of manually used, these tools will have copilots to help you use it. And so, instead of licensing software, we will continue to do that, of course, but we will also hire copilots and assistants to help us use these -- use the software. We'll connect all of these copilots and assistants into teams of AIs, which is going to be the modern version of software, modern version of enterprise business software. And so the transformation of software and the way that software has done is driving the hardware underneath. And you can see that it's transforming hardware in two ways. One is something that's largely independent of generative AI. There's two trends: one is related to accelerated computing, general purpose computing is too wasteful of energy and cost. And now that we have much, much better approaches, call it, accelerated computing, you could save an order of magnitude of energy, you can save an order of magnitude of time or you can save an order of magnitudes of cost by using acceleration. And so, accelerated computing is transitioning, if you will, general purpose computing into this new approach. And that's been augmented by a new class of data centers. This is the traditional data centers that you were just talking about where we represent about a third of that. But there is a new class of data centers and this new class of data centers, unlike the data centers of the past, where you have a lot of applications running used by a great many people that are different tenants that are using the same infrastructure and that data center stores a lot of files. These new data centers are very few applications, if not one application, used by basically one tenant and it processes data, it trains models and then generates tokens and generates AI. And we call these new data centers AI factories. We're seeing AI factories being built out everywhere, and just by every country. And so if you look at the way where we are in the expansion, the transition into this new computing approach, the first wave you saw with large language model start-ups, generative AI start-ups and consumer Internet companies, and weren't in the process of ramping that. Meanwhile, while that's being ramped, you see that we're starting to partner with enterprise software companies who would like to build chatbots and copilots and assistants to augment the tools that they have on their platforms. You're seeing GPU specialized CSPs cropping up all over the world and they are dedicated to do really one thing, which is processing AI. You're seeing sovereign AI infrastructures, people -- countries that now recognize that they have to utilize their own data, keep their own data, keep their own culture, process that data and develop their own AI. You see that in India. Several -- about a year ago in Sweden, you are seeing in Japan. Last week, a big announcement in France. But the number of sovereign AI clouds that are being built is really quite significant. And my guess is that almost every major region will have and surely every major country will have their own AI clouds. And so I think you're seeing just new developments as the generative AI wave propagates through every industry, every company, every region. And so we're at the beginning of this inflection, this computing transition.\\nOperator: Your next question comes from the line of Aaron Rakers of Wells Fargo. Your line is open.\\nAaron Rakers: Yeah. Thanks for taking the question. I wanted to ask about kind of the networking side of the business. Given the growth rates that you've now cited, I think, it's 155% year-over-year and strong growth sequentially, it looks like that business is like almost approaching $2.5 billion to $3 billion quarterly level. I'm curious of how you see Ethernet involved evolving and maybe how you would characterize your differentiation of Spectrum-X relative to the traditional Ethernet stack as we start to think about that becoming part of the networking narrative above and maybe beyond just InfiniBand as we look into next year? Thank you.\\nJensen Huang: Yeah. Thanks for the question. Our networking business is already on a $10 billion plus run rate and it's going to get much larger. And as you mentioned, we added a new networking platform to our networking business recently. The vast majority of the dedicated large scale AI factories standardize on InfiniBand. And the reason for that is not only because of its data rate and not only just the latency, but the way that it moves traffic around the network is really important. The way that you process AI and a multi-tenant hyperscale Ethernet environment, the traffic pattern is just radically different. And with InfiniBand and with software defined networks, we could do congestion control, adaptive routing, performance isolation and noise isolation, not to mention, of course, the day rate and the low latency that -- and a very low overhead of InfiniBand that's natural part of InfiniBand. And so, InfiniBand is not so much just the network, it's also a computing fabric. We've put a lot of software-defined capabilities into the fabric including computation. We will do 40-point calculations and computation right on the switch, and right in the fabric itself. And so that's the reason why that difference in Ethernet versus InfiniBand or InfiniBand versus Ethernet for AI factories is so dramatic. And the difference is profound. And the reason for that is because you've just invested in a $2 billion infrastructure for AI factories. A 20%, 25%, 30% difference in overall effectiveness, especially as you scale up is measured in hundreds of millions of dollars of value. And if you will, renting that infrastructure over the course of four to five years, it really, really adds up. And so InfiniBand's value proposition is undeniable for AI factories. However, as we move AI into enterprise. This is enterprise computing what we'd like to enable every company to be able to build their own custom AIs. We're building customer AIs in our company based on our proprietary data, our proprietary type of skills. For example, recently we spoke about one of the models that we're creating, it's called ChipNeMo; we're building many others. There'll be tens, hundreds of custom AI models that we create inside our company. And our company is -- for all of our employee use, doesn't have to be as high performance as the AI factories we used to train the models. And so we would like the AI to be able to run in Ethernet environment. And so what we've done is we invented this new platform that extends Ethernet; doesn't replace Ethernet, it's 100% compliant with Ethernet. And it's optimized for East-West traffic, which is where the computing fabric is. It adds to Ethernet with an end-to-end solution with Bluefield, as well as our Spectrum switch that allows us to perform some of the capabilities that we have in InfiniBand, not all but some. And we achieved excellent results. And the way we go to market is we go to market with our large enterprise partners who already offer our computing solution. And so, HP (NYSE:HPQ), Dell and Lenovo has the NVIDIA AI stack, the NVIDIA AI Enterprise software stack and now they integrate with Bluefield, as well as bundle -- take a market there, Spectrum switch, and they'll be able to offer enterprise customers all over the world with their vast sales force and vast network of resellers a fully integrated, if you will, fully optimized, at least end-to-end AI solution. And so that's basically it, bringing AI to Ethernet for the world's enterprise.\\nOperator: Thank you. Your next question comes from the line of Joe Moore of Morgan Stanley. Your line is open.\\nJoseph Moore: Great. Thank you. I'm wondering if you could talk a little bit more about Grace Hopper and how you see the ability to leverage kind of the microprocessor, how you see that as a TAM expander. And what applications do you see using Grace Hopper versus more traditional H100 applications?\\nJensen Huang: Yeah. Thanks for the question. Grace Hopper is in production -- in high volume production now. We're expecting next year just with all of the design wins that we have in high performance computing and AI infrastructures, we are on a very, very fast ramp with our first data center CPU to a multi-billion dollar product line. This is going to be a very large product line for us. The capability of Grace Hopper is really quite spectacular. It has the ability to create computing nodes that simultaneously has very fast memory, as well as very large memory. In the areas of vector databases or semantic surge, what is called RAG, retrieval augmented generation. So that you could have a generative AI model be able to refer to proprietary data or a factual data before it generates a response, that data is quite large. And you can also have applications or generative models where the context length is very high. You basically store it in entire book into end-to-end system memory before you ask your questions. And so the context length can be quite large this way. The generative models has the ability to still be able to naturally interact with you on one hand. On the other hand, be able to refer to factual data, proprietary data or domain-specific data, you data and be contextually relevant and reduce hallucination. And so that particular use case for example is really quite fantastic for Grace Hopper. It also serves the customers that really care to have a different CPU than x86. Maybe it's a European supercomputing centers or European companies who would like to build up their own ARM ecosystem and like to build up a full stack or CSPs that have decided that they would like to pivot to ARM, because their own custom CPUs are based on ARM. There are variety of different reasons that drives the success of Grace Hopper, but we're off to a just an extraordinary start. This is a home run product.\\nOperator: Your next question comes from the line of Tim Arcuri of UBS. Your line is open.\\nTim Arcuri: Hi. Thanks. I wanted to ask a little bit about the visibility that you have on revenue. I know there's a few moving parts. I guess, on one hand, the purchase commitments went up a lot again. But on the other hand, China bans would arguably pull in when you can fill the demand beyond China. So I know we're not even into 2024 yet and it doesn't sound like, Jensen, you think that next year would be a peak in your Data Center revenue, but I just wanted to sort of explicitly ask you that. Do you think that Data Center can grow even in 2025? Thanks.\\nJensen Huang: Absolutely believe the Data Center can grow through 2025. And there are, of course, several reasons for that. We are expanding our supply quite significantly. We have already one of the broadest and largest and most capable supply chain in the world. Now, remember, people think that the GPU is a chip. But the HGX H100, the Hopper HGX has 35,000 parts, it weighs 70 pounds. Eight of the chips are Hopper. The other 35,000 are not. It is -- even its passive components are incredible. High voltage parts. High frequency parts. High current parts. It is a supercomputer, and therefore, the only way to test a supercomputer is with another supercomputer. Even the manufacturing of it is complicated, the testing of it is complicated, the shipping of it complicated and installation is complicated. And so, every aspect of our HGX supply chain is complicated. And the remarkable team that we have here has really scaled out the supply chain incredibly. Not to mention, all of our HGXs are connected with NVIDIA networking. And the networking, the transceivers, the mix, the cables, the switches, the amount of complexity there is just incredible. And so, I'm just -- first of all, I'm just super proud of the team for scaling up this incredible supply chain. We are absolutely world class. But meanwhile, we're adding new customers and new products. So we have new supply. We have new customers, as I was mentioning earlier. Different regions are standing up GPU specialist clouds, sovereign AI clouds coming out from all over the world, as people realize that they can't afford to export their country's knowledge, their country's culture for somebody else to then resell AI back to them, they have to -- they should, they have the skills and surely with us in combination, we can help them to do that build up their national AI. And so, the first thing that they have to do is, create their AI cloud, national AI cloud. You're also seeing us now growing into enterprise. The enterprise market has two paths. One path -- or if I could say three paths. The first path, of course, just off-the-shelf AI. And there are of course Chat GPT, a fabulous off-the-shelf AI, there'll be others. There's also a proprietary AI, because software companies like ServiceNow and SAP, there are many, many others that can't afford to have their company's intelligence be outsourced to somebody else. And they are about building tools and on top of their tools they should build custom and proprietary and domain-specific copilots and assistants that they can then rent to their customer base. This is -- they're sitting on a goldmine, almost every major tools company in the world is sitting on a goldmine, and they recognize that they have to go build their own custom AIs. We have a new service called an AI foundry, where we leverage NVS (ph) capabilities to be able to serve them in that. And then the next one is enterprises building their own custom AIs, their own custom chatbots, their own custom RAGs. And this capability is spreading all over the world. And the way that we're going to serve that marketplace is with the entire stacks of systems, which includes our compute, our networking and our switches, running our software stack called NVIDIA AI Enterprise, taking it through our market partners, HP, Dell, Lenovo, so on and so forth. And so we're just -- we're seeing the waves of generative AI starting from the start-ups and CSPs, moving to consumer Internet companies, moving to enterprise software platforms, moving to enterprise companies. And then ultimately, one of the areas that you guys have seen us spend a lot of energy on has to do with industrial generative AI. This is where NVIDIA AI and NVIDIA Omniverse comes together and that is a really, really exciting work. And so I think the -- we're at the beginning of a basically across-the-board industrial transition to generative AI to accelerated computing. This is going to affect every company, every industry, every country.\\nOperator: Your next question comes from the line of Toshiya Hari of Goldman Sachs. Your line is open.\\nToshiya Hari: Hi. Thank you. I wanted to clarify something with Colette real quick, and then I had a question for Jensen as well. Colette, you mentioned that you'll be introducing regulation-compliant products over the next couple of months. Yet, the contribution to Q4 revenue should be relatively limited. Is that a timing issue and could it be a source of reacceleration in growth for Data Center in April and beyond or are the price points such that the contribution to revenue going forward should be relatively limited? And then the question for Jensen, the AI foundry service announcement from last week. I just wanted to ask about that, and hopefully, have you expand on it. How is the monetization model going to work? Is it primarily services and software revenue? How should we think about the long term opportunity set? And is this going to be exclusive to Microsoft or do you have plans to expand to other partners as well? Thank you.\\nColette Kress: Thanks, Toshiya. On the question regarding potentially new products that we could provide to our China customers. It's a significant process to both design and develop these new products. As we discussed, we're going to make sure that we are in full discussions with the U.S. government of our intent to move products as well. Given our state about where we are in the quarter, we're already several weeks into the quarter. So it's just going to take some time for us to go through and discussing with our customers the needs and desires of these new products that we have. And moving forward, whether that's medium-term or long-term, it's just hard to say both the [Technical Difficulty] of what we can produce with the U.S. government and what the interest of our China customers in this. So we stay still focused on finding that right balance for our China customers, but it's hard to say at this time.\\nJensen Huang: Toshiya, thanks for the question. There is a glaring opportunity in the world for AI foundry, and it makes so much sense. First, every company has its core intelligence. It makes up our company. Our data, our domain expertise, in the case of many companies, we create tools, and most of the software companies in the world are tool platforms, and those tools are used by people today. And in the future, it's going to be used by people augmented with a whole bunch of AIs that we hire. And these platforms just got to go across the world and you'll see and we've only announced a few; SAP, ServiceNow, Dropbox, Getty, many others are coming. And the reason for that is because they have their own proprietary AI. They want their own proprietary AI. They can't afford to outsource their intelligence and handout their data, and handout their flywheel for other companies to build the AI for them. And so, they come to us. We have several things that are really essential in a foundry. Just as TSMC as a foundry, you have to have AI technology. And as you know, we have just an incredible depth of AI capability -- AI technology capability. And then second, you have to have the best practice known practice, the skills of processing data through the invention of AI models to create AIs that are guardrails, fine-tuned, so on and so forth, that are safe, so on and so forth. And the third thing is you need factories. And that's what DGX Cloud is. Our AI models are called AI Foundations. Our process, if you will, our CAD system for creating AIs are called NeMo and they run on NVIDIA's factories we call DGX Cloud. Our monetization model is that with each one of our partners they rent a sandbox on DGX Cloud, where we work together, they bring their data, they bring their domain expertise, we bring our researchers and engineers, we help them build their custom AI. We help them make that custom AI incredible. Then that custom AI becomes theirs. And they deploy it on the runtime that is enterprise grade, enterprise optimized or outperformance optimized, runs across everything NVIDIA. We have a giant installed base in the cloud, on-prem, anywhere. And it's secure, securely patched, constantly patched and optimized and supported. And we call that NVIDIA AI Enterprise. NVIDIA AI Enterprise is $4,500 per GP per year, that's our business model. Our business model is basically a license. Our customers then with that basic license can build their monetization model on top of. In a lot of ways we're wholesale, they become retail. They could have a per -- they could have subscription license base, they could per instance or they could do per usage, there is a lot of different ways that they could take a -- create their own business model, but ours is basically like a software license, like an operating system. And so our business model is help you create your custom models, you run those custom models on NVIDIA AI Enterprise. And it's off to a great start. NVIDIA AI Enterprise is going to be a very large business for us.\\nOperator: Your next question comes from the line of Stacy Rasgon of Bernstein Research. Your line is open.\\nStacy Rasgon: Hi, guys. Thanks for taking my questions. Colette, I wanted to know if it weren't for the China restrictions would the Q4 guide has been higher or are you supply-constrained in just reshipping stuff that would have gone to China elsewhere? And I guess along those lines you give us a feeling for where your lead times are right now in data center and just the China redirection such as-is, is it lowering those lead times, because you've got parts that are sort of immediately available to ship?\\nColette Kress: Yeah. Stacy, let me see if I can help you understand. Yes, there are still situations where we are working on both improving our supply each and every quarter. We've done a really solid job of ramping every quarter, which has defined our revenue. But with the absence of China for our outlook for Q4, sure, there could have been some things that we are not supply-constrained that we could have sold, but kind of we would no longer can. So could our guidance had been a little higher in our Q4? Yes. We are still working on improving our supply on plan, on continuing growing all throughout next year as well towards that.\\nOperator: Your next question comes from the line of Matt Ramsay of TD Cowen. Your line is open.\\nMatt Ramsay: Thank you very much. Congrats, everybody, on the results. Jensen, I had a two-part question for you, and it comes off of sort of one premise. And the premise is, I still get a lot of questions from investors thinking about AI training as being NVIDIA's dominant domain and somehow inference, even large model inference takes more and more of the TAM that the market will become more competitive. You'll be less differentiated et cetera., et cetera. So I guess the two parts of the question are: number one, maybe you could spend a little bit of time talking about the evolution of the inference workload as we move to LLMs and how your company is positioned for that rather than smaller model inference. And second, up until a month or two ago, I never really got any questions at all about the data processing piece of the AI workloads. So the pieces of manipulating the data before training, between training and inference, after inference and I think that's a large part of the workload now. Maybe you could talk about how CUDA is enabling acceleration of those pieces of the workload. Thanks.\\nJensen Huang: Sure. Inference is complicated. It's actually incredibly complicated. If you -- we this quarter announced one of the most exciting new engines, optimizing compilers called TensorRT-LLM. The reception has been incredible. You got to GitHub, it's been downloaded a ton, a whole lot of stars, integrated into stacks and frameworks all over the world, almost instantaneously. And there are several reasons for that, obviously. We could create TensorRT-LLM, because CUDA is programmable. If CUDA and our GPUs were not so programmable, it would really be hard for us to improve software stacks at the pace that we do. TensorRT-LLM, on the same GPU, without anybody touching anything, improves the performance by a factor of two. And then on top of that, of course, the pace of our innovation is so high. H200 increases it by another factor of two. And so, our inference performance, another way of saying inference cost, just reduced by a factor of four within about a year's time. And so, that's really, really hard to keep up with. The reason why everybody likes our inference engine is because our installed base. We've been dedicated to our installed base for 20 years, 20-plus years. We have an installed base that is not only largest in every single cloud, it's in every available from every enterprise system maker, it's used by companies of just about every industry. And every -- anytime you see a NVIDIA GPU, it runs our stack. It's architecturally compatible, something we've been dedicated to for a very long time. We're very disciplined about it. We make it our, if you will, architecture compatibility is job one. And that has conveyed to the world, the certainty of our platform stability. NVIDIA's platform stability certainty is the reason why everybody builds on us first and the reason why everybody optimizes on us first. All the engineering and all the work that you do, all the invention of technologies that you build on top of NVIDIA accrues to the -- and benefits everybody that uses our GPUs. And we have such a large installed base, large -- millions and millions of GPUs in cloud, 100 million GPUs from people’s PCs just about every workstation in the world, and they all architecturally compatible. And so, if you are an inference platform and you're deploying an inference application, you are basically an application provider. And as a software application provider, you're looking for large installed base. Data processing, before you could train a model, you have to curate the data, you have to dedupe the data, maybe you have to augment the data with synthetic data. So, process the data, clean the data, align the data, normalize the data, all of that data is measured not in bytes or megabytes, it's measured in terabytes and petabytes. And the amount of data processing that you do before data engineering, before that you do training is quite significant. It could represent 30%, 40%, 50% of the amount of work that you ultimately do. And what you -- and ultimately creating a data driven machine learning service. And so data processing is just a massive part. We accelerate Spark, we accelerate Python. One of the coolest things that we just did is called cuDF Pandas. Without one line of code, Pandas, which is the single most successful data science framework in the world. Pandas now is accelerated by NVIDIA CUDA. And just out-of-the box, without the line of code and so the acceleration is really quite terrific and people are just incredibly excited about it. And Pandas was designed for one purpose and one purpose only, really data processing, it's for data science. And so NVIDIA CUDA gives you all of that.\\nOperator: Your final question comes from the line of Harlan Sur of J.P. Morgan. Your line is open.\\nHarlan Sur: Good afternoon. Thanks for taking my question. If you look at the history of the tech industry like those companies that have been successful have always been focused on ecosystem; silicon, hardware, software, strong partnerships and just as importantly, right, an aggressive cadence of new products, more segmentation over time. The team recently announced a more aggressive new product cadence in data center from two years to now every year with higher levels of segmentation, training, optimization in printing CPU, GPU, DPU networking. How do we think about your R&D OpEx growth outlook to support a more aggressive and expanding forward roadmap, but more importantly, what is the team doing to manage and drive execution through all of this complexity?\\nJensen Huang: Gosh. Boy, that's just really excellent. You just wrote NVIDIA's business plan, and so you described our strategy. First of all, there is a fundamental reason why we accelerate our execution. And the reason for that is because it fundamentally drives down cost. When the combination of TensorRT-LLM and H200 reduce the cost for our customers for large model inference by a factor of four, and so that includes, of course, our speeds and feeds, but mostly it's because of our software, mostly the software benefits because of the architecture. And so we want to accelerate our roadmap for that reason. The second reason is to expand the reach of generative AI, the world's number of data center configurations -- this is kind of the amazing thing. NVIDIA is in every cloud, but not one cloud is the same. NVIDIA is working with every single cloud service provider and not one of the networking control plane, security posture is the same. Everybody's platform is different and yet we're integrated into all of their stacks, all of their data centers and we work incredibly well with all of them. And not to mention, we then take the whole thing and we create AI factories that are standalone. We take our platform, we can put them into supercomputers, we can put them into enterprise. Bringing AI to enterprise is something generative AI Enterprise something nobody's ever done before. And we're right now in the process of going to market with all of that. And so the complexity includes, of course, all the technologies and segments and the pace. It includes the fact that we are architecturally compatible across every single one of those. It includes all of the domain specific libraries that we create. The reason why every computer company, without thinking, can integrate NVIDIA into their roadmap and take it to market. And the reason for that is, because there is market demand for it. There is market demand in healthcare, there is market demand in manufacturing, there is market demand, of course, in AI, including financial services, in supercomputing and quantum computing. The list of markets and segments that we have domain specific libraries is incredibly broaden. And then finally, we have an end-to-end solution for data centers; InfiniBand networking, Ethernet networking, x86, ARM, just about every permutation combination of solutions -- technology solutions and software stacks provided. And that translates to having the largest number of ecosystem software developers; the largest ecosystem of system makers; the largest and broadest distribution partnership network; and ultimately, the greatest reach. And that takes -- surely that takes a lot of energy. But the thing that really holds it together, and this is a great decision that we made decades ago, which is everything is architecturally compatible. When we develop a domain specific language that runs on one GPU, it runs on every GPU. When we optimize TensorRT for the cloud, we optimized it for enterprise. When we do something that brings in a new feature, a new library, a new feature or a new developer, they instantly get the benefit of all of our reach. And so that discipline, that architecture compatible discipline that has lasted more than a couple of decades now, is one of the reasons why NVIDIA is still really, really efficient. I mean, we're 28,000 people large and serving just about every single company, every single industry, every single market around the world.\\nOperator: Thank you. I will now turn the call back over to Jensen Huang for closing remarks.\\nJensen Huang: Our strong growth reflects the broad industry platform transition from general purpose to accelerated computing and generative AI. Large language models start-ups consumer Internet companies and global cloud service providers are the first movers. The next waves are starting to build. Nations and regional CSPs are building AI clouds to serve local demand. Enterprise software companies like Adobe and Dropbox, SAP and ServiceNow are adding AI copilots and assistants to their platforms. Enterprises in the world's largest industries are creating custom AIs to automate and boost productivity. The generative AI era is in full steam and has created the need for a new type of data center, an AI factory; optimized for refining data and training, and inference, and generating AI. AI factory workloads are different and incremental to legacy data center workloads supporting IT tasks. AI factories run copilots and AI assistants, which are significant software TAM expansion and are driving significant new investment. Expanding the $1 trillion traditional data center infrastructure installed base, empowering the AI Industrial Revolution. NVIDIA H100 HGX with InfiniBand and the NVIDIA AI software stack define an AI factory today. As we expand our supply chain to meet the world's demand, we are also building new growth drivers for the next wave of AI. We highlighted three elements to our new growth strategy that are hitting their stride: CPU, networking, and software and services. Grace is NVIDIA's first data center CPU. Grace and Grace Hopper are in full production and ramping into a new multi-billion dollar product line next year. Irrespective of the CPU choice, we can help customers build an AI factory. NVIDIA networking now exceeds $10 billion annualized revenue run rate. InfiniBand grew five-fold year-over-year, and is positioned for excellent growth ahead as the networking of AI factories. Enterprises are also racing to adopt AI and Ethernet is the standard networking. This week we announced an Ethernet for AI platform for enterprises. NVIDIA Spectrum-X is an end-to-end solution of Bluefield SuperNIC, Spectrum-4 Ethernet switch and software that boosts Ethernet performance by up to 1.6x for AI workloads. Dell, HPE and Lenovo have joined us to bring a full generative AI solution of NVIDIA AI computing, networking and software to the world's enterprises. NVIDIA software and services is on track to exit the year at an annualized run rate of $1 billion. Enterprise software platforms like ServiceNow and SAP need to build and operate proprietary AI. Enterprises need to build and deploy custom AI copilots. We have the AI technology, expertise and scale to help customers build custom models with their proprietary data on NVIDIA DGX Cloud and deploy the AI applications on enterprise grade NVIDIA AI Enterprise. NVIDIA is essentially an AI foundry. NVIDIA's GPUs, CPUs, networking, AI foundry services and NVIDIA AI Enterprise software are all growth engines in full throttle. Thanks for joining us today. We look forward to updating you on our progress next quarter.\\nOperator: This concludes today's conference call. You may now disconnect.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " \"NVIDIA's latest earnings call revealed record-breaking revenue of $18.1 billion, marking a 34% sequential increase and a 200% year-on-year increase. The company's Data Center segment was a primary driver of this growth, achieving record revenue of $14.5 billion. However, the company anticipates a significant decline in the fourth quarter due to new U.S. export control regulations affecting sales to China and other markets. \\nKey takeaways from the call include:NVIDIA's Data Center segment saw record revenue, driven by the NVIDIA HGX platform and InfiniBand networking. Major companies like Meta (NASDAQ:META) and Adobe (NASDAQ:ADBE) are integrating NVIDIA's technology into their platforms, contributing to the strong demand for AI supercomputers and data center infrastructure.NVIDIA is expanding its Data Center product portfolio to comply with new U.S. export control regulations. Despite expecting a decrease in sales to China and other markets in the fourth quarter, the company is focusing on national investment in sovereign AI infrastructure.The Gaming segment recorded revenue of $2.86 billion, a 15% sequential increase and an 80% year-on-year increase, driven by strong demand for NVIDIA's RTX ray tracing and AI technology.In the Pro Vis segment, revenue reached $416 million, a 10% sequential increase and a 108% year-on-year increase. NVIDIA RTX is the preferred platform for professional design, engineering, and simulation, with AI emerging as a demand driver.NVIDIA's CEO, Jensen Huang, discussed the expansion of AI factories and the transition into a new computing approach. He also expressed confidence in the continued growth of the Data Center business through 2025.During the call, NVIDIA highlighted its product innovations, including the H200 GPU, which offers faster and larger memory for generative AI and language models, and the GH200 Grace Hopper Superchip, a combination of an ARM-based CPU with a Hopper GPU. The Grace Hopper Superchip is being adopted by supercomputing customers, with the combined AI compute capacity of supercomputers built on Grace Hopper expected to exceed 200 exaflops next year. \\nIn addition to its product offerings, NVIDIA is also expanding its networking into the Ethernet space with its new Spectrum-X end-to-end Ethernet offering, which will be available in Q1 next year. This offering can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. \\nNVIDIA also discussed its business strategy, emphasizing the importance of reselling AI technology to customers and its growth in the enterprise market. The company introduced an AI foundry service to help enterprises build custom AI models and announced an aggressive product release schedule in the data center market. \\nLooking ahead, NVIDIA expects total revenue of $20 billion for Q4 2024, driven by strong demand in the Data Center sector. Despite regulatory challenges, the company is confident in its growth potential, citing expanded supply, new customers, and its entrance into the enterprise market.Full transcript -  Nvidia Corp  (NASDAQ:NVDA) Q3 2024:Operator: Good afternoon. My name is JL, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's Third Quarter Earnings Call. All lines have been placed on mute to prevent any background noise. After the speakers’ remarks, there will be a question-and-answer session. [Operator Instructions] Simona Jankowski, you may now begin your conference.\\nSimona Jankowski: Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2024. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter and fiscal 2024. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All statements are made as of today, November 21, 2023, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\\nColette Kress: Thanks, Simona. Q3 was another record quarter. Revenue of $18.1 billion was up 34% sequentially and up more than 200% year-on-year and well above our outlook for $16 billion. Starting with Data Center. The continued ramp of the NVIDIA HGX platform based on our Hopper Tensor Core GPU architecture, along with InfiniBand end-to-end networking, drove record revenue of $14.5 billion, up 41% sequentially and up 279% year-on-year. NVIDIA HGX with InfiniBand together are essentially the reference architecture for AI supercomputers and data center infrastructures. Some of the most exciting generative AI applications are built and run on NVIDIA, including Adobe Firefly, ChatGPT, Microsoft (NASDAQ:MSFT) 365 Copilot, CoAssist, now assist with ServiceNow (NYSE:NOW) and Zoom (NASDAQ:ZM) AI Companion. Our Data Center compute revenue quadrupled from last year and networking revenue nearly tripled. Investments in infrastructure for training and inferencing large language models, deep learning, recommender systems and generative AI applications is fueling strong broad-based demand for NVIDIA accelerated computing. Inferencing is now a major workload for NVIDIA AI computing. Consumer Internet companies and enterprises drove exceptional sequential growth in Q3, comprising approximately half of our Data Center revenue and outpacing total growth. Companies like Meta are in full production with deep learning, recommender systems and also investing in generative AI to help advertisers optimize images and text. Most major consumer Internet companies are racing to ramp up generative AI deployment. The enterprise wave of AI adoption is now beginning. Enterprise software companies such as Adobe, Databricks, Snowflake (NYSE:SNOW) and ServiceNow are adding AI copilots and the systems to their platforms. And broader enterprises are developing custom AI for vertical industry applications such as Tesla (NASDAQ:TSLA) in autonomous driving. Cloud service providers drove roughly the other half of our Data Center revenue in the quarter. Demand was strong from all hyperscale CSPs, as well as from a broadening set of GPU-specialized CSPs globally that are rapidly growing to address the new market opportunities in AI. NVIDIA H100 Tensor Core GPU instances are now generally available in virtually every cloud with instances in high demand. We have significantly increased supply every quarter this year to meet strong demand and expect to continue to do so next year. We will also have a broader and faster product launch cadence to meet the growing and diverse set of AI opportunities. Towards the end of the quarter, the U.S. government announced a new set of export control regulations for China and other markets, including Vietnam and certain countries in the Middle East. These regulations require licenses for the export of a number of our products, including our Hopper and Ampere 100 and 800 series and several others. Our sales to China and other affected destinations derived from products that are now subject to licensing requirements have consistently contributed approximately 20% to 25% of Data Center revenue over the past few quarters. We expect that our sales to these destinations will decline significantly in the fourth quarter. So we believe will be more than offset by strong growth in other regions. The U.S. government designed the regulation to allow the U.S. industry to provide data center compute products to markets worldwide, including China. Continuing to compete worldwide as the regulations encourage, promotes U.S. technology leadership, spurs economic growth and supports U.S. jobs. For the highest performance levels, the government requires licenses. For lower performance levels, the government requires a streamlined prior notification process. And for products even lower performance levels, the government does not require any notice at all. Following the government's clear guidelines, we are working to expand our Data Center product portfolio to offer compliance solutions for each regulatory category, including products for which the U.S. government does not wish to have advance notice before each shipment. We are working with some customers in China and the Middle East to pursue licenses from the U.S. government. It is too early to know whether these will be granted for any significant amount of revenue. Many countries are awakening to the need to invest in sovereign AI infrastructure to support economic growth and industrial innovation. With investments in domestic compute capacity, nations can use their own data to train LLMs and support their local generative AI ecosystems. For example, we are working with India's government and largest tech companies including  Infosys  (NS:INFY), Reliance and Tata to boost their sovereign AI infrastructure. And French private cloud provider, Scaleway, is building a regional AI cloud based on NVIDIA H100 InfiniBand and NVIDIA's AI Enterprise software to fuel advancement across France and Europe. National investment in compute capacity is a new economic imperative and serving the sovereign AI infrastructure market represents a multi-billion dollar opportunity over the next few years. From a product perspective, the vast majority of revenue in Q3 was driven by the NVIDIA HGX platform based on our Hopper GPU architecture with lower contribution from the prior generation Ampere GPU architecture. The new L40S GPU built for industry standard servers began to ship, supporting training and inference workloads across a variety of consumers. This was also the first revenue quarter of our GH200 Grace Hopper Superchip, which combines our ARM-based Grace CPU with a Hopper GPU. Grace and Grace Hopper are ramping into a new multi-billion dollar product line. Grace Hopper instances are now available at GPU specialized cloud providers, and coming soon to Oracle (NYSE:ORCL) Cloud. Grace Hopper is also getting significant traction with supercomputing customers. Initial shipments to Los Alamos National Lab and the Swiss National Supercomputing Center took place in the third quarter. The UK government announced it will build one of the world's fastest AI supercomputers called Isambard-AI with almost 5,500 Grace Hopper Superchips. German supercomputing center, Julich, also announced that it will build its next-generation AI supercomputer with close to 24,000 Grace Hopper Superchips and Quantum-2 InfiniBand, making it the world's most powerful AI supercomputer with over 90 exaflops of AI performance. All-in, we estimate that the combined AI compute capacity of all the supercomputers built on Grace Hopper across the U.S., Europe and Japan next year will exceed 200 exaflops with more wins to come. Inference is contributing significantly to our data center demand, as AI is now in full production for deep learning, recommenders, chatbots, copilots and text to image generation and this is just the beginning. NVIDIA AI offers the best inference performance and versatility, and thus the lower power and cost of ownership. We are also driving a fast cost reduction curve. With the release of TensorRT-LLM, we now achieved more than 2x the inference performance for half the cost of inferencing LLMs on NVIDIA GPUs. We also announced the latest member of the Hopper family, the H200, which will be the first GPU to offer HBM3e, faster, larger memory to further accelerate generative AI and LLMs. It moves inference speed up to another 2x compared to H100 GPUs for running LLMs like Norma2 (ph). Combined, TensorRT-LLM and H200, increased performance or reduced cost by 4x in just one year. With our customers changing their stack, this is a benefit of CUDA and our architecture compatibility. Compared to the A100, H200 delivers an 18x performance increase for inferencing models like GPT-3, allowing customers to move to larger models and with no increase in latency. Amazon (NASDAQ:AMZN) Web Services, Google (NASDAQ:GOOGL) Cloud, Microsoft Azure and Oracle Cloud will be among the first CSPs to offer H200-based instances starting next year. At last week's Microsoft Ignite, we deepened and expanded our collaboration with Microsoft across the entire stock. We introduced an AI foundry service for the development and tuning of custom generative AI enterprise applications running on Azure. Customers can bring their domain knowledge and proprietary data and we help them build their AI models using our AI expertise and software stock in our DGX cloud, all with enterprise grade security and support. SAP and  Amdocs  (NASDAQ:DOX) are the first customers of the NVIDIA AI foundry service on Microsoft Azure. In addition, Microsoft will launch new confidential computing instances based on the H100. The H100 remains the top performing and most versatile platform for AI training and by a wide margin, as shown in the latest MLPerf industry benchmark results. Our training cluster included more than 10,000 H100 GPUs or 3x more than in June, reflecting very efficient scaling. Efficient scaling is a key requirement in generative AI, because LLMs are growing by an order of magnitude every year. Microsoft Azure achieved similar results on a nearly identical cluster, demonstrating the efficiency of NVIDIA AI in public cloud deployments. Networking now exceeds a $10 billion annualized revenue run rate. Strong growth was driven by exceptional demand for InfiniBand, which grew fivefold year-on-year. InfiniBand is critical to gaining the scale and performance needed for training LLMs. Microsoft made this very point last week, highlighting that Azure uses over 29,000 miles of InfiniBand cabling, enough to circle the globe. We are expanding NVIDIA networking into the Ethernet space. Our new Spectrum-X end-to-end Ethernet offering with technologies, purpose built for AI, will be available in Q1 next year. With support from leading OEMs, including Dell (NYSE:DELL), HPE and Lenovo. Spectrum-X can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. Let me also provide an update on our software and services offerings, where we are starting to see excellent adoption. We are on track to exit the year at an annualized revenue run rate of $1 billion for our recurring software, support and services offerings. We see two primary opportunities for growth over the intermediate term with our DGX cloud service and with our NVIDIA AI Enterprise software, each reflects the growth of enterprise AI training and enterprise AI inference, respectively. Our latest DGX cloud customer announcement was this morning as part of an AI research collaboration with Gentech, the biotechnology pioneer also plans to use our BioNeMo LLM framework to help accelerate and optimize their AI drug discovery platform. We now have enterprise AI partnership with Adobe, Dropbox (NASDAQ:DBX), Getty, SAP, ServiceNow, Snowflake and others to come. Okay. Moving to Gaming. Gaming revenue of $2.86 billion was up 15% sequentially and up more than 80% year-on-year with strong demand in the important back-to-school shopping season with NVIDIA RTX ray tracing and AI technology now available at price points as low as $299. We entered the holidays with the best-ever line-up for gamers and creators. Gaming has doubled relative to pre-COVID levels even against the backdrop of lackluster PC market performance. This reflects the significant value we've brought to the gaming ecosystem with innovations like RTX and DLSS. The number of games and applications supporting these technologies has exploded in that period, driving upgrades and attracting new buyers. The RTX ecosystem continues to grow. There are now over 475 RTX-enabled games and applications. Generative AI is quickly emerging as the new pillar app for high performance PCs. NVIDIA RTX GPUs to find the most performance AI PCs and workstations. We just released TensorRT-LLM for Windows, which speeds on-device LLM inference up by 4x. With an installed base of over 100 million, NVIDIA RTX is the natural platform for AI application developers. Finally, our GeForce NOW cloud gaming service continues to build momentum. Its library of PC games surpassed 1,700 titles, including the launches of Alan Wake 2, Baldur's Gate 3, Cyberpunk 2077: Phantom Liberty and Starfield. Moving to the Pro Vis. Revenue of $416 million was up 10% sequentially and up 108% year-on year. NVIDIA RTX is the workstation platform of choice for professional design, engineering and simulation use cases and AI is emerging as a powerful demand driver. Early applications include inference for AI imaging in healthcare and edge AI in smart spaces and the public sector. We launched a new line of desktop workstations based on NVIDIA RTX Ada Lovelace generation GPUs and ConnectX, SmartNICs offering up to 2x the AI processing ray tracing and graphics performance of the previous generations. These powerful new workstations are optimized for AI workloads such as fine tune AI models, training smaller models and running inference locally. We continue to make progress on Omniverse, our software platform for designing, building and operating 3D virtual worlds. Mercedes-Benz (OTC:MBGAF) is using Omniverse powered digital twins to plan, design, build and operate its manufacturing and assembly facilities, helping it increase efficiency and reduce defects. Oxxon (ph) is also incorporating Omniverse into its manufacturing process, including end-to-end simulation for the entire robotics and automation pipeline, saving time and cost. We announced two new Omniverse Cloud services for automotive digitalization available on Microsoft Azure, a virtual factory simulation engine and autonomous vehicle simulation engine. Moving to Automotive. Revenue was $261 million, up 3% sequentially and up 4% year-on year, primarily driven by continued growth in self-driving platforms based on NVIDIA DRIVE Orin SOC and the ramp of AI cockpit solutions with global OEM customers. We extended our automotive partnership of Foxconn to include NVIDIA DRIVE for our next-generation automotive SOC. Foxconn has become the ODM for EVs. Our partnership provides Foxconn with a standard AV sensor and computing platform for their customers to easily build a state-of-an-art safe and secure software defined car. Now we're going to move to the rest of the P&L. GAAP gross margin expanded to 74% and non-GAAP gross margin to 75%, driven by higher Data Center sales and lower net inventory reserve, including a 1 percentage point benefit from the release of previously reserved inventory related to the Ampere GPU architecture products. Sequentially, GAAP operating expenses were up 12% and non-GAAP operating expenses were up 10%, primarily reflecting increased compensation and benefits. Let me turn to the fourth quarter of fiscal 2024. Total revenue is expected to be $20 billion, plus or minus 2%. We expect strong sequential growth to be driven by Data Center, with continued strong demand for both compute and networking. Gaming will likely decline sequentially as it is now more aligned with notebook seasonality. GAAP and non-GAAP gross margins are expected to be 74.5% and 75.5%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $3.17 billion and $2.2 billion, respectively. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $200 million, excluding gains and losses from non-affiliated investments. GAAP and non-GAAP tax rates are expected to be 15%, plus or minus 1% excluding any discrete items. Further financial information are included in the CFO commentary and other information available on our IR website. In closing, let me highlight some upcoming events for the financial community. We will attend the UBS Global Technology Conference in Scottsdale, Arizona, on November 28; the Wells Fargo TMT Summit in Rancho Palos Verdes, California on November 29; the Arete Virtual Tech Conference on December 7; and the J.P. Morgan Health Care Conference in San Francisco on January 8. Our earnings call to discuss the results of our fourth quarter and fiscal 2024 is scheduled for Wednesday, February 21. We will now open the call for questions. Operator, will you please poll for questions.\\nOperator: [Operator Instructions] Your first question comes from the line of Vivek Arya of Bank of America. Your line is open.\\nVivek Arya: Thanks for taking my question. Just, Colette, wanted to clarify what China contributions are you expecting in Q4. And then, Jensen, the main question is for you, where do you think we are in the adoption curve in terms of your shipments into the generative AI market? Because when I just look at the trajectory of your data center, is growth -- it will be close to nearly 30% of all the spending in data center next year. So what metrics are you keeping an eye on to inform you that you can continue to grow? Just where are we in the adoption curve of your products into the generative AI market? Thank you.\\nColette Kress: So, first let me start with your question, Vivek, on export controls and the impacts that we are seeing in our Q4 outlook and guidance that we provided. We had seen historically over the last several quarters that China and some of the other impacted destinations to be about 20% to 25% of our Data Center revenue. We are expecting in our guidance for that to decrease substantially as we move into Q4. The export controls will have a negative effect on our China business. And we do not have good visibility into the magnitude of that impact even over the long-term. We are though working to expand our Data Center product portfolio to possibly offer new regulation compliance solutions that do not require a license, these products, they may become available in the next coming months. However, we don't expect their contribution to be material or meaningful as a percentage of the revenue in Q4.\\nJensen Huang: Generative AI is the largest TAM expansion of software and hardware that we've seen in several decades. At the core of it, what's really exciting is that, what was largely a retrieval based computing approach, almost everything that you do is retrieved off of storage somewhere, has been augmented now, added with a generative method. And it's changed almost everything. You could see that text-to-text, text-to-image, text-to-video, text-to-3D, text-to-protein, text-to-chemicals, these were things that were processed and typed in by humans in the past. And these are now generative approaches. The way that we access data is changed. It used to be based on explicit queries. It is now based on natural language queries, intention queries, semantic queries. And so, we're excited about the work that we're doing with SAP and Dropbox and many others that you're going to hear about. And one of the areas that is really impactful is the software industry, which is about $1 trillion or so, has been building tools that are manually used over the last couple of decades. And now there's a whole new segment of software called copilots and assistants. Instead of manually used, these tools will have copilots to help you use it. And so, instead of licensing software, we will continue to do that, of course, but we will also hire copilots and assistants to help us use these -- use the software. We'll connect all of these copilots and assistants into teams of AIs, which is going to be the modern version of software, modern version of enterprise business software. And so the transformation of software and the way that software has done is driving the hardware underneath. And you can see that it's transforming hardware in two ways. One is something that's largely independent of generative AI. There's two trends: one is related to accelerated computing, general purpose computing is too wasteful of energy and cost. And now that we have much, much better approaches, call it, accelerated computing, you could save an order of magnitude of energy, you can save an order of magnitude of time or you can save an order of magnitudes of cost by using acceleration. And so, accelerated computing is transitioning, if you will, general purpose computing into this new approach. And that's been augmented by a new class of data centers. This is the traditional data centers that you were just talking about where we represent about a third of that. But there is a new class of data centers and this new class of data centers, unlike the data centers of the past, where you have a lot of applications running used by a great many people that are different tenants that are using the same infrastructure and that data center stores a lot of files. These new data centers are very few applications, if not one application, used by basically one tenant and it processes data, it trains models and then generates tokens and generates AI. And we call these new data centers AI factories. We're seeing AI factories being built out everywhere, and just by every country. And so if you look at the way where we are in the expansion, the transition into this new computing approach, the first wave you saw with large language model start-ups, generative AI start-ups and consumer Internet companies, and weren't in the process of ramping that. Meanwhile, while that's being ramped, you see that we're starting to partner with enterprise software companies who would like to build chatbots and copilots and assistants to augment the tools that they have on their platforms. You're seeing GPU specialized CSPs cropping up all over the world and they are dedicated to do really one thing, which is processing AI. You're seeing sovereign AI infrastructures, people -- countries that now recognize that they have to utilize their own data, keep their own data, keep their own culture, process that data and develop their own AI. You see that in India. Several -- about a year ago in Sweden, you are seeing in Japan. Last week, a big announcement in France. But the number of sovereign AI clouds that are being built is really quite significant. And my guess is that almost every major region will have and surely every major country will have their own AI clouds. And so I think you're seeing just new developments as the generative AI wave propagates through every industry, every company, every region. And so we're at the beginning of this inflection, this computing transition.\\nOperator: Your next question comes from the line of Aaron Rakers of Wells Fargo. Your line is open.\\nAaron Rakers: Yeah. Thanks for taking the question. I wanted to ask about kind of the networking side of the business. Given the growth rates that you've now cited, I think, it's 155% year-over-year and strong growth sequentially, it looks like that business is like almost approaching $2.5 billion to $3 billion quarterly level. I'm curious of how you see Ethernet involved evolving and maybe how you would characterize your differentiation of Spectrum-X relative to the traditional Ethernet stack as we start to think about that becoming part of the networking narrative above and maybe beyond just InfiniBand as we look into next year? Thank you.\\nJensen Huang: Yeah. Thanks for the question. Our networking business is already on a $10 billion plus run rate and it's going to get much larger. And as you mentioned, we added a new networking platform to our networking business recently. The vast majority of the dedicated large scale AI factories standardize on InfiniBand. And the reason for that is not only because of its data rate and not only just the latency, but the way that it moves traffic around the network is really important. The way that you process AI and a multi-tenant hyperscale Ethernet environment, the traffic pattern is just radically different. And with InfiniBand and with software defined networks, we could do congestion control, adaptive routing, performance isolation and noise isolation, not to mention, of course, the day rate and the low latency that -- and a very low overhead of InfiniBand that's natural part of InfiniBand. And so, InfiniBand is not so much just the network, it's also a computing fabric. We've put a lot of software-defined capabilities into the fabric including computation. We will do 40-point calculations and computation right on the switch, and right in the fabric itself. And so that's the reason why that difference in Ethernet versus InfiniBand or InfiniBand versus Ethernet for AI factories is so dramatic. And the difference is profound. And the reason for that is because you've just invested in a $2 billion infrastructure for AI factories. A 20%, 25%, 30% difference in overall effectiveness, especially as you scale up is measured in hundreds of millions of dollars of value. And if you will, renting that infrastructure over the course of four to five years, it really, really adds up. And so InfiniBand's value proposition is undeniable for AI factories. However, as we move AI into enterprise. This is enterprise computing what we'd like to enable every company to be able to build their own custom AIs. We're building customer AIs in our company based on our proprietary data, our proprietary type of skills. For example, recently we spoke about one of the models that we're creating, it's called ChipNeMo; we're building many others. There'll be tens, hundreds of custom AI models that we create inside our company. And our company is -- for all of our employee use, doesn't have to be as high performance as the AI factories we used to train the models. And so we would like the AI to be able to run in Ethernet environment. And so what we've done is we invented this new platform that extends Ethernet; doesn't replace Ethernet, it's 100% compliant with Ethernet. And it's optimized for East-West traffic, which is where the computing fabric is. It adds to Ethernet with an end-to-end solution with Bluefield, as well as our Spectrum switch that allows us to perform some of the capabilities that we have in InfiniBand, not all but some. And we achieved excellent results. And the way we go to market is we go to market with our large enterprise partners who already offer our computing solution. And so, HP (NYSE:HPQ), Dell and Lenovo has the NVIDIA AI stack, the NVIDIA AI Enterprise software stack and now they integrate with Bluefield, as well as bundle -- take a market there, Spectrum switch, and they'll be able to offer enterprise customers all over the world with their vast sales force and vast network of resellers a fully integrated, if you will, fully optimized, at least end-to-end AI solution. And so that's basically it, bringing AI to Ethernet for the world's enterprise.\\nOperator: Thank you. Your next question comes from the line of Joe Moore of Morgan Stanley. Your line is open.\\nJoseph Moore: Great. Thank you. I'm wondering if you could talk a little bit more about Grace Hopper and how you see the ability to leverage kind of the microprocessor, how you see that as a TAM expander. And what applications do you see using Grace Hopper versus more traditional H100 applications?\\nJensen Huang: Yeah. Thanks for the question. Grace Hopper is in production -- in high volume production now. We're expecting next year just with all of the design wins that we have in high performance computing and AI infrastructures, we are on a very, very fast ramp with our first data center CPU to a multi-billion dollar product line. This is going to be a very large product line for us. The capability of Grace Hopper is really quite spectacular. It has the ability to create computing nodes that simultaneously has very fast memory, as well as very large memory. In the areas of vector databases or semantic surge, what is called RAG, retrieval augmented generation. So that you could have a generative AI model be able to refer to proprietary data or a factual data before it generates a response, that data is quite large. And you can also have applications or generative models where the context length is very high. You basically store it in entire book into end-to-end system memory before you ask your questions. And so the context length can be quite large this way. The generative models has the ability to still be able to naturally interact with you on one hand. On the other hand, be able to refer to factual data, proprietary data or domain-specific data, you data and be contextually relevant and reduce hallucination. And so that particular use case for example is really quite fantastic for Grace Hopper. It also serves the customers that really care to have a different CPU than x86. Maybe it's a European supercomputing centers or European companies who would like to build up their own ARM ecosystem and like to build up a full stack or CSPs that have decided that they would like to pivot to ARM, because their own custom CPUs are based on ARM. There are variety of different reasons that drives the success of Grace Hopper, but we're off to a just an extraordinary start. This is a home run product.\\nOperator: Your next question comes from the line of Tim Arcuri of UBS. Your line is open.\\nTim Arcuri: Hi. Thanks. I wanted to ask a little bit about the visibility that you have on revenue. I know there's a few moving parts. I guess, on one hand, the purchase commitments went up a lot again. But on the other hand, China bans would arguably pull in when you can fill the demand beyond China. So I know we're not even into 2024 yet and it doesn't sound like, Jensen, you think that next year would be a peak in your Data Center revenue, but I just wanted to sort of explicitly ask you that. Do you think that Data Center can grow even in 2025? Thanks.\\nJensen Huang: Absolutely believe the Data Center can grow through 2025. And there are, of course, several reasons for that. We are expanding our supply quite significantly. We have already one of the broadest and largest and most capable supply chain in the world. Now, remember, people think that the GPU is a chip. But the HGX H100, the Hopper HGX has 35,000 parts, it weighs 70 pounds. Eight of the chips are Hopper. The other 35,000 are not. It is -- even its passive components are incredible. High voltage parts. High frequency parts. High current parts. It is a supercomputer, and therefore, the only way to test a supercomputer is with another supercomputer. Even the manufacturing of it is complicated, the testing of it is complicated, the shipping of it complicated and installation is complicated. And so, every aspect of our HGX supply chain is complicated. And the remarkable team that we have here has really scaled out the supply chain incredibly. Not to mention, all of our HGXs are connected with NVIDIA networking. And the networking, the transceivers, the mix, the cables, the switches, the amount of complexity there is just incredible. And so, I'm just -- first of all, I'm just super proud of the team for scaling up this incredible supply chain. We are absolutely world class. But meanwhile, we're adding new customers and new products. So we have new supply. We have new customers, as I was mentioning earlier. Different regions are standing up GPU specialist clouds, sovereign AI clouds coming out from all over the world, as people realize that they can't afford to export their country's knowledge, their country's culture for somebody else to then resell AI back to them, they have to -- they should, they have the skills and surely with us in combination, we can help them to do that build up their national AI. And so, the first thing that they have to do is, create their AI cloud, national AI cloud. You're also seeing us now growing into enterprise. The enterprise market has two paths. One path -- or if I could say three paths. The first path, of course, just off-the-shelf AI. And there are of course Chat GPT, a fabulous off-the-shelf AI, there'll be others. There's also a proprietary AI, because software companies like ServiceNow and SAP, there are many, many others that can't afford to have their company's intelligence be outsourced to somebody else. And they are about building tools and on top of their tools they should build custom and proprietary and domain-specific copilots and assistants that they can then rent to their customer base. This is -- they're sitting on a goldmine, almost every major tools company in the world is sitting on a goldmine, and they recognize that they have to go build their own custom AIs. We have a new service called an AI foundry, where we leverage NVS (ph) capabilities to be able to serve them in that. And then the next one is enterprises building their own custom AIs, their own custom chatbots, their own custom RAGs. And this capability is spreading all over the world. And the way that we're going to serve that marketplace is with the entire stacks of systems, which includes our compute, our networking and our switches, running our software stack called NVIDIA AI Enterprise, taking it through our market partners, HP, Dell, Lenovo, so on and so forth. And so we're just -- we're seeing the waves of generative AI starting from the start-ups and CSPs, moving to consumer Internet companies, moving to enterprise software platforms, moving to enterprise companies. And then ultimately, one of the areas that you guys have seen us spend a lot of energy on has to do with industrial generative AI. This is where NVIDIA AI and NVIDIA Omniverse comes together and that is a really, really exciting work. And so I think the -- we're at the beginning of a basically across-the-board industrial transition to generative AI to accelerated computing. This is going to affect every company, every industry, every country.\\nOperator: Your next question comes from the line of Toshiya Hari of Goldman Sachs. Your line is open.\\nToshiya Hari: Hi. Thank you. I wanted to clarify something with Colette real quick, and then I had a question for Jensen as well. Colette, you mentioned that you'll be introducing regulation-compliant products over the next couple of months. Yet, the contribution to Q4 revenue should be relatively limited. Is that a timing issue and could it be a source of reacceleration in growth for Data Center in April and beyond or are the price points such that the contribution to revenue going forward should be relatively limited? And then the question for Jensen, the AI foundry service announcement from last week. I just wanted to ask about that, and hopefully, have you expand on it. How is the monetization model going to work? Is it primarily services and software revenue? How should we think about the long term opportunity set? And is this going to be exclusive to Microsoft or do you have plans to expand to other partners as well? Thank you.\\nColette Kress: Thanks, Toshiya. On the question regarding potentially new products that we could provide to our China customers. It's a significant process to both design and develop these new products. As we discussed, we're going to make sure that we are in full discussions with the U.S. government of our intent to move products as well. Given our state about where we are in the quarter, we're already several weeks into the quarter. So it's just going to take some time for us to go through and discussing with our customers the needs and desires of these new products that we have. And moving forward, whether that's medium-term or long-term, it's just hard to say both the [Technical Difficulty] of what we can produce with the U.S. government and what the interest of our China customers in this. So we stay still focused on finding that right balance for our China customers, but it's hard to say at this time.\\nJensen Huang: Toshiya, thanks for the question. There is a glaring opportunity in the world for AI foundry, and it makes so much sense. First, every company has its core intelligence. It makes up our company. Our data, our domain expertise, in the case of many companies, we create tools, and most of the software companies in the world are tool platforms, and those tools are used by people today. And in the future, it's going to be used by people augmented with a whole bunch of AIs that we hire. And these platforms just got to go across the world and you'll see and we've only announced a few; SAP, ServiceNow, Dropbox, Getty, many others are coming. And the reason for that is because they have their own proprietary AI. They want their own proprietary AI. They can't afford to outsource their intelligence and handout their data, and handout their flywheel for other companies to build the AI for them. And so, they come to us. We have several things that are really essential in a foundry. Just as TSMC as a foundry, you have to have AI technology. And as you know, we have just an incredible depth of AI capability -- AI technology capability. And then second, you have to have the best practice known practice, the skills of processing data through the invention of AI models to create AIs that are guardrails, fine-tuned, so on and so forth, that are safe, so on and so forth. And the third thing is you need factories. And that's what DGX Cloud is. Our AI models are called AI Foundations. Our process, if you will, our CAD system for creating AIs are called NeMo and they run on NVIDIA's factories we call DGX Cloud. Our monetization model is that with each one of our partners they rent a sandbox on DGX Cloud, where we work together, they bring their data, they bring their domain expertise, we bring our researchers and engineers, we help them build their custom AI. We help them make that custom AI incredible. Then that custom AI becomes theirs. And they deploy it on the runtime that is enterprise grade, enterprise optimized or outperformance optimized, runs across everything NVIDIA. We have a giant installed base in the cloud, on-prem, anywhere. And it's secure, securely patched, constantly patched and optimized and supported. And we call that NVIDIA AI Enterprise. NVIDIA AI Enterprise is $4,500 per GP per year, that's our business model. Our business model is basically a license. Our customers then with that basic license can build their monetization model on top of. In a lot of ways we're wholesale, they become retail. They could have a per -- they could have subscription license base, they could per instance or they could do per usage, there is a lot of different ways that they could take a -- create their own business model, but ours is basically like a software license, like an operating system. And so our business model is help you create your custom models, you run those custom models on NVIDIA AI Enterprise. And it's off to a great start. NVIDIA AI Enterprise is going to be a very large business for us.\\nOperator: Your next question comes from the line of Stacy Rasgon of Bernstein Research. Your line is open.\\nStacy Rasgon: Hi, guys. Thanks for taking my questions. Colette, I wanted to know if it weren't for the China restrictions would the Q4 guide has been higher or are you supply-constrained in just reshipping stuff that would have gone to China elsewhere? And I guess along those lines you give us a feeling for where your lead times are right now in data center and just the China redirection such as-is, is it lowering those lead times, because you've got parts that are sort of immediately available to ship?\\nColette Kress: Yeah. Stacy, let me see if I can help you understand. Yes, there are still situations where we are working on both improving our supply each and every quarter. We've done a really solid job of ramping every quarter, which has defined our revenue. But with the absence of China for our outlook for Q4, sure, there could have been some things that we are not supply-constrained that we could have sold, but kind of we would no longer can. So could our guidance had been a little higher in our Q4? Yes. We are still working on improving our supply on plan, on continuing growing all throughout next year as well towards that.\\nOperator: Your next question comes from the line of Matt Ramsay of TD Cowen. Your line is open.\\nMatt Ramsay: Thank you very much. Congrats, everybody, on the results. Jensen, I had a two-part question for you, and it comes off of sort of one premise. And the premise is, I still get a lot of questions from investors thinking about AI training as being NVIDIA's dominant domain and somehow inference, even large model inference takes more and more of the TAM that the market will become more competitive. You'll be less differentiated et cetera., et cetera. So I guess the two parts of the question are: number one, maybe you could spend a little bit of time talking about the evolution of the inference workload as we move to LLMs and how your company is positioned for that rather than smaller model inference. And second, up until a month or two ago, I never really got any questions at all about the data processing piece of the AI workloads. So the pieces of manipulating the data before training, between training and inference, after inference and I think that's a large part of the workload now. Maybe you could talk about how CUDA is enabling acceleration of those pieces of the workload. Thanks.\\nJensen Huang: Sure. Inference is complicated. It's actually incredibly complicated. If you -- we this quarter announced one of the most exciting new engines, optimizing compilers called TensorRT-LLM. The reception has been incredible. You got to GitHub, it's been downloaded a ton, a whole lot of stars, integrated into stacks and frameworks all over the world, almost instantaneously. And there are several reasons for that, obviously. We could create TensorRT-LLM, because CUDA is programmable. If CUDA and our GPUs were not so programmable, it would really be hard for us to improve software stacks at the pace that we do. TensorRT-LLM, on the same GPU, without anybody touching anything, improves the performance by a factor of two. And then on top of that, of course, the pace of our innovation is so high. H200 increases it by another factor of two. And so, our inference performance, another way of saying inference cost, just reduced by a factor of four within about a year's time. And so, that's really, really hard to keep up with. The reason why everybody likes our inference engine is because our installed base. We've been dedicated to our installed base for 20 years, 20-plus years. We have an installed base that is not only largest in every single cloud, it's in every available from every enterprise system maker, it's used by companies of just about every industry. And every -- anytime you see a NVIDIA GPU, it runs our stack. It's architecturally compatible, something we've been dedicated to for a very long time. We're very disciplined about it. We make it our, if you will, architecture compatibility is job one. And that has conveyed to the world, the certainty of our platform stability. NVIDIA's platform stability certainty is the reason why everybody builds on us first and the reason why everybody optimizes on us first. All the engineering and all the work that you do, all the invention of technologies that you build on top of NVIDIA accrues to the -- and benefits everybody that uses our GPUs. And we have such a large installed base, large -- millions and millions of GPUs in cloud, 100 million GPUs from people’s PCs just about every workstation in the world, and they all architecturally compatible. And so, if you are an inference platform and you're deploying an inference application, you are basically an application provider. And as a software application provider, you're looking for large installed base. Data processing, before you could train a model, you have to curate the data, you have to dedupe the data, maybe you have to augment the data with synthetic data. So, process the data, clean the data, align the data, normalize the data, all of that data is measured not in bytes or megabytes, it's measured in terabytes and petabytes. And the amount of data processing that you do before data engineering, before that you do training is quite significant. It could represent 30%, 40%, 50% of the amount of work that you ultimately do. And what you -- and ultimately creating a data driven machine learning service. And so data processing is just a massive part. We accelerate Spark, we accelerate Python. One of the coolest things that we just did is called cuDF Pandas. Without one line of code, Pandas, which is the single most successful data science framework in the world. Pandas now is accelerated by NVIDIA CUDA. And just out-of-the box, without the line of code and so the acceleration is really quite terrific and people are just incredibly excited about it. And Pandas was designed for one purpose and one purpose only, really data processing, it's for data science. And so NVIDIA CUDA gives you all of that.\\nOperator: Your final question comes from the line of Harlan Sur of J.P. Morgan. Your line is open.\\nHarlan Sur: Good afternoon. Thanks for taking my question. If you look at the history of the tech industry like those companies that have been successful have always been focused on ecosystem; silicon, hardware, software, strong partnerships and just as importantly, right, an aggressive cadence of new products, more segmentation over time. The team recently announced a more aggressive new product cadence in data center from two years to now every year with higher levels of segmentation, training, optimization in printing CPU, GPU, DPU networking. How do we think about your R&D OpEx growth outlook to support a more aggressive and expanding forward roadmap, but more importantly, what is the team doing to manage and drive execution through all of this complexity?\\nJensen Huang: Gosh. Boy, that's just really excellent. You just wrote NVIDIA's business plan, and so you described our strategy. First of all, there is a fundamental reason why we accelerate our execution. And the reason for that is because it fundamentally drives down cost. When the combination of TensorRT-LLM and H200 reduce the cost for our customers for large model inference by a factor of four, and so that includes, of course, our speeds and feeds, but mostly it's because of our software, mostly the software benefits because of the architecture. And so we want to accelerate our roadmap for that reason. The second reason is to expand the reach of generative AI, the world's number of data center configurations -- this is kind of the amazing thing. NVIDIA is in every cloud, but not one cloud is the same. NVIDIA is working with every single cloud service provider and not one of the networking control plane, security posture is the same. Everybody's platform is different and yet we're integrated into all of their stacks, all of their data centers and we work incredibly well with all of them. And not to mention, we then take the whole thing and we create AI factories that are standalone. We take our platform, we can put them into supercomputers, we can put them into enterprise. Bringing AI to enterprise is something generative AI Enterprise something nobody's ever done before. And we're right now in the process of going to market with all of that. And so the complexity includes, of course, all the technologies and segments and the pace. It includes the fact that we are architecturally compatible across every single one of those. It includes all of the domain specific libraries that we create. The reason why every computer company, without thinking, can integrate NVIDIA into their roadmap and take it to market. And the reason for that is, because there is market demand for it. There is market demand in healthcare, there is market demand in manufacturing, there is market demand, of course, in AI, including financial services, in supercomputing and quantum computing. The list of markets and segments that we have domain specific libraries is incredibly broaden. And then finally, we have an end-to-end solution for data centers; InfiniBand networking, Ethernet networking, x86, ARM, just about every permutation combination of solutions -- technology solutions and software stacks provided. And that translates to having the largest number of ecosystem software developers; the largest ecosystem of system makers; the largest and broadest distribution partnership network; and ultimately, the greatest reach. And that takes -- surely that takes a lot of energy. But the thing that really holds it together, and this is a great decision that we made decades ago, which is everything is architecturally compatible. When we develop a domain specific language that runs on one GPU, it runs on every GPU. When we optimize TensorRT for the cloud, we optimized it for enterprise. When we do something that brings in a new feature, a new library, a new feature or a new developer, they instantly get the benefit of all of our reach. And so that discipline, that architecture compatible discipline that has lasted more than a couple of decades now, is one of the reasons why NVIDIA is still really, really efficient. I mean, we're 28,000 people large and serving just about every single company, every single industry, every single market around the world.\\nOperator: Thank you. I will now turn the call back over to Jensen Huang for closing remarks.\\nJensen Huang: Our strong growth reflects the broad industry platform transition from general purpose to accelerated computing and generative AI. Large language models start-ups consumer Internet companies and global cloud service providers are the first movers. The next waves are starting to build. Nations and regional CSPs are building AI clouds to serve local demand. Enterprise software companies like Adobe and Dropbox, SAP and ServiceNow are adding AI copilots and assistants to their platforms. Enterprises in the world's largest industries are creating custom AIs to automate and boost productivity. The generative AI era is in full steam and has created the need for a new type of data center, an AI factory; optimized for refining data and training, and inference, and generating AI. AI factory workloads are different and incremental to legacy data center workloads supporting IT tasks. AI factories run copilots and AI assistants, which are significant software TAM expansion and are driving significant new investment. Expanding the $1 trillion traditional data center infrastructure installed base, empowering the AI Industrial Revolution. NVIDIA H100 HGX with InfiniBand and the NVIDIA AI software stack define an AI factory today. As we expand our supply chain to meet the world's demand, we are also building new growth drivers for the next wave of AI. We highlighted three elements to our new growth strategy that are hitting their stride: CPU, networking, and software and services. Grace is NVIDIA's first data center CPU. Grace and Grace Hopper are in full production and ramping into a new multi-billion dollar product line next year. Irrespective of the CPU choice, we can help customers build an AI factory. NVIDIA networking now exceeds $10 billion annualized revenue run rate. InfiniBand grew five-fold year-over-year, and is positioned for excellent growth ahead as the networking of AI factories. Enterprises are also racing to adopt AI and Ethernet is the standard networking. This week we announced an Ethernet for AI platform for enterprises. NVIDIA Spectrum-X is an end-to-end solution of Bluefield SuperNIC, Spectrum-4 Ethernet switch and software that boosts Ethernet performance by up to 1.6x for AI workloads. Dell, HPE and Lenovo have joined us to bring a full generative AI solution of NVIDIA AI computing, networking and software to the world's enterprises. NVIDIA software and services is on track to exit the year at an annualized run rate of $1 billion. Enterprise software platforms like ServiceNow and SAP need to build and operate proprietary AI. Enterprises need to build and deploy custom AI copilots. We have the AI technology, expertise and scale to help customers build custom models with their proprietary data on NVIDIA DGX Cloud and deploy the AI applications on enterprise grade NVIDIA AI Enterprise. NVIDIA is essentially an AI foundry. NVIDIA's GPUs, CPUs, networking, AI foundry services and NVIDIA AI Enterprise software are all growth engines in full throttle. Thanks for joining us today. We look forward to updating you on our progress next quarter.\\nOperator: This concludes today's conference call. You may now disconnect.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " \"NVIDIA's latest earnings call revealed record-breaking revenue of $18.1 billion, marking a 34% sequential increase and a 200% year-on-year increase. The company's Data Center segment was a primary driver of this growth, achieving record revenue of $14.5 billion. However, the company anticipates a significant decline in the fourth quarter due to new U.S. export control regulations affecting sales to China and other markets. \\nKey takeaways from the call include:NVIDIA's Data Center segment saw record revenue, driven by the NVIDIA HGX platform and InfiniBand networking. Major companies like Meta (NASDAQ:META) and Adobe (NASDAQ:ADBE) are integrating NVIDIA's technology into their platforms, contributing to the strong demand for AI supercomputers and data center infrastructure.NVIDIA is expanding its Data Center product portfolio to comply with new U.S. export control regulations. Despite expecting a decrease in sales to China and other markets in the fourth quarter, the company is focusing on national investment in sovereign AI infrastructure.The Gaming segment recorded revenue of $2.86 billion, a 15% sequential increase and an 80% year-on-year increase, driven by strong demand for NVIDIA's RTX ray tracing and AI technology.In the Pro Vis segment, revenue reached $416 million, a 10% sequential increase and a 108% year-on-year increase. NVIDIA RTX is the preferred platform for professional design, engineering, and simulation, with AI emerging as a demand driver.NVIDIA's CEO, Jensen Huang, discussed the expansion of AI factories and the transition into a new computing approach. He also expressed confidence in the continued growth of the Data Center business through 2025.During the call, NVIDIA highlighted its product innovations, including the H200 GPU, which offers faster and larger memory for generative AI and language models, and the GH200 Grace Hopper Superchip, a combination of an ARM-based CPU with a Hopper GPU. The Grace Hopper Superchip is being adopted by supercomputing customers, with the combined AI compute capacity of supercomputers built on Grace Hopper expected to exceed 200 exaflops next year. \\nIn addition to its product offerings, NVIDIA is also expanding its networking into the Ethernet space with its new Spectrum-X end-to-end Ethernet offering, which will be available in Q1 next year. This offering can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. \\nNVIDIA also discussed its business strategy, emphasizing the importance of reselling AI technology to customers and its growth in the enterprise market. The company introduced an AI foundry service to help enterprises build custom AI models and announced an aggressive product release schedule in the data center market. \\nLooking ahead, NVIDIA expects total revenue of $20 billion for Q4 2024, driven by strong demand in the Data Center sector. Despite regulatory challenges, the company is confident in its growth potential, citing expanded supply, new customers, and its entrance into the enterprise market.Full transcript -  Nvidia Corp  (NASDAQ:NVDA) Q3 2024:Operator: Good afternoon. My name is JL, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's Third Quarter Earnings Call. All lines have been placed on mute to prevent any background noise. After the speakers’ remarks, there will be a question-and-answer session. [Operator Instructions] Simona Jankowski, you may now begin your conference.\\nSimona Jankowski: Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2024. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter and fiscal 2024. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All statements are made as of today, November 21, 2023, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\\nColette Kress: Thanks, Simona. Q3 was another record quarter. Revenue of $18.1 billion was up 34% sequentially and up more than 200% year-on-year and well above our outlook for $16 billion. Starting with Data Center. The continued ramp of the NVIDIA HGX platform based on our Hopper Tensor Core GPU architecture, along with InfiniBand end-to-end networking, drove record revenue of $14.5 billion, up 41% sequentially and up 279% year-on-year. NVIDIA HGX with InfiniBand together are essentially the reference architecture for AI supercomputers and data center infrastructures. Some of the most exciting generative AI applications are built and run on NVIDIA, including Adobe Firefly, ChatGPT, Microsoft (NASDAQ:MSFT) 365 Copilot, CoAssist, now assist with ServiceNow (NYSE:NOW) and Zoom (NASDAQ:ZM) AI Companion. Our Data Center compute revenue quadrupled from last year and networking revenue nearly tripled. Investments in infrastructure for training and inferencing large language models, deep learning, recommender systems and generative AI applications is fueling strong broad-based demand for NVIDIA accelerated computing. Inferencing is now a major workload for NVIDIA AI computing. Consumer Internet companies and enterprises drove exceptional sequential growth in Q3, comprising approximately half of our Data Center revenue and outpacing total growth. Companies like Meta are in full production with deep learning, recommender systems and also investing in generative AI to help advertisers optimize images and text. Most major consumer Internet companies are racing to ramp up generative AI deployment. The enterprise wave of AI adoption is now beginning. Enterprise software companies such as Adobe, Databricks, Snowflake (NYSE:SNOW) and ServiceNow are adding AI copilots and the systems to their platforms. And broader enterprises are developing custom AI for vertical industry applications such as Tesla (NASDAQ:TSLA) in autonomous driving. Cloud service providers drove roughly the other half of our Data Center revenue in the quarter. Demand was strong from all hyperscale CSPs, as well as from a broadening set of GPU-specialized CSPs globally that are rapidly growing to address the new market opportunities in AI. NVIDIA H100 Tensor Core GPU instances are now generally available in virtually every cloud with instances in high demand. We have significantly increased supply every quarter this year to meet strong demand and expect to continue to do so next year. We will also have a broader and faster product launch cadence to meet the growing and diverse set of AI opportunities. Towards the end of the quarter, the U.S. government announced a new set of export control regulations for China and other markets, including Vietnam and certain countries in the Middle East. These regulations require licenses for the export of a number of our products, including our Hopper and Ampere 100 and 800 series and several others. Our sales to China and other affected destinations derived from products that are now subject to licensing requirements have consistently contributed approximately 20% to 25% of Data Center revenue over the past few quarters. We expect that our sales to these destinations will decline significantly in the fourth quarter. So we believe will be more than offset by strong growth in other regions. The U.S. government designed the regulation to allow the U.S. industry to provide data center compute products to markets worldwide, including China. Continuing to compete worldwide as the regulations encourage, promotes U.S. technology leadership, spurs economic growth and supports U.S. jobs. For the highest performance levels, the government requires licenses. For lower performance levels, the government requires a streamlined prior notification process. And for products even lower performance levels, the government does not require any notice at all. Following the government's clear guidelines, we are working to expand our Data Center product portfolio to offer compliance solutions for each regulatory category, including products for which the U.S. government does not wish to have advance notice before each shipment. We are working with some customers in China and the Middle East to pursue licenses from the U.S. government. It is too early to know whether these will be granted for any significant amount of revenue. Many countries are awakening to the need to invest in sovereign AI infrastructure to support economic growth and industrial innovation. With investments in domestic compute capacity, nations can use their own data to train LLMs and support their local generative AI ecosystems. For example, we are working with India's government and largest tech companies including  Infosys  (NS:INFY), Reliance and Tata to boost their sovereign AI infrastructure. And French private cloud provider, Scaleway, is building a regional AI cloud based on NVIDIA H100 InfiniBand and NVIDIA's AI Enterprise software to fuel advancement across France and Europe. National investment in compute capacity is a new economic imperative and serving the sovereign AI infrastructure market represents a multi-billion dollar opportunity over the next few years. From a product perspective, the vast majority of revenue in Q3 was driven by the NVIDIA HGX platform based on our Hopper GPU architecture with lower contribution from the prior generation Ampere GPU architecture. The new L40S GPU built for industry standard servers began to ship, supporting training and inference workloads across a variety of consumers. This was also the first revenue quarter of our GH200 Grace Hopper Superchip, which combines our ARM-based Grace CPU with a Hopper GPU. Grace and Grace Hopper are ramping into a new multi-billion dollar product line. Grace Hopper instances are now available at GPU specialized cloud providers, and coming soon to Oracle (NYSE:ORCL) Cloud. Grace Hopper is also getting significant traction with supercomputing customers. Initial shipments to Los Alamos National Lab and the Swiss National Supercomputing Center took place in the third quarter. The UK government announced it will build one of the world's fastest AI supercomputers called Isambard-AI with almost 5,500 Grace Hopper Superchips. German supercomputing center, Julich, also announced that it will build its next-generation AI supercomputer with close to 24,000 Grace Hopper Superchips and Quantum-2 InfiniBand, making it the world's most powerful AI supercomputer with over 90 exaflops of AI performance. All-in, we estimate that the combined AI compute capacity of all the supercomputers built on Grace Hopper across the U.S., Europe and Japan next year will exceed 200 exaflops with more wins to come. Inference is contributing significantly to our data center demand, as AI is now in full production for deep learning, recommenders, chatbots, copilots and text to image generation and this is just the beginning. NVIDIA AI offers the best inference performance and versatility, and thus the lower power and cost of ownership. We are also driving a fast cost reduction curve. With the release of TensorRT-LLM, we now achieved more than 2x the inference performance for half the cost of inferencing LLMs on NVIDIA GPUs. We also announced the latest member of the Hopper family, the H200, which will be the first GPU to offer HBM3e, faster, larger memory to further accelerate generative AI and LLMs. It moves inference speed up to another 2x compared to H100 GPUs for running LLMs like Norma2 (ph). Combined, TensorRT-LLM and H200, increased performance or reduced cost by 4x in just one year. With our customers changing their stack, this is a benefit of CUDA and our architecture compatibility. Compared to the A100, H200 delivers an 18x performance increase for inferencing models like GPT-3, allowing customers to move to larger models and with no increase in latency. Amazon (NASDAQ:AMZN) Web Services, Google (NASDAQ:GOOGL) Cloud, Microsoft Azure and Oracle Cloud will be among the first CSPs to offer H200-based instances starting next year. At last week's Microsoft Ignite, we deepened and expanded our collaboration with Microsoft across the entire stock. We introduced an AI foundry service for the development and tuning of custom generative AI enterprise applications running on Azure. Customers can bring their domain knowledge and proprietary data and we help them build their AI models using our AI expertise and software stock in our DGX cloud, all with enterprise grade security and support. SAP and  Amdocs  (NASDAQ:DOX) are the first customers of the NVIDIA AI foundry service on Microsoft Azure. In addition, Microsoft will launch new confidential computing instances based on the H100. The H100 remains the top performing and most versatile platform for AI training and by a wide margin, as shown in the latest MLPerf industry benchmark results. Our training cluster included more than 10,000 H100 GPUs or 3x more than in June, reflecting very efficient scaling. Efficient scaling is a key requirement in generative AI, because LLMs are growing by an order of magnitude every year. Microsoft Azure achieved similar results on a nearly identical cluster, demonstrating the efficiency of NVIDIA AI in public cloud deployments. Networking now exceeds a $10 billion annualized revenue run rate. Strong growth was driven by exceptional demand for InfiniBand, which grew fivefold year-on-year. InfiniBand is critical to gaining the scale and performance needed for training LLMs. Microsoft made this very point last week, highlighting that Azure uses over 29,000 miles of InfiniBand cabling, enough to circle the globe. We are expanding NVIDIA networking into the Ethernet space. Our new Spectrum-X end-to-end Ethernet offering with technologies, purpose built for AI, will be available in Q1 next year. With support from leading OEMs, including Dell (NYSE:DELL), HPE and Lenovo. Spectrum-X can achieve 1.6x higher networking performance for AI communication compared to traditional Ethernet offerings. Let me also provide an update on our software and services offerings, where we are starting to see excellent adoption. We are on track to exit the year at an annualized revenue run rate of $1 billion for our recurring software, support and services offerings. We see two primary opportunities for growth over the intermediate term with our DGX cloud service and with our NVIDIA AI Enterprise software, each reflects the growth of enterprise AI training and enterprise AI inference, respectively. Our latest DGX cloud customer announcement was this morning as part of an AI research collaboration with Gentech, the biotechnology pioneer also plans to use our BioNeMo LLM framework to help accelerate and optimize their AI drug discovery platform. We now have enterprise AI partnership with Adobe, Dropbox (NASDAQ:DBX), Getty, SAP, ServiceNow, Snowflake and others to come. Okay. Moving to Gaming. Gaming revenue of $2.86 billion was up 15% sequentially and up more than 80% year-on-year with strong demand in the important back-to-school shopping season with NVIDIA RTX ray tracing and AI technology now available at price points as low as $299. We entered the holidays with the best-ever line-up for gamers and creators. Gaming has doubled relative to pre-COVID levels even against the backdrop of lackluster PC market performance. This reflects the significant value we've brought to the gaming ecosystem with innovations like RTX and DLSS. The number of games and applications supporting these technologies has exploded in that period, driving upgrades and attracting new buyers. The RTX ecosystem continues to grow. There are now over 475 RTX-enabled games and applications. Generative AI is quickly emerging as the new pillar app for high performance PCs. NVIDIA RTX GPUs to find the most performance AI PCs and workstations. We just released TensorRT-LLM for Windows, which speeds on-device LLM inference up by 4x. With an installed base of over 100 million, NVIDIA RTX is the natural platform for AI application developers. Finally, our GeForce NOW cloud gaming service continues to build momentum. Its library of PC games surpassed 1,700 titles, including the launches of Alan Wake 2, Baldur's Gate 3, Cyberpunk 2077: Phantom Liberty and Starfield. Moving to the Pro Vis. Revenue of $416 million was up 10% sequentially and up 108% year-on year. NVIDIA RTX is the workstation platform of choice for professional design, engineering and simulation use cases and AI is emerging as a powerful demand driver. Early applications include inference for AI imaging in healthcare and edge AI in smart spaces and the public sector. We launched a new line of desktop workstations based on NVIDIA RTX Ada Lovelace generation GPUs and ConnectX, SmartNICs offering up to 2x the AI processing ray tracing and graphics performance of the previous generations. These powerful new workstations are optimized for AI workloads such as fine tune AI models, training smaller models and running inference locally. We continue to make progress on Omniverse, our software platform for designing, building and operating 3D virtual worlds. Mercedes-Benz (OTC:MBGAF) is using Omniverse powered digital twins to plan, design, build and operate its manufacturing and assembly facilities, helping it increase efficiency and reduce defects. Oxxon (ph) is also incorporating Omniverse into its manufacturing process, including end-to-end simulation for the entire robotics and automation pipeline, saving time and cost. We announced two new Omniverse Cloud services for automotive digitalization available on Microsoft Azure, a virtual factory simulation engine and autonomous vehicle simulation engine. Moving to Automotive. Revenue was $261 million, up 3% sequentially and up 4% year-on year, primarily driven by continued growth in self-driving platforms based on NVIDIA DRIVE Orin SOC and the ramp of AI cockpit solutions with global OEM customers. We extended our automotive partnership of Foxconn to include NVIDIA DRIVE for our next-generation automotive SOC. Foxconn has become the ODM for EVs. Our partnership provides Foxconn with a standard AV sensor and computing platform for their customers to easily build a state-of-an-art safe and secure software defined car. Now we're going to move to the rest of the P&L. GAAP gross margin expanded to 74% and non-GAAP gross margin to 75%, driven by higher Data Center sales and lower net inventory reserve, including a 1 percentage point benefit from the release of previously reserved inventory related to the Ampere GPU architecture products. Sequentially, GAAP operating expenses were up 12% and non-GAAP operating expenses were up 10%, primarily reflecting increased compensation and benefits. Let me turn to the fourth quarter of fiscal 2024. Total revenue is expected to be $20 billion, plus or minus 2%. We expect strong sequential growth to be driven by Data Center, with continued strong demand for both compute and networking. Gaming will likely decline sequentially as it is now more aligned with notebook seasonality. GAAP and non-GAAP gross margins are expected to be 74.5% and 75.5%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $3.17 billion and $2.2 billion, respectively. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $200 million, excluding gains and losses from non-affiliated investments. GAAP and non-GAAP tax rates are expected to be 15%, plus or minus 1% excluding any discrete items. Further financial information are included in the CFO commentary and other information available on our IR website. In closing, let me highlight some upcoming events for the financial community. We will attend the UBS Global Technology Conference in Scottsdale, Arizona, on November 28; the Wells Fargo TMT Summit in Rancho Palos Verdes, California on November 29; the Arete Virtual Tech Conference on December 7; and the J.P. Morgan Health Care Conference in San Francisco on January 8. Our earnings call to discuss the results of our fourth quarter and fiscal 2024 is scheduled for Wednesday, February 21. We will now open the call for questions. Operator, will you please poll for questions.\\nOperator: [Operator Instructions] Your first question comes from the line of Vivek Arya of Bank of America. Your line is open.\\nVivek Arya: Thanks for taking my question. Just, Colette, wanted to clarify what China contributions are you expecting in Q4. And then, Jensen, the main question is for you, where do you think we are in the adoption curve in terms of your shipments into the generative AI market? Because when I just look at the trajectory of your data center, is growth -- it will be close to nearly 30% of all the spending in data center next year. So what metrics are you keeping an eye on to inform you that you can continue to grow? Just where are we in the adoption curve of your products into the generative AI market? Thank you.\\nColette Kress: So, first let me start with your question, Vivek, on export controls and the impacts that we are seeing in our Q4 outlook and guidance that we provided. We had seen historically over the last several quarters that China and some of the other impacted destinations to be about 20% to 25% of our Data Center revenue. We are expecting in our guidance for that to decrease substantially as we move into Q4. The export controls will have a negative effect on our China business. And we do not have good visibility into the magnitude of that impact even over the long-term. We are though working to expand our Data Center product portfolio to possibly offer new regulation compliance solutions that do not require a license, these products, they may become available in the next coming months. However, we don't expect their contribution to be material or meaningful as a percentage of the revenue in Q4.\\nJensen Huang: Generative AI is the largest TAM expansion of software and hardware that we've seen in several decades. At the core of it, what's really exciting is that, what was largely a retrieval based computing approach, almost everything that you do is retrieved off of storage somewhere, has been augmented now, added with a generative method. And it's changed almost everything. You could see that text-to-text, text-to-image, text-to-video, text-to-3D, text-to-protein, text-to-chemicals, these were things that were processed and typed in by humans in the past. And these are now generative approaches. The way that we access data is changed. It used to be based on explicit queries. It is now based on natural language queries, intention queries, semantic queries. And so, we're excited about the work that we're doing with SAP and Dropbox and many others that you're going to hear about. And one of the areas that is really impactful is the software industry, which is about $1 trillion or so, has been building tools that are manually used over the last couple of decades. And now there's a whole new segment of software called copilots and assistants. Instead of manually used, these tools will have copilots to help you use it. And so, instead of licensing software, we will continue to do that, of course, but we will also hire copilots and assistants to help us use these -- use the software. We'll connect all of these copilots and assistants into teams of AIs, which is going to be the modern version of software, modern version of enterprise business software. And so the transformation of software and the way that software has done is driving the hardware underneath. And you can see that it's transforming hardware in two ways. One is something that's largely independent of generative AI. There's two trends: one is related to accelerated computing, general purpose computing is too wasteful of energy and cost. And now that we have much, much better approaches, call it, accelerated computing, you could save an order of magnitude of energy, you can save an order of magnitude of time or you can save an order of magnitudes of cost by using acceleration. And so, accelerated computing is transitioning, if you will, general purpose computing into this new approach. And that's been augmented by a new class of data centers. This is the traditional data centers that you were just talking about where we represent about a third of that. But there is a new class of data centers and this new class of data centers, unlike the data centers of the past, where you have a lot of applications running used by a great many people that are different tenants that are using the same infrastructure and that data center stores a lot of files. These new data centers are very few applications, if not one application, used by basically one tenant and it processes data, it trains models and then generates tokens and generates AI. And we call these new data centers AI factories. We're seeing AI factories being built out everywhere, and just by every country. And so if you look at the way where we are in the expansion, the transition into this new computing approach, the first wave you saw with large language model start-ups, generative AI start-ups and consumer Internet companies, and weren't in the process of ramping that. Meanwhile, while that's being ramped, you see that we're starting to partner with enterprise software companies who would like to build chatbots and copilots and assistants to augment the tools that they have on their platforms. You're seeing GPU specialized CSPs cropping up all over the world and they are dedicated to do really one thing, which is processing AI. You're seeing sovereign AI infrastructures, people -- countries that now recognize that they have to utilize their own data, keep their own data, keep their own culture, process that data and develop their own AI. You see that in India. Several -- about a year ago in Sweden, you are seeing in Japan. Last week, a big announcement in France. But the number of sovereign AI clouds that are being built is really quite significant. And my guess is that almost every major region will have and surely every major country will have their own AI clouds. And so I think you're seeing just new developments as the generative AI wave propagates through every industry, every company, every region. And so we're at the beginning of this inflection, this computing transition.\\nOperator: Your next question comes from the line of Aaron Rakers of Wells Fargo. Your line is open.\\nAaron Rakers: Yeah. Thanks for taking the question. I wanted to ask about kind of the networking side of the business. Given the growth rates that you've now cited, I think, it's 155% year-over-year and strong growth sequentially, it looks like that business is like almost approaching $2.5 billion to $3 billion quarterly level. I'm curious of how you see Ethernet involved evolving and maybe how you would characterize your differentiation of Spectrum-X relative to the traditional Ethernet stack as we start to think about that becoming part of the networking narrative above and maybe beyond just InfiniBand as we look into next year? Thank you.\\nJensen Huang: Yeah. Thanks for the question. Our networking business is already on a $10 billion plus run rate and it's going to get much larger. And as you mentioned, we added a new networking platform to our networking business recently. The vast majority of the dedicated large scale AI factories standardize on InfiniBand. And the reason for that is not only because of its data rate and not only just the latency, but the way that it moves traffic around the network is really important. The way that you process AI and a multi-tenant hyperscale Ethernet environment, the traffic pattern is just radically different. And with InfiniBand and with software defined networks, we could do congestion control, adaptive routing, performance isolation and noise isolation, not to mention, of course, the day rate and the low latency that -- and a very low overhead of InfiniBand that's natural part of InfiniBand. And so, InfiniBand is not so much just the network, it's also a computing fabric. We've put a lot of software-defined capabilities into the fabric including computation. We will do 40-point calculations and computation right on the switch, and right in the fabric itself. And so that's the reason why that difference in Ethernet versus InfiniBand or InfiniBand versus Ethernet for AI factories is so dramatic. And the difference is profound. And the reason for that is because you've just invested in a $2 billion infrastructure for AI factories. A 20%, 25%, 30% difference in overall effectiveness, especially as you scale up is measured in hundreds of millions of dollars of value. And if you will, renting that infrastructure over the course of four to five years, it really, really adds up. And so InfiniBand's value proposition is undeniable for AI factories. However, as we move AI into enterprise. This is enterprise computing what we'd like to enable every company to be able to build their own custom AIs. We're building customer AIs in our company based on our proprietary data, our proprietary type of skills. For example, recently we spoke about one of the models that we're creating, it's called ChipNeMo; we're building many others. There'll be tens, hundreds of custom AI models that we create inside our company. And our company is -- for all of our employee use, doesn't have to be as high performance as the AI factories we used to train the models. And so we would like the AI to be able to run in Ethernet environment. And so what we've done is we invented this new platform that extends Ethernet; doesn't replace Ethernet, it's 100% compliant with Ethernet. And it's optimized for East-West traffic, which is where the computing fabric is. It adds to Ethernet with an end-to-end solution with Bluefield, as well as our Spectrum switch that allows us to perform some of the capabilities that we have in InfiniBand, not all but some. And we achieved excellent results. And the way we go to market is we go to market with our large enterprise partners who already offer our computing solution. And so, HP (NYSE:HPQ), Dell and Lenovo has the NVIDIA AI stack, the NVIDIA AI Enterprise software stack and now they integrate with Bluefield, as well as bundle -- take a market there, Spectrum switch, and they'll be able to offer enterprise customers all over the world with their vast sales force and vast network of resellers a fully integrated, if you will, fully optimized, at least end-to-end AI solution. And so that's basically it, bringing AI to Ethernet for the world's enterprise.\\nOperator: Thank you. Your next question comes from the line of Joe Moore of Morgan Stanley. Your line is open.\\nJoseph Moore: Great. Thank you. I'm wondering if you could talk a little bit more about Grace Hopper and how you see the ability to leverage kind of the microprocessor, how you see that as a TAM expander. And what applications do you see using Grace Hopper versus more traditional H100 applications?\\nJensen Huang: Yeah. Thanks for the question. Grace Hopper is in production -- in high volume production now. We're expecting next year just with all of the design wins that we have in high performance computing and AI infrastructures, we are on a very, very fast ramp with our first data center CPU to a multi-billion dollar product line. This is going to be a very large product line for us. The capability of Grace Hopper is really quite spectacular. It has the ability to create computing nodes that simultaneously has very fast memory, as well as very large memory. In the areas of vector databases or semantic surge, what is called RAG, retrieval augmented generation. So that you could have a generative AI model be able to refer to proprietary data or a factual data before it generates a response, that data is quite large. And you can also have applications or generative models where the context length is very high. You basically store it in entire book into end-to-end system memory before you ask your questions. And so the context length can be quite large this way. The generative models has the ability to still be able to naturally interact with you on one hand. On the other hand, be able to refer to factual data, proprietary data or domain-specific data, you data and be contextually relevant and reduce hallucination. And so that particular use case for example is really quite fantastic for Grace Hopper. It also serves the customers that really care to have a different CPU than x86. Maybe it's a European supercomputing centers or European companies who would like to build up their own ARM ecosystem and like to build up a full stack or CSPs that have decided that they would like to pivot to ARM, because their own custom CPUs are based on ARM. There are variety of different reasons that drives the success of Grace Hopper, but we're off to a just an extraordinary start. This is a home run product.\\nOperator: Your next question comes from the line of Tim Arcuri of UBS. Your line is open.\\nTim Arcuri: Hi. Thanks. I wanted to ask a little bit about the visibility that you have on revenue. I know there's a few moving parts. I guess, on one hand, the purchase commitments went up a lot again. But on the other hand, China bans would arguably pull in when you can fill the demand beyond China. So I know we're not even into 2024 yet and it doesn't sound like, Jensen, you think that next year would be a peak in your Data Center revenue, but I just wanted to sort of explicitly ask you that. Do you think that Data Center can grow even in 2025? Thanks.\\nJensen Huang: Absolutely believe the Data Center can grow through 2025. And there are, of course, several reasons for that. We are expanding our supply quite significantly. We have already one of the broadest and largest and most capable supply chain in the world. Now, remember, people think that the GPU is a chip. But the HGX H100, the Hopper HGX has 35,000 parts, it weighs 70 pounds. Eight of the chips are Hopper. The other 35,000 are not. It is -- even its passive components are incredible. High voltage parts. High frequency parts. High current parts. It is a supercomputer, and therefore, the only way to test a supercomputer is with another supercomputer. Even the manufacturing of it is complicated, the testing of it is complicated, the shipping of it complicated and installation is complicated. And so, every aspect of our HGX supply chain is complicated. And the remarkable team that we have here has really scaled out the supply chain incredibly. Not to mention, all of our HGXs are connected with NVIDIA networking. And the networking, the transceivers, the mix, the cables, the switches, the amount of complexity there is just incredible. And so, I'm just -- first of all, I'm just super proud of the team for scaling up this incredible supply chain. We are absolutely world class. But meanwhile, we're adding new customers and new products. So we have new supply. We have new customers, as I was mentioning earlier. Different regions are standing up GPU specialist clouds, sovereign AI clouds coming out from all over the world, as people realize that they can't afford to export their country's knowledge, their country's culture for somebody else to then resell AI back to them, they have to -- they should, they have the skills and surely with us in combination, we can help them to do that build up their national AI. And so, the first thing that they have to do is, create their AI cloud, national AI cloud. You're also seeing us now growing into enterprise. The enterprise market has two paths. One path -- or if I could say three paths. The first path, of course, just off-the-shelf AI. And there are of course Chat GPT, a fabulous off-the-shelf AI, there'll be others. There's also a proprietary AI, because software companies like ServiceNow and SAP, there are many, many others that can't afford to have their company's intelligence be outsourced to somebody else. And they are about building tools and on top of their tools they should build custom and proprietary and domain-specific copilots and assistants that they can then rent to their customer base. This is -- they're sitting on a goldmine, almost every major tools company in the world is sitting on a goldmine, and they recognize that they have to go build their own custom AIs. We have a new service called an AI foundry, where we leverage NVS (ph) capabilities to be able to serve them in that. And then the next one is enterprises building their own custom AIs, their own custom chatbots, their own custom RAGs. And this capability is spreading all over the world. And the way that we're going to serve that marketplace is with the entire stacks of systems, which includes our compute, our networking and our switches, running our software stack called NVIDIA AI Enterprise, taking it through our market partners, HP, Dell, Lenovo, so on and so forth. And so we're just -- we're seeing the waves of generative AI starting from the start-ups and CSPs, moving to consumer Internet companies, moving to enterprise software platforms, moving to enterprise companies. And then ultimately, one of the areas that you guys have seen us spend a lot of energy on has to do with industrial generative AI. This is where NVIDIA AI and NVIDIA Omniverse comes together and that is a really, really exciting work. And so I think the -- we're at the beginning of a basically across-the-board industrial transition to generative AI to accelerated computing. This is going to affect every company, every industry, every country.\\nOperator: Your next question comes from the line of Toshiya Hari of Goldman Sachs. Your line is open.\\nToshiya Hari: Hi. Thank you. I wanted to clarify something with Colette real quick, and then I had a question for Jensen as well. Colette, you mentioned that you'll be introducing regulation-compliant products over the next couple of months. Yet, the contribution to Q4 revenue should be relatively limited. Is that a timing issue and could it be a source of reacceleration in growth for Data Center in April and beyond or are the price points such that the contribution to revenue going forward should be relatively limited? And then the question for Jensen, the AI foundry service announcement from last week. I just wanted to ask about that, and hopefully, have you expand on it. How is the monetization model going to work? Is it primarily services and software revenue? How should we think about the long term opportunity set? And is this going to be exclusive to Microsoft or do you have plans to expand to other partners as well? Thank you.\\nColette Kress: Thanks, Toshiya. On the question regarding potentially new products that we could provide to our China customers. It's a significant process to both design and develop these new products. As we discussed, we're going to make sure that we are in full discussions with the U.S. government of our intent to move products as well. Given our state about where we are in the quarter, we're already several weeks into the quarter. So it's just going to take some time for us to go through and discussing with our customers the needs and desires of these new products that we have. And moving forward, whether that's medium-term or long-term, it's just hard to say both the [Technical Difficulty] of what we can produce with the U.S. government and what the interest of our China customers in this. So we stay still focused on finding that right balance for our China customers, but it's hard to say at this time.\\nJensen Huang: Toshiya, thanks for the question. There is a glaring opportunity in the world for AI foundry, and it makes so much sense. First, every company has its core intelligence. It makes up our company. Our data, our domain expertise, in the case of many companies, we create tools, and most of the software companies in the world are tool platforms, and those tools are used by people today. And in the future, it's going to be used by people augmented with a whole bunch of AIs that we hire. And these platforms just got to go across the world and you'll see and we've only announced a few; SAP, ServiceNow, Dropbox, Getty, many others are coming. And the reason for that is because they have their own proprietary AI. They want their own proprietary AI. They can't afford to outsource their intelligence and handout their data, and handout their flywheel for other companies to build the AI for them. And so, they come to us. We have several things that are really essential in a foundry. Just as TSMC as a foundry, you have to have AI technology. And as you know, we have just an incredible depth of AI capability -- AI technology capability. And then second, you have to have the best practice known practice, the skills of processing data through the invention of AI models to create AIs that are guardrails, fine-tuned, so on and so forth, that are safe, so on and so forth. And the third thing is you need factories. And that's what DGX Cloud is. Our AI models are called AI Foundations. Our process, if you will, our CAD system for creating AIs are called NeMo and they run on NVIDIA's factories we call DGX Cloud. Our monetization model is that with each one of our partners they rent a sandbox on DGX Cloud, where we work together, they bring their data, they bring their domain expertise, we bring our researchers and engineers, we help them build their custom AI. We help them make that custom AI incredible. Then that custom AI becomes theirs. And they deploy it on the runtime that is enterprise grade, enterprise optimized or outperformance optimized, runs across everything NVIDIA. We have a giant installed base in the cloud, on-prem, anywhere. And it's secure, securely patched, constantly patched and optimized and supported. And we call that NVIDIA AI Enterprise. NVIDIA AI Enterprise is $4,500 per GP per year, that's our business model. Our business model is basically a license. Our customers then with that basic license can build their monetization model on top of. In a lot of ways we're wholesale, they become retail. They could have a per -- they could have subscription license base, they could per instance or they could do per usage, there is a lot of different ways that they could take a -- create their own business model, but ours is basically like a software license, like an operating system. And so our business model is help you create your custom models, you run those custom models on NVIDIA AI Enterprise. And it's off to a great start. NVIDIA AI Enterprise is going to be a very large business for us.\\nOperator: Your next question comes from the line of Stacy Rasgon of Bernstein Research. Your line is open.\\nStacy Rasgon: Hi, guys. Thanks for taking my questions. Colette, I wanted to know if it weren't for the China restrictions would the Q4 guide has been higher or are you supply-constrained in just reshipping stuff that would have gone to China elsewhere? And I guess along those lines you give us a feeling for where your lead times are right now in data center and just the China redirection such as-is, is it lowering those lead times, because you've got parts that are sort of immediately available to ship?\\nColette Kress: Yeah. Stacy, let me see if I can help you understand. Yes, there are still situations where we are working on both improving our supply each and every quarter. We've done a really solid job of ramping every quarter, which has defined our revenue. But with the absence of China for our outlook for Q4, sure, there could have been some things that we are not supply-constrained that we could have sold, but kind of we would no longer can. So could our guidance had been a little higher in our Q4? Yes. We are still working on improving our supply on plan, on continuing growing all throughout next year as well towards that.\\nOperator: Your next question comes from the line of Matt Ramsay of TD Cowen. Your line is open.\\nMatt Ramsay: Thank you very much. Congrats, everybody, on the results. Jensen, I had a two-part question for you, and it comes off of sort of one premise. And the premise is, I still get a lot of questions from investors thinking about AI training as being NVIDIA's dominant domain and somehow inference, even large model inference takes more and more of the TAM that the market will become more competitive. You'll be less differentiated et cetera., et cetera. So I guess the two parts of the question are: number one, maybe you could spend a little bit of time talking about the evolution of the inference workload as we move to LLMs and how your company is positioned for that rather than smaller model inference. And second, up until a month or two ago, I never really got any questions at all about the data processing piece of the AI workloads. So the pieces of manipulating the data before training, between training and inference, after inference and I think that's a large part of the workload now. Maybe you could talk about how CUDA is enabling acceleration of those pieces of the workload. Thanks.\\nJensen Huang: Sure. Inference is complicated. It's actually incredibly complicated. If you -- we this quarter announced one of the most exciting new engines, optimizing compilers called TensorRT-LLM. The reception has been incredible. You got to GitHub, it's been downloaded a ton, a whole lot of stars, integrated into stacks and frameworks all over the world, almost instantaneously. And there are several reasons for that, obviously. We could create TensorRT-LLM, because CUDA is programmable. If CUDA and our GPUs were not so programmable, it would really be hard for us to improve software stacks at the pace that we do. TensorRT-LLM, on the same GPU, without anybody touching anything, improves the performance by a factor of two. And then on top of that, of course, the pace of our innovation is so high. H200 increases it by another factor of two. And so, our inference performance, another way of saying inference cost, just reduced by a factor of four within about a year's time. And so, that's really, really hard to keep up with. The reason why everybody likes our inference engine is because our installed base. We've been dedicated to our installed base for 20 years, 20-plus years. We have an installed base that is not only largest in every single cloud, it's in every available from every enterprise system maker, it's used by companies of just about every industry. And every -- anytime you see a NVIDIA GPU, it runs our stack. It's architecturally compatible, something we've been dedicated to for a very long time. We're very disciplined about it. We make it our, if you will, architecture compatibility is job one. And that has conveyed to the world, the certainty of our platform stability. NVIDIA's platform stability certainty is the reason why everybody builds on us first and the reason why everybody optimizes on us first. All the engineering and all the work that you do, all the invention of technologies that you build on top of NVIDIA accrues to the -- and benefits everybody that uses our GPUs. And we have such a large installed base, large -- millions and millions of GPUs in cloud, 100 million GPUs from people’s PCs just about every workstation in the world, and they all architecturally compatible. And so, if you are an inference platform and you're deploying an inference application, you are basically an application provider. And as a software application provider, you're looking for large installed base. Data processing, before you could train a model, you have to curate the data, you have to dedupe the data, maybe you have to augment the data with synthetic data. So, process the data, clean the data, align the data, normalize the data, all of that data is measured not in bytes or megabytes, it's measured in terabytes and petabytes. And the amount of data processing that you do before data engineering, before that you do training is quite significant. It could represent 30%, 40%, 50% of the amount of work that you ultimately do. And what you -- and ultimately creating a data driven machine learning service. And so data processing is just a massive part. We accelerate Spark, we accelerate Python. One of the coolest things that we just did is called cuDF Pandas. Without one line of code, Pandas, which is the single most successful data science framework in the world. Pandas now is accelerated by NVIDIA CUDA. And just out-of-the box, without the line of code and so the acceleration is really quite terrific and people are just incredibly excited about it. And Pandas was designed for one purpose and one purpose only, really data processing, it's for data science. And so NVIDIA CUDA gives you all of that.\\nOperator: Your final question comes from the line of Harlan Sur of J.P. Morgan. Your line is open.\\nHarlan Sur: Good afternoon. Thanks for taking my question. If you look at the history of the tech industry like those companies that have been successful have always been focused on ecosystem; silicon, hardware, software, strong partnerships and just as importantly, right, an aggressive cadence of new products, more segmentation over time. The team recently announced a more aggressive new product cadence in data center from two years to now every year with higher levels of segmentation, training, optimization in printing CPU, GPU, DPU networking. How do we think about your R&D OpEx growth outlook to support a more aggressive and expanding forward roadmap, but more importantly, what is the team doing to manage and drive execution through all of this complexity?\\nJensen Huang: Gosh. Boy, that's just really excellent. You just wrote NVIDIA's business plan, and so you described our strategy. First of all, there is a fundamental reason why we accelerate our execution. And the reason for that is because it fundamentally drives down cost. When the combination of TensorRT-LLM and H200 reduce the cost for our customers for large model inference by a factor of four, and so that includes, of course, our speeds and feeds, but mostly it's because of our software, mostly the software benefits because of the architecture. And so we want to accelerate our roadmap for that reason. The second reason is to expand the reach of generative AI, the world's number of data center configurations -- this is kind of the amazing thing. NVIDIA is in every cloud, but not one cloud is the same. NVIDIA is working with every single cloud service provider and not one of the networking control plane, security posture is the same. Everybody's platform is different and yet we're integrated into all of their stacks, all of their data centers and we work incredibly well with all of them. And not to mention, we then take the whole thing and we create AI factories that are standalone. We take our platform, we can put them into supercomputers, we can put them into enterprise. Bringing AI to enterprise is something generative AI Enterprise something nobody's ever done before. And we're right now in the process of going to market with all of that. And so the complexity includes, of course, all the technologies and segments and the pace. It includes the fact that we are architecturally compatible across every single one of those. It includes all of the domain specific libraries that we create. The reason why every computer company, without thinking, can integrate NVIDIA into their roadmap and take it to market. And the reason for that is, because there is market demand for it. There is market demand in healthcare, there is market demand in manufacturing, there is market demand, of course, in AI, including financial services, in supercomputing and quantum computing. The list of markets and segments that we have domain specific libraries is incredibly broaden. And then finally, we have an end-to-end solution for data centers; InfiniBand networking, Ethernet networking, x86, ARM, just about every permutation combination of solutions -- technology solutions and software stacks provided. And that translates to having the largest number of ecosystem software developers; the largest ecosystem of system makers; the largest and broadest distribution partnership network; and ultimately, the greatest reach. And that takes -- surely that takes a lot of energy. But the thing that really holds it together, and this is a great decision that we made decades ago, which is everything is architecturally compatible. When we develop a domain specific language that runs on one GPU, it runs on every GPU. When we optimize TensorRT for the cloud, we optimized it for enterprise. When we do something that brings in a new feature, a new library, a new feature or a new developer, they instantly get the benefit of all of our reach. And so that discipline, that architecture compatible discipline that has lasted more than a couple of decades now, is one of the reasons why NVIDIA is still really, really efficient. I mean, we're 28,000 people large and serving just about every single company, every single industry, every single market around the world.\\nOperator: Thank you. I will now turn the call back over to Jensen Huang for closing remarks.\\nJensen Huang: Our strong growth reflects the broad industry platform transition from general purpose to accelerated computing and generative AI. Large language models start-ups consumer Internet companies and global cloud service providers are the first movers. The next waves are starting to build. Nations and regional CSPs are building AI clouds to serve local demand. Enterprise software companies like Adobe and Dropbox, SAP and ServiceNow are adding AI copilots and assistants to their platforms. Enterprises in the world's largest industries are creating custom AIs to automate and boost productivity. The generative AI era is in full steam and has created the need for a new type of data center, an AI factory; optimized for refining data and training, and inference, and generating AI. AI factory workloads are different and incremental to legacy data center workloads supporting IT tasks. AI factories run copilots and AI assistants, which are significant software TAM expansion and are driving significant new investment. Expanding the $1 trillion traditional data center infrastructure installed base, empowering the AI Industrial Revolution. NVIDIA H100 HGX with InfiniBand and the NVIDIA AI software stack define an AI factory today. As we expand our supply chain to meet the world's demand, we are also building new growth drivers for the next wave of AI. We highlighted three elements to our new growth strategy that are hitting their stride: CPU, networking, and software and services. Grace is NVIDIA's first data center CPU. Grace and Grace Hopper are in full production and ramping into a new multi-billion dollar product line next year. Irrespective of the CPU choice, we can help customers build an AI factory. NVIDIA networking now exceeds $10 billion annualized revenue run rate. InfiniBand grew five-fold year-over-year, and is positioned for excellent growth ahead as the networking of AI factories. Enterprises are also racing to adopt AI and Ethernet is the standard networking. This week we announced an Ethernet for AI platform for enterprises. NVIDIA Spectrum-X is an end-to-end solution of Bluefield SuperNIC, Spectrum-4 Ethernet switch and software that boosts Ethernet performance by up to 1.6x for AI workloads. Dell, HPE and Lenovo have joined us to bring a full generative AI solution of NVIDIA AI computing, networking and software to the world's enterprises. NVIDIA software and services is on track to exit the year at an annualized run rate of $1 billion. Enterprise software platforms like ServiceNow and SAP need to build and operate proprietary AI. Enterprises need to build and deploy custom AI copilots. We have the AI technology, expertise and scale to help customers build custom models with their proprietary data on NVIDIA DGX Cloud and deploy the AI applications on enterprise grade NVIDIA AI Enterprise. NVIDIA is essentially an AI foundry. NVIDIA's GPUs, CPUs, networking, AI foundry services and NVIDIA AI Enterprise software are all growth engines in full throttle. Thanks for joining us today. We look forward to updating you on our progress next quarter.\\nOperator: This concludes today's conference call. You may now disconnect.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " \"By Jeffrey Dastin (Reuters) - OpenAI's board of directors approached rival Anthropic's CEO about replacing chief Sam Altman and potentially merging the two AI startups, according to two people briefed on the matter. Anthropic CEO Dario Amodei declined on both fronts, the people said. The news, reported earlier by The Information on Monday, follows various reported calls to find Altman's successor days after OpenAI's board ousted him. OpenAI declined to comment and board member Adam D'Angelo did not immediately respond to a request for comment. On Sunday the board offered ex-Twitch CEO Emmett Shear to be its interim chief, who accepted. Meanwhile, Altman agreed to join the startup's backer Microsoft (NASDAQ:MSFT) along with other key personnel from OpenAI. By Monday, nearly all of the startup's more than 700 employees threatened to quit, including one of its executives on OpenAI's now four-person board. \\nThe co-founders of Anthropic, who were also executives at OpenAI until 2020, had broken from their employer over disagreements regarding how to ensure AI's safe development and governance. Anthropic has won investments from Alphabet (NASDAQ:GOOGL)'s Google and Amazon.com (NASDAQ:AMZN). Its Claude AI models have vied for prominence with OpenAI's GPT series.\",\n",
       " 'Investing.com – U.S. stocks were higher after the close on Wednesday, as gains in the Consumer Services, Healthcare and Financials sectors led shares higher.\\nAt the close in NYSE, the Dow Jones Industrial Average rose 0.53% to hit a new 3-months high, while the S&P 500 index climbed 0.41%, and the NASDAQ Composite index added 0.46%.\\nThe best performers of the session on the Dow Jones Industrial Average were 3M Company (NYSE:MMM), which rose 1.47% or 1.38 points to trade at 95.37 at the close. Meanwhile, Microsoft Corporation (NASDAQ:MSFT) added 1.28% or 4.78 points to end at 377.85 and  Home Depot  Inc (NYSE:HD) was up 1.27% or 3.87 points to 309.21 in late trade.\\nThe worst performers of the session were Walgreens Boots Alliance Inc (NASDAQ:WBA), which fell 1.38% or 0.29 points to trade at 20.69 at the close.  Caterpillar  Inc (NYSE:CAT) declined 1.38% or 3.43 points to end at 245.98 and Walmart Inc (NYSE:WMT) was down 0.76% or 1.18 points to 154.68.\\nThe top performers on the S&P 500 were eBay Inc (NASDAQ:EBAY) which rose 3.09% to 41.64, Advanced Micro Devices Inc (NASDAQ:AMD) which was up 2.81% to settle at 122.51 and HP Inc (NYSE:HPQ) which gained 2.80% to close at 28.65.\\nThe worst performers were  Autodesk Inc  (NASDAQ:ADSK) which was down 6.90% to 202.66 in late trade, Deere & Company (NYSE:DE) which lost 3.11% to settle at 370.74 and Tesla Inc (NASDAQ:TSLA) which was down 2.90% to 234.21 at the close.\\nThe top performers on the NASDAQ Composite were  Impel Neuropharma  Inc (NASDAQ:IMPL) which rose 143.44% to 0.78, AgileThought Inc (NASDAQ:AGIL) which was up 137.35% to settle at 0.20 and  Presidio Property Trust  (NASDAQ:SQFT) which gained 53.43% to close at 0.94.\\nThe worst performers were  Alpha Technology  Group Ltd (NASDAQ:ATGL) which was down 32.65% to 13.80 in late trade,  Entrada Therapeutics  Inc (NASDAQ:TRDA) which lost 31.85% to settle at 11.36 and  Hongli  Group Inc (NASDAQ:HLP) which was down 30.19% to 3.70 at the close.\\nRising stocks outnumbered declining ones on the New York Stock Exchange by 1845 to 1023 and 107 ended unchanged; on the Nasdaq Stock Exchange, 2137 rose and 1296 declined, while 150 ended unchanged.\\nShares in Microsoft Corporation (NASDAQ:MSFT) rose to all time highs; rising 1.28% or 4.78 to 377.85. \\nThe CBOE Volatility Index, which measures the implied volatility of S&P 500 options, was down 3.75% to 12.85 a new 1-month low.\\nGold Futures for December delivery was down 0.50% or 10.05 to $1,991.55 a troy ounce. Elsewhere in commodities trading, Crude oil for delivery in January fell 1.26% or 0.98 to hit $76.79 a barrel, while the January Brent oil contract fell 0.96% or 0.79 to trade at $81.66 a barrel.\\nEUR/USD was unchanged 0.18% to 1.09, while USD/JPY rose 0.83% to 149.62.\\nThe US Dollar Index Futures was up 0.31% at 103.77.',\n",
       " 'Investing.com -- The Dow ended higher Wednesday ahead of the Thanksgiving holiday as tech shrugged off a slip in Nvidia (NASDAQ:NVDA) following chipmaker\\'s cautious outlook on growth in China ahead of\\xa0\\nBy 16:00 ET (21:00 GMT), the Dow Jones Industrial Average\\xa0rose 184 points, or 0.5%, the S&P 500\\xa0gained 0.4% and the NASDAQ Composite\\xa0added 0.5%.\\xa0\\nTech stocks rise despite Nvidia slip; Altman returns to OpenAI\\nTech stocks were in the ascendency, with a more than 1% increase in Google (NASDAQ:GOOGL), Microsoft (NASDAQ:MSFT) and Meta (NASDAQ:META) helping to stem the blow from a more than 2% fall in Nvidia.\\xa0\\nNVIDIA Corporation (NASDAQ:NVDA) reported guidance and Q3 results that topped Wall Street estimates, but the chipmaker\\xa0warned that sales in China, which accounts for up to a fifth of sales in its high-margin data center business,\\xa0\\xa0would \"decline significantly\" in its current quarter amid U.S. export restrictions on AI chips to China.\\nElsewhere, Sam Altman is set to return to the helm of OpenAI just days after he was sacked as chief executive of the big-name generative artificial intelligence group.\\nHP Inc (NYSE:HPQ), meanwhile, rose nearly 3% despite reporting mixed fourth-quarter results as revenue missed Wall Street estimates, while current guidance also fell short of expectations. Some on Wall Street took a cautious view on the stock, expecting HP\\'s printing business to continue to weigh.\\xa0\\n\"[W]e remain cautious on slowing capital returns, elevated leverage and a printing business that is just beginning to roll, which we believewill likely limit outperformance, leaving us equal-weight\"Morgan Stanely said in a note.\\nDeere in seller headlights after guidance spooks investors; Nordstorm\\'s Q3 results fall short\\nDeere & Company (NYSE:DE), the world\\'s largest farm equipment maker, fell 3% after it forecasted 2024 profit below expectations as high borrowing costs and squeezed budgets dented demand for farm equipment.\\nThe dour fiscal 2024 guidance, which includes a forecast for a fall in net income to range of $7.75B to $8.25B from $10.16B in 2023, offset quarterly results that beat on both the top and bottom lines.\\nNordstrom (NYSE:JWN), meanwhile, also offered a cautious outlook, flagging a \"softening consumer spend\" and reported third-quarter revenue that missed analyst estimates, sending its shares more than 4% lower.\\xa0\\nEnergy closes above session lows as oil prices cut losses despite delayed OPEC+ meeting,\\xa0U.S. inventories build\\nEnergy stocks ended the day marginally lower after oil prices cut losses to settle well above session lows despite fresh signs of increasing crude supply.\\xa0 \\xa0\\nThe Organization of the Petroleum Exporting Countries and and its allies, or OPEC+, postponed a scheduled meeting to Nov. 30 from Nov. 26, amid struggles to agree on production levels, stoking uncertain over potential output cuts.\\nOil prices were also pressured by a larger than expected draw in weekly U.S. crude inventories.\\xa0\\nEOG Resources Inc (NYSE:EOG),  Baker Hughes  Co (NASDAQ:BKR) and Occidental Petroleum (NYSE:OXY) were among the biggest decliners, closing nearly 1% lower.\\nTreasury yields end mixed on signs of strength in jobs market\\nAfter briefly touching 2-month lows, Treasury yields ended mixed on the day, with the yield on the 2-year Treasury up 17 basis points to 4.9%, while the 10-year Treasury yield slipped 5 bps to 4.409% following mixed economic data.\\xa0\\nWeekly U.S. initial jobless claims missed expectations, pointing to ongoing labor market strength, while durable goods for October undershot economist expectations.\\xa0 \\xa0\\n(Peter Nurse and Oliver Gray contributed to this report.)',\n",
       " \"Baidu Inc (NASDAQ:BIDU) reported a notable increase in third-quarter revenue, reaching $4.72 billion, though the pace has decelerated compared to the second quarter. The announcement led to a 2% surge in the company's shares today. Baidu's net income also rose significantly to approximately $943 million, alongside increased investments in selling, administrative expenses, and research and development. These financial commitments are particularly focused on the advancement of Baidu's Ernie bot, a direct competitor to OpenAI's GPT-4 series.\\nThe Chinese tech giant has made headlines with its AI developments despite facing challenges such as U.S. chip sanctions that could potentially hinder AI progress for up to two years. Nevertheless, Baidu CEO Robin Li has showcased Ernie bot's capabilities, which has already attracted over 45 million users. This development positions Baidu as a key player in the AI industry, competing on a global scale.\\nIn related industry news, Microsoft Corporation (NASDAQ:MSFT) recently turned what could have been a significant setback into an opportunity by hiring Sam Altman and other ex-OpenAI staff. This move underscores the intense competition and strategic talent acquisition prevalent in the tech sector.\\nMoreover, Tencent Holdings (OTC:TCEHY) has also demonstrated its ability to withstand the impact of U.S. sanctions. The company confirmed that its stockpile of chips ensures its resilience and continued operations despite external pressures.\\nBaidu's strategic focus on AI and its ability to navigate through regulatory challenges highlights the company's adaptability and commitment to maintaining a competitive edge in the rapidly evolving technology landscape.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " 'Tuesday\\'s stock market activity has been marked by fluctuations as investors awaited the release of the Federal Reserve\\'s meeting minutes and key earnings reports. Earlier in the day, following a successful 20-year bond auction, both bonds and stocks, including tech giants Microsoft (NASDAQ:MSFT) and Nvidia (NASDAQ:NVDA), rallied to record highs. The optimism was tempered later on, however, as shares fell ahead of the anticipated economic updates.\\nThe Federal Reserve is expected to release its meeting minutes at 2 p.m. today. These records are closely watched by traders for hints about future interest rate moves. The consensus among market participants is that the Fed will maintain steady rates towards year-end, especially given the recent decline in long-term yields. Despite a slowdown in October\\'s inflation, central bank officials have not dismissed the possibility of further rate hikes.\\nIn addition to the Fed\\'s minutes, Nvidia is projected to announce its third-quarter earnings after the market closes today. The company is expected to surpass its $16 billion forecast, which has contributed to investor confidence and a surge in its stock price to record levels.\\nContrastingly, retailers like Lowe\\'s (NYSE:LOW) and American Eagle (NYSE:AEO) released disappointing earnings reports, reflecting the impact of reduced consumer spending. This news influenced a downturn in the stock market and dampened the earlier rally.\\nAmidst these developments, a group of leading tech companies, referred to as the \"Magnificent 7,\" saw their market capitalization increase by $150 billion. This surge was partly fueled by excitement over news from OpenAI, highlighting the ongoing interest in technology investments.\\nOverall, market behavior suggests that investors may be anticipating an early \"Santa rally,\" a term used to describe a rise in stock prices during December. This upbeat sentiment contrasts with anecdotes such as that of a former jazz critic who expressed regret over selling Apple shares (NASDAQ:AAPL) too soon and missing out on substantial gains.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.',\n",
       " \"European companies, including Airbus SE (OTC:EADSY) and Deutsche Telekom AG (ETR:DTEGn), have endorsed the European Union Agency for Cybersecurity's (ENISA) proposal for stricter cybersecurity regulations. The new rules aim to safeguard EU data from access by non-EU governments, with a particular focus on NATO exemptions. This move could have significant implications for US-based cloud service providers such as Amazon (NASDAQ:AMZN) and Microsoft (NASDAQ:MSFT), which are seeking voluntary certification to secure contracts within the EU.\\nThe endorsement of stricter cybersecurity measures by major European corporations underscores the heightened concerns over data protection and privacy within the EU. These companies are advocating for a harmonized set of security standards across the EU, which they believe should be applied universally without exceptions for organizations like NATO. The push for tighter regulations is part of a broader effort to ensure that European data remains secure from external threats, including potential access by foreign governments.\\nThe proposed rules could pose challenges for US cloud providers that operate in Europe. As they seek to obtain voluntary certification to continue serving their EU clients, these providers may need to adapt their operations to comply with the enhanced security requirements. The certification process is designed to demonstrate a company's commitment to protecting EU data, but with the recommended stricter guidelines, achieving this certification could become more complex.\\nThis development is particularly relevant in the context of ongoing discussions about digital sovereignty and the control of critical data infrastructure. By supporting stronger cybersecurity regulations, European entities are signaling their commitment to maintaining control over their data and ensuring its protection against a wide array of cyber threats.\\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\",\n",
       " 'By Samrhitha A (Reuters) -HP Inc on Tuesday forecast first-quarter profit below Wall Street estimates but maintained annual earnings outlook, a sign that demand in the personal computers market is still recovering, sending its shares down nearly 4% after market. Companies such as HP (NYSE:HPQ), Lenovo and Dell (NYSE:DELL) have seen demand ease from peaks hit during the pandemic, when work-from-home trends boosted sales of laptops and other electronic devices. HP expects first-quarter adjusted profit per share to be between 76 cents and 86 cents, the midpoint of which was below LSEG estimates of 86 cents. The company said it is on track to launch AI PCs in the second half of next year and expects its penetration to increase gradually. \"While we don\\'t think the market will immediately shift to AI PCs, we believe there will be a gradual uptick in the uptake, with some in 2024 and more penetration to come in 2025 and even more in 2026,\" CEO Enrique Lores said in a media call. Recent earnings at major PC chipmakers have signaled that more than a two-year long slump in the market could be nearing an end as demand picks up ahead of the holiday season and an expected Windows update next year from Microsoft (NASDAQ:MSFT). HP maintained fiscal 2024 adjusted profit forecast of $3.25 to $3.65 per share. Its fourth-quarter revenue was $13.82 billion, slightly lower than the estimated $13.85 billion. \"We continue to see weak demand in China both across consumer and commercial. At this point we don\\'t expect that to change and we\\'ve built that into our plans,\" Lores said. \\nSales for HP\\'s personal systems segment — home to its desktop and notebook PCs — fell 8% from a year ago, while its printing segment posted a 3% fall. Peer Lenovo posted a decline in revenue last week. Dell is scheduled to release third-quarter results on Nov. 30.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = article_links[:]\n",
    "test_list\n",
    "\n",
    "def get_article_text(link):\n",
    "\n",
    "    response = requests.get(link)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        article = soup.find('div', class_='WYSIWYG articlePage')\n",
    "\n",
    "        script = article.find(\"script\")\n",
    "        if script != None:\n",
    "            script.extract()\n",
    "        img_carousel = article.find('div', class_='imgCarousel')\n",
    "        if img_carousel != None:\n",
    "            img_carousel.extract()\n",
    "        related_instruments_wrapper = article.find('div', class_='relatedInstrumentsWrapper')\n",
    "        if related_instruments_wrapper != None:\n",
    "            related_instruments_wrapper.extract()\n",
    "        paragraph = article.find(\"p\")\n",
    "        if paragraph != None:\n",
    "            em = paragraph.find(\"em\")\n",
    "            if em != None:\n",
    "                paragraph.extract()\n",
    "\n",
    "        text_inside_div = article.get_text()\n",
    "        return text_inside_div.strip()\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_article_texts(links):\n",
    "    return [get_article_text(link) for link in links]\n",
    "\n",
    "get_article_texts(test_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
